[
  {
    "id": "0704.1394",
    "title": "Calculating Valid Domains for BDD-Based Interactive Configuration",
    "abstract": "  In these notes we formally describe the functionality of Calculating Valid\nDomains from the BDD representing the solution space of valid configurations.\nThe formalization is largely based on the CLab configuration framework.\n"
  },
  {
    "id": "0704.2010",
    "title": "A study of structural properties on profiles HMMs",
    "abstract": "  Motivation: Profile hidden Markov Models (pHMMs) are a popular and very\nuseful tool in the detection of the remote homologue protein families.\nUnfortunately, their performance is not always satisfactory when proteins are\nin the 'twilight zone'. We present HMMER-STRUCT, a model construction algorithm\nand tool that tries to improve pHMM performance by using structural information\nwhile training pHMMs. As a first step, HMMER-STRUCT constructs a set of pHMMs.\nEach pHMM is constructed by weighting each residue in an aligned protein\naccording to a specific structural property of the residue. Properties used\nwere primary, secondary and tertiary structures, accessibility and packing.\nHMMER-STRUCT then prioritizes the results by voting. Results: We used the SCOP\ndatabase to perform our experiments. Throughout, we apply leave-one-family-out\ncross-validation over protein superfamilies. First, we used the MAMMOTH-mult\nstructural aligner to align the training set proteins. Then, we performed two\nsets of experiments. In a first experiment, we compared structure weighted\nmodels against standard pHMMs and against each other. In a second experiment,\nwe compared the voting model against individual pHMMs. We compare method\nperformance through ROC curves and through Precision/Recall curves, and assess\nsignificance through the paired two tailed t-test. Our results show significant\nperformance improvements of all structurally weighted models over default\nHMMER, and a significant improvement in sensitivity of the combined models over\nboth the original model and the structurally weighted models.\n"
  },
  {
    "id": "0704.3433",
    "title": "Bayesian approach to rough set",
    "abstract": "  This paper proposes an approach to training rough set models using Bayesian\nframework trained using Markov Chain Monte Carlo (MCMC) method. The prior\nprobabilities are constructed from the prior knowledge that good rough set\nmodels have fewer rules. Markov Chain Monte Carlo sampling is conducted through\nsampling in the rough set granule space and Metropolis algorithm is used as an\nacceptance criteria. The proposed method is tested to estimate the risk of HIV\ngiven demographic data. The results obtained shows that the proposed approach\nis able to achieve an average accuracy of 58% with the accuracy varying up to\n66%. In addition the Bayesian rough set give the probabilities of the estimated\nHIV status as well as the linguistic rules describing how the demographic\nparameters drive the risk of HIV.\n"
  },
  {
    "id": "0704.3515",
    "title": "Comparing Robustness of Pairwise and Multiclass Neural-Network Systems\n  for Face Recognition",
    "abstract": "  Noise, corruptions and variations in face images can seriously hurt the\nperformance of face recognition systems. To make such systems robust,\nmulticlass neuralnetwork classifiers capable of learning from noisy data have\nbeen suggested. However on large face data sets such systems cannot provide the\nrobustness at a high level. In this paper we explore a pairwise neural-network\nsystem as an alternative approach to improving the robustness of face\nrecognition. In our experiments this approach is shown to outperform the\nmulticlass neural-network system in terms of the predictive accuracy on the\nface images corrupted by noise.\n"
  },
  {
    "id": "0704.3905",
    "title": "Ensemble Learning for Free with Evolutionary Algorithms ?",
    "abstract": "  Evolutionary Learning proceeds by evolving a population of classifiers, from\nwhich it generally returns (with some notable exceptions) the single\nbest-of-run classifier as final result. In the meanwhile, Ensemble Learning,\none of the most efficient approaches in supervised Machine Learning for the\nlast decade, proceeds by building a population of diverse classifiers. Ensemble\nLearning with Evolutionary Computation thus receives increasing attention. The\nEvolutionary Ensemble Learning (EEL) approach presented in this paper features\ntwo contributions. First, a new fitness function, inspired by co-evolution and\nenforcing the classifier diversity, is presented. Further, a new selection\ncriterion based on the classification margin is proposed. This criterion is\nused to extract the classifier ensemble from the final population only\n(Off-line) or incrementally along evolution (On-line). Experiments on a set of\nbenchmark problems show that Off-line outperforms single-hypothesis\nevolutionary learning and state-of-art Boosting and generates smaller\nclassifier ensembles.\n"
  },
  {
    "id": "0705.0197",
    "title": "Fault Classification in Cylinders Using Multilayer Perceptrons, Support\n  Vector Machines and Guassian Mixture Models",
    "abstract": "  Gaussian mixture models (GMM) and support vector machines (SVM) are\nintroduced to classify faults in a population of cylindrical shells. The\nproposed procedures are tested on a population of 20 cylindrical shells and\ntheir performance is compared to the procedure, which uses multi-layer\nperceptrons (MLP). The modal properties extracted from vibration data are used\nto train the GMM, SVM and MLP. It is observed that the GMM produces 98%, SVM\nproduces 94% classification accuracy while the MLP produces 88% classification\nrates.\n"
  },
  {
    "id": "0705.0693",
    "title": "Learning to Bluff",
    "abstract": "  The act of bluffing confounds game designers to this day. The very nature of\nbluffing is even open for debate, adding further complication to the process of\ncreating intelligent virtual players that can bluff, and hence play,\nrealistically. Through the use of intelligent, learning agents, and carefully\ndesigned agent outlooks, an agent can in fact learn to predict its opponents\nreactions based not only on its own cards, but on the actions of those around\nit. With this wider scope of understanding, an agent can in learn to bluff its\nopponents, with the action representing not an illogical action, as bluffing is\noften viewed, but rather as an act of maximising returns through an effective\nstatistical optimisation. By using a tee dee lambda learning algorithm to\ncontinuously adapt neural network agent intelligence, agents have been shown to\nbe able to learn to bluff without outside prompting, and even to learn to call\neach others bluffs in free, competitive play.\n"
  },
  {
    "id": "0705.0734",
    "title": "Soft constraint abstraction based on semiring homomorphism",
    "abstract": "  The semiring-based constraint satisfaction problems (semiring CSPs), proposed\nby Bistarelli, Montanari and Rossi \\cite{BMR97}, is a very general framework of\nsoft constraints. In this paper we propose an abstraction scheme for soft\nconstraints that uses semiring homomorphism. To find optimal solutions of the\nconcrete problem, the idea is, first working in the abstract problem and\nfinding its optimal solutions, then using them to solve the concrete problem.\n  In particular, we show that a mapping preserves optimal solutions if and only\nif it is an order-reflecting semiring homomorphism. Moreover, for a semiring\nhomomorphism $\\alpha$ and a problem $P$ over $S$, if $t$ is optimal in\n$\\alpha(P)$, then there is an optimal solution $\\bar{t}$ of $P$ such that\n$\\bar{t}$ has the same value as $t$ in $\\alpha(P)$.\n"
  },
  {
    "id": "0705.0761",
    "title": "Bayesian Approach to Neuro-Rough Models",
    "abstract": "  This paper proposes a neuro-rough model based on multi-layered perceptron and\nrough set. The neuro-rough model is then tested on modelling the risk of HIV\nfrom demographic data. The model is formulated using Bayesian framework and\ntrained using Monte Carlo method and Metropolis criterion. When the model was\ntested to estimate the risk of HIV infection given the demographic data it was\nfound to give the accuracy of 62%. The proposed model is able to combine the\naccuracy of the Bayesian MLP model and the transparency of Bayesian rough set\nmodel.\n"
  },
  {
    "id": "0705.0969",
    "title": "Artificial Neural Networks and Support Vector Machines for Water Demand\n  Time Series Forecasting",
    "abstract": "  Water plays a pivotal role in many physical processes, and most importantly\nin sustaining human life, animal life and plant life. Water supply entities\ntherefore have the responsibility to supply clean and safe water at the rate\nrequired by the consumer. It is therefore necessary to implement mechanisms and\nsystems that can be employed to predict both short-term and long-term water\ndemands. The increasingly growing field of computational intelligence\ntechniques has been proposed as an efficient tool in the modelling of dynamic\nphenomena. The primary objective of this paper is to compare the efficiency of\ntwo computational intelligence techniques in water demand forecasting. The\ntechniques under comparison are the Artificial Neural Networks (ANNs) and the\nSupport Vector Machines (SVMs). In this study it was observed that the ANNs\nperform better than the SVMs. This performance is measured against the\ngeneralisation ability of the two.\n"
  },
  {
    "id": "0705.1031",
    "title": "Fuzzy Artmap and Neural Network Approach to Online Processing of Inputs\n  with Missing Values",
    "abstract": "  An ensemble based approach for dealing with missing data, without predicting\nor imputing the missing values is proposed. This technique is suitable for\nonline operations of neural networks and as a result, is used for online\ncondition monitoring. The proposed technique is tested in both classification\nand regression problems. An ensemble of Fuzzy-ARTMAPs is used for\nclassification whereas an ensemble of multi-layer perceptrons is used for the\nregression problem. Results obtained using this ensemble-based technique are\ncompared to those obtained using a combination of auto-associative neural\nnetworks and genetic algorithms and findings show that this method can perform\nup to 9% better in regression problems. Another advantage of the proposed\ntechnique is that it eliminates the need for finding the best estimate of the\ndata, and hence, saves time.\n"
  },
  {
    "id": "0705.1209",
    "title": "Artificial Intelligence for Conflict Management",
    "abstract": "  Militarised conflict is one of the risks that have a significant impact on\nsociety. Militarised Interstate Dispute (MID) is defined as an outcome of\ninterstate interactions, which result on either peace or conflict. Effective\nprediction of the possibility of conflict between states is an important\ndecision support tool for policy makers. In a previous research, neural\nnetworks (NNs) have been implemented to predict the MID. Support Vector\nMachines (SVMs) have proven to be very good prediction techniques and are\nintroduced for the prediction of MIDs in this study and compared to neural\nnetworks. The results show that SVMs predict MID better than NNs while NNs give\nmore consistent and easy to interpret sensitivity analysis than SVMs.\n"
  },
  {
    "id": "0705.1244",
    "title": "Evolving Symbolic Controllers",
    "abstract": "  The idea of symbolic controllers tries to bridge the gap between the top-down\nmanual design of the controller architecture, as advocated in Brooks'\nsubsumption architecture, and the bottom-up designer-free approach that is now\nstandard within the Evolutionary Robotics community. The designer provides a\nset of elementary behavior, and evolution is given the goal of assembling them\nto solve complex tasks. Two experiments are presented, demonstrating the\nefficiency and showing the recursiveness of this approach. In particular, the\nsensitivity with respect to the proposed elementary behaviors, and the\nrobustness w.r.t. generalization of the resulting controllers are studied in\ndetail.\n"
  },
  {
    "id": "0705.1309",
    "title": "Robust Multi-Cellular Developmental Design",
    "abstract": "  This paper introduces a continuous model for Multi-cellular Developmental\nDesign. The cells are fixed on a 2D grid and exchange \"chemicals\" with their\nneighbors during the growth process. The quantity of chemicals that a cell\nproduces, as well as the differentiation value of the cell in the phenotype,\nare controlled by a Neural Network (the genotype) that takes as inputs the\nchemicals produced by the neighboring cells at the previous time step. In the\nproposed model, the number of iterations of the growth process is not\npre-determined, but emerges during evolution: only organisms for which the\ngrowth process stabilizes give a phenotype (the stable state), others are\ndeclared nonviable. The optimization of the controller is done using the NEAT\nalgorithm, that optimizes both the topology and the weights of the Neural\nNetworks. Though each cell only receives local information from its neighbors,\nthe experimental results of the proposed approach on the 'flags' problems (the\nphenotype must match a given 2D pattern) are almost as good as those of a\ndirect regression approach using the same model with global information.\nMoreover, the resulting multi-cellular organisms exhibit almost perfect\nself-healing characteristics.\n"
  },
  {
    "id": "0705.2235",
    "title": "Response Prediction of Structural System Subject to Earthquake Motions\n  using Artificial Neural Network",
    "abstract": "  This paper uses Artificial Neural Network (ANN) models to compute response of\nstructural system subject to Indian earthquakes at Chamoli and Uttarkashi\nground motion data. The system is first trained for a single real earthquake\ndata. The trained ANN architecture is then used to simulate earthquakes with\nvarious intensities and it was found that the predicted responses given by ANN\nmodel are accurate for practical purposes. When the ANN is trained by a part of\nthe ground motion data, it can also identify the responses of the structural\nsystem well. In this way the safeness of the structural systems may be\npredicted in case of future earthquakes without waiting for the earthquake to\noccur for the lessons. Time period and the corresponding maximum response of\nthe building for an earthquake has been evaluated, which is again trained to\npredict the maximum response of the building at different time periods. The\ntrained time period versus maximum response ANN model is also tested for real\nearthquake data of other place, which was not used in the training and was\nfound to be in good agreement.\n"
  },
  {
    "id": "0705.2236",
    "title": "Fault Classification using Pseudomodal Energies and Neuro-fuzzy\n  modelling",
    "abstract": "  This paper presents a fault classification method which makes use of a\nTakagi-Sugeno neuro-fuzzy model and Pseudomodal energies calculated from the\nvibration signals of cylindrical shells. The calculation of Pseudomodal\nEnergies, for the purposes of condition monitoring, has previously been found\nto be an accurate method of extracting features from vibration signals. This\ncalculation is therefore used to extract features from vibration signals\nobtained from a diverse population of cylindrical shells. Some of the cylinders\nin the population have faults in different substructures. The pseudomodal\nenergies calculated from the vibration signals are then used as inputs to a\nneuro-fuzzy model. A leave-one-out cross-validation process is used to test the\nperformance of the model. It is found that the neuro-fuzzy model is able to\nclassify faults with an accuracy of 91.62%, which is higher than the previously\nused multilayer perceptron.\n"
  },
  {
    "id": "0705.2310",
    "title": "On-Line Condition Monitoring using Computational Intelligence",
    "abstract": "  This paper presents bushing condition monitoring frameworks that use\nmulti-layer perceptrons (MLP), radial basis functions (RBF) and support vector\nmachines (SVM) classifiers. The first level of the framework determines if the\nbushing is faulty or not while the second level determines the type of fault.\nThe diagnostic gases in the bushings are analyzed using the dissolve gas\nanalysis. MLP gives superior performance in terms of accuracy and training time\nthan SVM and RBF. In addition, an on-line bushing condition monitoring\napproach, which is able to adapt to newly acquired data are introduced. This\napproach is able to accommodate new classes that are introduced by incoming\ndata and is implemented using an incremental learning algorithm that uses MLP.\nThe testing results improved from 67.5% to 95.8% as new data were introduced\nand the testing results improved from 60% to 95.3% as new conditions were\nintroduced. On average the confidence value of the framework on its decision\nwas 0.92.\n"
  },
  {
    "id": "0705.3360",
    "title": "The Road to Quantum Artificial Intelligence",
    "abstract": "  This paper overviews the basic principles and recent advances in the emerging\nfield of Quantum Computation (QC), highlighting its potential application to\nArtificial Intelligence (AI). The paper provides a very brief introduction to\nbasic QC issues like quantum registers, quantum gates and quantum algorithms\nand then it presents references, ideas and research guidelines on how QC can be\nused to deal with some basic AI problems, such as search and pattern matching,\nas soon as quantum computers become widely available.\n"
  },
  {
    "id": "0705.4302",
    "title": "Truecluster matching",
    "abstract": "  Cluster matching by permuting cluster labels is important in many clustering\ncontexts such as cluster validation and cluster ensemble techniques. The\nclassic approach is to minimize the euclidean distance between two cluster\nsolutions which induces inappropriate stability in certain settings. Therefore,\nwe present the truematch algorithm that introduces two improvements best\nexplained in the crisp case. First, instead of maximizing the trace of the\ncluster crosstable, we propose to maximize a chi-square transformation of this\ncrosstable. Thus, the trace will not be dominated by the cells with the largest\ncounts but by the cells with the most non-random observations, taking into\naccount the marginals. Second, we suggest a probabilistic component in order to\nbreak ties and to make the matching algorithm truly random on random data. The\ntruematch algorithm is designed as a building block of the truecluster\nframework and scales in polynomial time. First simulation results confirm that\nthe truematch algorithm gives more consistent truecluster results for unequal\ncluster sizes. Free R software is available.\n"
  },
  {
    "id": "0706.0022",
    "title": "Modeling Computations in a Semantic Network",
    "abstract": "  Semantic network research has seen a resurgence from its early history in the\ncognitive sciences with the inception of the Semantic Web initiative. The\nSemantic Web effort has brought forth an array of technologies that support the\nencoding, storage, and querying of the semantic network data structure at the\nworld stage. Currently, the popular conception of the Semantic Web is that of a\ndata modeling medium where real and conceptual entities are related in\nsemantically meaningful ways. However, new models have emerged that explicitly\nencode procedural information within the semantic network substrate. With these\nnew technologies, the Semantic Web has evolved from a data modeling medium to a\ncomputational medium. This article provides a classification of existing\ncomputational modeling efforts and the requirements of supporting technologies\nthat will aid in the further growth of this burgeoning domain.\n"
  },
  {
    "id": "0706.1137",
    "title": "Automatically Restructuring Practice Guidelines using the GEM DTD",
    "abstract": "  This paper describes a system capable of semi-automatically filling an XML\ntemplate from free texts in the clinical domain (practice guidelines). The XML\ntemplate includes semantic information not explicitly encoded in the text\n(pairs of conditions and actions/recommendations). Therefore, there is a need\nto compute the exact scope of conditions over text sequences expressing the\nrequired actions. We present a system developed for this task. We show that it\nyields good performance when applied to the analysis of French practice\nguidelines.\n"
  },
  {
    "id": "0706.1290",
    "title": "Temporal Reasoning without Transitive Tables",
    "abstract": "  Representing and reasoning about qualitative temporal information is an\nessential part of many artificial intelligence tasks. Lots of models have been\nproposed in the litterature for representing such temporal information. All\nderive from a point-based or an interval-based framework. One fundamental\nreasoning task that arises in applications of these frameworks is given by the\nfollowing scheme: given possibly indefinite and incomplete knowledge of the\nbinary relationships between some temporal objects, find the consistent\nscenarii between all these objects. All these models require transitive tables\n-- or similarly inference rules-- for solving such tasks. We have defined an\nalternative model, S-languages - to represent qualitative temporal information,\nbased on the only two relations of \\emph{precedence} and \\emph{simultaneity}.\nIn this paper, we show how this model enables to avoid transitive tables or\ninference rules to handle this kind of problem.\n"
  },
  {
    "id": "0706.3639",
    "title": "A Collection of Definitions of Intelligence",
    "abstract": "  This paper is a survey of a large number of informal definitions of\n``intelligence'' that the authors have collected over the years. Naturally,\ncompiling a complete list would be impossible as many definitions of\nintelligence are buried deep inside articles and books. Nevertheless, the\n70-odd definitions presented here are, to the authors' knowledge, the largest\nand most well referenced collection there is.\n"
  },
  {
    "id": "0706.4375",
    "title": "A Robust Linguistic Platform for Efficient and Domain specific Web\n  Content Analysis",
    "abstract": "  Web semantic access in specific domains calls for specialized search engines\nwith enhanced semantic querying and indexing capacities, which pertain both to\ninformation retrieval (IR) and to information extraction (IE). A rich\nlinguistic analysis is required either to identify the relevant semantic units\nto index and weight them according to linguistic specific statistical\ndistribution, or as the basis of an information extraction process. Recent\ndevelopments make Natural Language Processing (NLP) techniques reliable enough\nto process large collections of documents and to enrich them with semantic\nannotations. This paper focuses on the design and the development of a text\nprocessing platform, Ogmios, which has been developed in the ALVIS project. The\nOgmios platform exploits existing NLP modules and resources, which may be tuned\nto specific domains and produces linguistically annotated documents. We show\nhow the three constraints of genericity, domain semantic awareness and\nperformance can be handled all together.\n"
  },
  {
    "id": "0707.2506",
    "title": "Mixed Integer Linear Programming For Exact Finite-Horizon Planning In\n  Decentralized Pomdps",
    "abstract": "  We consider the problem of finding an n-agent joint-policy for the optimal\nfinite-horizon control of a decentralized Pomdp (Dec-Pomdp). This is a problem\nof very high complexity (NEXP-hard in n >= 2). In this paper, we propose a new\nmathematical programming approach for the problem. Our approach is based on two\nideas: First, we represent each agent's policy in the sequence-form and not in\nthe tree-form, thereby obtaining a very compact representation of the set of\njoint-policies. Second, using this compact representation, we solve this\nproblem as an instance of combinatorial optimization for which we formulate a\nmixed integer linear program (MILP). The optimal solution of the MILP directly\nyields an optimal joint-policy for the Dec-Pomdp. Computational experience\nshows that formulating and solving the MILP requires significantly less time to\nsolve benchmark Dec-Pomdp problems than existing algorithms. For example, the\nmulti-agent tiger problem for horizon 4 is solved in 72 secs with the MILP\nwhereas existing algorithms require several hours to solve it.\n"
  },
  {
    "id": "0707.4289",
    "title": "A Leaf Recognition Algorithm for Plant Classification Using\n  Probabilistic Neural Network",
    "abstract": "  In this paper, we employ Probabilistic Neural Network (PNN) with image and\ndata processing techniques to implement a general purpose automated leaf\nrecognition algorithm. 12 leaf features are extracted and orthogonalized into 5\nprincipal variables which consist the input vector of the PNN. The PNN is\ntrained by 1800 leaves to classify 32 kinds of plants with an accuracy greater\nthan 90%. Compared with other approaches, our algorithm is an accurate\nartificial intelligence approach which is fast in execution and easy in\nimplementation.\n"
  },
  {
    "id": "0708.4311",
    "title": "2006: Celebrating 75 years of AI - History and Outlook: the Next 25\n  Years",
    "abstract": "  When Kurt Goedel layed the foundations of theoretical computer science in\n1931, he also introduced essential concepts of the theory of Artificial\nIntelligence (AI). Although much of subsequent AI research has focused on\nheuristics, which still play a major role in many practical AI applications, in\nthe new millennium AI theory has finally become a full-fledged formal science,\nwith important optimality results for embodied agents living in unknown\nenvironments, obtained through a combination of theory a la Goedel and\nprobability theory. Here we look back at important milestones of AI history,\nmention essential recent results, and speculate about what we may expect from\nthe next 25 years, emphasizing the significance of the ongoing dramatic\nhardware speedups, and discussing Goedel-inspired, self-referential,\nself-improving universal problem solvers.\n"
  },
  {
    "id": "0709.0522",
    "title": "Qualitative Belief Conditioning Rules (QBCR)",
    "abstract": "  In this paper we extend the new family of (quantitative) Belief Conditioning\nRules (BCR) recently developed in the Dezert-Smarandache Theory (DSmT) to their\nqualitative counterpart for belief revision. Since the revision of quantitative\nas well as qualitative belief assignment given the occurrence of a new event\n(the conditioning constraint) can be done in many possible ways, we present\nhere only what we consider as the most appealing Qualitative Belief\nConditioning Rules (QBCR) which allow to revise the belief directly with words\nand linguistic labels and thus avoids the introduction of ad-hoc translations\nof quantitative beliefs into quantitative ones for solving the problem.\n"
  },
  {
    "id": "0709.1167",
    "title": "Using RDF to Model the Structure and Process of Systems",
    "abstract": "  Many systems can be described in terms of networks of discrete elements and\ntheir various relationships to one another. A semantic network, or\nmulti-relational network, is a directed labeled graph consisting of a\nheterogeneous set of entities connected by a heterogeneous set of\nrelationships. Semantic networks serve as a promising general-purpose modeling\nsubstrate for complex systems. Various standardized formats and tools are now\navailable to support practical, large-scale semantic network models. First, the\nResource Description Framework (RDF) offers a standardized semantic network\ndata model that can be further formalized by ontology modeling languages such\nas RDF Schema (RDFS) and the Web Ontology Language (OWL). Second, the recent\nintroduction of highly performant triple-stores (i.e. semantic network\ndatabases) allows semantic network models on the order of $10^9$ edges to be\nefficiently stored and manipulated. RDF and its related technologies are\ncurrently used extensively in the domains of computer science, digital library\nscience, and the biological sciences. This article will provide an introduction\nto RDF/RDFS/OWL and an examination of its suitability to model discrete element\ncomplex systems.\n"
  },
  {
    "id": "0709.1701",
    "title": "Enrichment of Qualitative Beliefs for Reasoning under Uncertainty",
    "abstract": "  This paper deals with enriched qualitative belief functions for reasoning\nunder uncertainty and for combining information expressed in natural language\nthrough linguistic labels. In this work, two possible enrichments (quantitative\nand/or qualitative) of linguistic labels are considered and operators\n(addition, multiplication, division, etc) for dealing with them are proposed\nand explained. We denote them $qe$-operators, $qe$ standing for\n\"qualitative-enriched\" operators. These operators can be seen as a direct\nextension of the classical qualitative operators ($q$-operators) proposed\nrecently in the Dezert-Smarandache Theory of plausible and paradoxist reasoning\n(DSmT). $q$-operators are also justified in details in this paper. The\nquantitative enrichment of linguistic label is a numerical supporting degree in\n$[0,\\infty)$, while the qualitative enrichment takes its values in a finite\nordered set of linguistic values. Quantitative enrichment is less precise than\nqualitative enrichment, but it is expected more close with what human experts\ncan easily provide when expressing linguistic labels with supporting degrees.\nTwo simple examples are given to show how the fusion of qualitative-enriched\nbelief assignments can be done.\n"
  },
  {
    "id": "0709.2065",
    "title": "Toward Psycho-robots",
    "abstract": "  We try to perform geometrization of psychology by representing mental states,\n<<ideas>>, by points of a metric space, <<mental space>>. Evolution of ideas is\ndescribed by dynamical systems in metric mental space. We apply the mental\nspace approach for modeling of flows of unconscious and conscious information\nin the human brain. In a series of models, Models 1-4, we consider cognitive\nsystems with increasing complexity of psychological behavior determined by\nstructure of flows of ideas. Since our models are in fact models of the\nAI-type, one immediately recognizes that they can be used for creation of\nAI-systems, which we call psycho-robots, exhibiting important elements of human\npsyche. Creation of such psycho-robots may be useful improvement of domestic\nrobots. At the moment domestic robots are merely simple working devices (e.g.\nvacuum cleaners or lawn mowers) . However, in future one can expect demand in\nsystems which be able not only perform simple work tasks, but would have\nelements of human self-developing psyche. Such AI-psyche could play an\nimportant role both in relations between psycho-robots and their owners as well\nas between psycho-robots. Since the presence of a huge numbers of\npsycho-complexes is an essential characteristic of human psychology, it would\nbe interesting to model them in the AI-framework.\n"
  },
  {
    "id": "0709.3974",
    "title": "Fitness landscape of the cellular automata majority problem: View from\n  the Olympus",
    "abstract": "  In this paper we study cellular automata (CAs) that perform the computational\nMajority task. This task is a good example of what the phenomenon of emergence\nin complex systems is. We take an interest in the reasons that make this\nparticular fitness landscape a difficult one. The first goal is to study the\nlandscape as such, and thus it is ideally independent from the actual\nheuristics used to search the space. However, a second goal is to understand\nthe features a good search technique for this particular problem space should\npossess. We statistically quantify in various ways the degree of difficulty of\nsearching this landscape. Due to neutrality, investigations based on sampling\ntechniques on the whole landscape are difficult to conduct. So, we go exploring\nthe landscape from the top. Although it has been proved that no CA can perform\nthe task perfectly, several efficient CAs for this task have been found.\nExploiting similarities between these CAs and symmetries in the landscape, we\ndefine the Olympus landscape which is regarded as the ''heavenly home'' of the\nbest local optima known (blok). Then we measure several properties of this\nsubspace. Although it is easier to find relevant CAs in this subspace than in\nthe overall landscape, there are structural reasons that prevent a searcher\nfrom finding overfitted CAs in the Olympus. Finally, we study dynamics and\nperformance of genetic algorithms on the Olympus in order to confirm our\nanalysis and to find efficient CAs for the Majority problem with low\ncomputational cost.\n"
  },
  {
    "id": "0709.4010",
    "title": "Local search heuristics: Fitness Cloud versus Fitness Landscape",
    "abstract": "  This paper introduces the concept of fitness cloud as an alternative way to\nvisualize and analyze search spaces than given by the geographic notion of\nfitness landscape. It is argued that the fitness cloud concept overcomes\nseveral deficiencies of the landscape representation. Our analysis is based on\nthe correlation between fitness of solutions and fitnesses of nearest solutions\naccording to some neighboring. We focus on the behavior of local search\nheuristics, such as hill climber, on the well-known NK fitness landscape. In\nboth cases the fitness vs. fitness correlation is shown to be related to the\nepistatic parameter K.\n"
  },
  {
    "id": "0709.4011",
    "title": "Measuring the Evolvability Landscape to study Neutrality",
    "abstract": "  This theoretical work defines the measure of autocorrelation of evolvability\nin the context of neutral fitness landscape. This measure has been studied on\nthe classical MAX-SAT problem. This work highlight a new characteristic of\nneutral fitness landscapes which allows to design new adapted metaheuristic.\n"
  },
  {
    "id": "0709.4015",
    "title": "From Texts to Structured Documents: The Case of Health Practice\n  Guidelines",
    "abstract": "  This paper describes a system capable of semi-automatically filling an XML\ntemplate from free texts in the clinical domain (practice guidelines). The XML\ntemplate includes semantic information not explicitly encoded in the text\n(pairs of conditions and actions/recommendations). Therefore, there is a need\nto compute the exact scope of conditions over text sequences expressing the\nrequired actions. We present in this paper the rules developed for this task.\nWe show that the system yields good performance when applied to the analysis of\nFrench practice guidelines.\n"
  },
  {
    "id": "0710.0013",
    "title": "Lagrangian Relaxation for MAP Estimation in Graphical Models",
    "abstract": "  We develop a general framework for MAP estimation in discrete and Gaussian\ngraphical models using Lagrangian relaxation techniques. The key idea is to\nreformulate an intractable estimation problem as one defined on a more\ntractable graph, but subject to additional constraints. Relaxing these\nconstraints gives a tractable dual problem, one defined by a thin graph, which\nis then optimized by an iterative procedure. When this iterative optimization\nleads to a consistent estimate, one which also satisfies the constraints, then\nit corresponds to an optimal MAP estimate of the original model. Otherwise\nthere is a ``duality gap'', and we obtain a bound on the optimal solution.\nThus, our approach combines convex optimization with dynamic programming\ntechniques applicable for thin graphs. The popular tree-reweighted max-product\n(TRMP) method may be seen as solving a particular class of such relaxations,\nwhere the intractable graph is relaxed to a set of spanning trees. We also\nconsider relaxations to a set of small induced subgraphs, thin subgraphs (e.g.\nloops), and a connected tree obtained by ``unwinding'' cycles. In addition, we\npropose a new class of multiscale relaxations that introduce ``summary''\nvariables. The potential benefits of such generalizations include: reducing or\neliminating the ``duality gap'' in hard problems, reducing the number or\nLagrange multipliers in the dual problem, and accelerating convergence of the\niterative optimization procedure.\n"
  },
  {
    "id": "0710.4231",
    "title": "Analyzing covert social network foundation behind terrorism disaster",
    "abstract": "  This paper addresses a method to analyze the covert social network foundation\nhidden behind the terrorism disaster. It is to solve a node discovery problem,\nwhich means to discover a node, which functions relevantly in a social network,\nbut escaped from monitoring on the presence and mutual relationship of nodes.\nThe method aims at integrating the expert investigator's prior understanding,\ninsight on the terrorists' social network nature derived from the complex graph\ntheory, and computational data processing. The social network responsible for\nthe 9/11 attack in 2001 is used to execute simulation experiment to evaluate\nthe performance of the method.\n"
  },
  {
    "id": "0710.4975",
    "title": "Node discovery problem for a social network",
    "abstract": "  Methods to solve a node discovery problem for a social network are presented.\nCovert nodes refer to the nodes which are not observable directly. They\ntransmit the influence and affect the resulting collaborative activities among\nthe persons in a social network, but do not appear in the surveillance logs\nwhich record the participants of the collaborative activities. Discovering the\ncovert nodes is identifying the suspicious logs where the covert nodes would\nappear if the covert nodes became overt. The performance of the methods is\ndemonstrated with a test dataset generated from computationally synthesized\nnetworks and a real organization.\n"
  },
  {
    "id": "0711.1466",
    "title": "Predicting relevant empty spots in social interaction",
    "abstract": "  An empty spot refers to an empty hard-to-fill space which can be found in the\nrecords of the social interaction, and is the clue to the persons in the\nunderlying social network who do not appear in the records. This contribution\naddresses a problem to predict relevant empty spots in social interaction.\nHomogeneous and inhomogeneous networks are studied as a model underlying the\nsocial interaction. A heuristic predictor function approach is presented as a\nnew method to address the problem. Simulation experiment is demonstrated over a\nhomogeneous network. A test data in the form of baskets is generated from the\nsimulated communication. Precision to predict the empty spots is calculated to\ndemonstrate the performance of the presented approach.\n"
  },
  {
    "id": "0711.3419",
    "title": "Translating OWL and Semantic Web Rules into Prolog: Moving Toward\n  Description Logic Programs",
    "abstract": "  To appear in Theory and Practice of Logic Programming (TPLP), 2008.\n  We are researching the interaction between the rule and the ontology layers\nof the Semantic Web, by comparing two options: 1) using OWL and its rule\nextension SWRL to develop an integrated ontology/rule language, and 2) layering\nrules on top of an ontology with RuleML and OWL. Toward this end, we are\ndeveloping the SWORIER system, which enables efficient automated reasoning on\nontologies and rules, by translating all of them into Prolog and adding a set\nof general rules that properly capture the semantics of OWL. We have also\nenabled the user to make dynamic changes on the fly, at run time. This work\naddresses several of the concerns expressed in previous work, such as negation,\ncomplementary classes, disjunctive heads, and cardinality, and it discusses\nalternative approaches for dealing with inconsistencies in the knowledge base.\nIn addition, for efficiency, we implemented techniques called\nextensionalization, avoiding reanalysis, and code minimization.\n"
  },
  {
    "id": "0712.0836",
    "title": "Evolving localizations in reaction-diffusion cellular automata",
    "abstract": "  We consider hexagonal cellular automata with immediate cell neighbourhood and\nthree cell-states. Every cell calculates its next state depending on the\nintegral representation of states in its neighbourhood, i.e. how many\nneighbours are in each one state. We employ evolutionary algorithms to breed\nlocal transition functions that support mobile localizations (gliders), and\ncharacterize sets of the functions selected in terms of quasi-chemical systems.\nAnalysis of the set of functions evolved allows to speculate that mobile\nlocalizations are likely to emerge in the quasi-chemical systems with limited\ndiffusion of one reagent, a small number of molecules is required for\namplification of travelling localizations, and reactions leading to stationary\nlocalizations involve relatively equal amount of quasi-chemical species.\nTechniques developed can be applied in cascading signals in nature-inspired\nspatially extended computing devices, and phenomenological studies and\nclassification of non-linear discrete systems.\n"
  },
  {
    "id": "0712.2389",
    "title": "Decomposition During Search for Propagation-Based Constraint Solvers",
    "abstract": "  We describe decomposition during search (DDS), an integration of And/Or tree\nsearch into propagation-based constraint solvers. The presented search\nalgorithm dynamically decomposes sub-problems of a constraint satisfaction\nproblem into independent partial problems, avoiding redundant work.\n  The paper discusses how DDS interacts with key features that make\npropagation-based solvers successful: constraint propagation, especially for\nglobal constraints, and dynamic search heuristics.\n  We have implemented DDS for the Gecode constraint programming library. Two\napplications, solution counting in graph coloring and protein structure\nprediction, exemplify the benefits of DDS in practice.\n"
  },
  {
    "id": "0712.3329",
    "title": "Universal Intelligence: A Definition of Machine Intelligence",
    "abstract": "  A fundamental problem in artificial intelligence is that nobody really knows\nwhat intelligence is. The problem is especially acute when we need to consider\nartificial systems which are significantly different to humans. In this paper\nwe approach this problem in the following way: We take a number of well known\ninformal definitions of human intelligence that have been given by experts, and\nextract their essential features. These are then mathematically formalised to\nproduce a general measure of intelligence for arbitrary machines. We believe\nthat this equation formally captures the concept of machine intelligence in the\nbroadest reasonable sense. We then show how this formal definition is related\nto the theory of universal optimal learning agents. Finally, we survey the many\nother tests and definitions of intelligence that have been proposed for\nmachines.\n"
  },
  {
    "id": "0712.3825",
    "title": "Tests of Machine Intelligence",
    "abstract": "  Although the definition and measurement of intelligence is clearly of\nfundamental importance to the field of artificial intelligence, no general\nsurvey of definitions and tests of machine intelligence exists. Indeed few\nresearchers are even aware of alternatives to the Turing test and its many\nderivatives. In this paper we fill this gap by providing a short survey of the\nmany tests of machine intelligence that have been proposed.\n"
  },
  {
    "id": "0712.4318",
    "title": "Convergence of Expected Utilities with Algorithmic Probability\n  Distributions",
    "abstract": "  We consider an agent interacting with an unknown environment. The environment\nis a function which maps natural numbers to natural numbers; the agent's set of\nhypotheses about the environment contains all such functions which are\ncomputable and compatible with a finite set of known input-output pairs, and\nthe agent assigns a positive probability to each such hypothesis. We do not\nrequire that this probability distribution be computable, but it must be\nbounded below by a positive computable function. The agent has a utility\nfunction on outputs from the environment. We show that if this utility function\nis bounded below in absolute value by an unbounded computable function, then\nthe expected utility of any input is undefined. This implies that a computable\nutility function will have convergent expected utilities iff that function is\nbounded.\n"
  },
  {
    "id": "0801.1275",
    "title": "Le terme et le concept : fondements d'une ontoterminologie",
    "abstract": "  Most definitions of ontology, viewed as a \"specification of a\nconceptualization\", agree on the fact that if an ontology can take different\nforms, it necessarily includes a vocabulary of terms and some specification of\ntheir meaning in relation to the domain's conceptualization. And as domain\nknowledge is mainly conveyed through scientific and technical texts, we can\nhope to extract some useful information from them for building ontology. But is\nit as simple as this? In this article we shall see that the lexical structure,\ni.e. the network of words linked by linguistic relationships, does not\nnecessarily match the domain conceptualization. We have to bear in mind that\nwriting documents is the concern of textual linguistics, of which one of the\nprinciples is the incompleteness of text, whereas building ontology - viewed as\ntask-independent knowledge - is concerned with conceptualization based on\nformal and not natural languages. Nevertheless, the famous Sapir and Whorf\nhypothesis, concerning the interdependence of thought and language, is also\napplicable to formal languages. This means that the way an ontology is built\nand a concept is defined depends directly on the formal language which is used;\nand the results will not be the same. The introduction of the notion of\nontoterminology allows to take into account epistemological principles for\nformal ontology building.\n"
  },
  {
    "id": "0801.1336",
    "title": "Stream Computing",
    "abstract": "  Stream computing is the use of multiple autonomic and parallel modules\ntogether with integrative processors at a higher level of abstraction to embody\n\"intelligent\" processing. The biological basis of this computing is sketched\nand the matter of learning is examined.\n"
  },
  {
    "id": "0802.2429",
    "title": "Anisotropic selection in cellular genetic algorithms",
    "abstract": "  In this paper we introduce a new selection scheme in cellular genetic\nalgorithms (cGAs). Anisotropic Selection (AS) promotes diversity and allows\naccurate control of the selective pressure. First we compare this new scheme\nwith the classical rectangular grid shapes solution according to the selective\npressure: we can obtain the same takeover time with the two techniques although\nthe spreading of the best individual is different. We then give experimental\nresults that show to what extent AS promotes the emergence of niches that\nsupport low coupling and high cohesion. Finally, using a cGA with anisotropic\nselection on a Quadratic Assignment Problem we show the existence of an\nanisotropic optimal value for which the best average performance is observed.\nFurther work will focus on the selective pressure self-adjustment ability\nprovided by this new selection scheme.\n"
  },
  {
    "id": "0803.1087",
    "title": "The Future of Scientific Simulations: from Artificial Life to Artificial\n  Cosmogenesis",
    "abstract": "  This philosophical paper explores the relation between modern scientific\nsimulations and the future of the universe. We argue that a simulation of an\nentire universe will result from future scientific activity. This requires us\nto tackle the challenge of simulating open-ended evolution at all levels in a\nsingle simulation. The simulation should encompass not only biological\nevolution, but also physical evolution (a level below) and cultural evolution\n(a level above). The simulation would allow us to probe what would happen if we\nwould \"replay the tape of the universe\" with the same or different laws and\ninitial conditions. We also distinguish between real-world and artificial-world\nmodelling. Assuming that intelligent life could indeed simulate an entire\nuniverse, this leads to two tentative hypotheses. Some authors have argued that\nwe may already be in a simulation run by an intelligent entity. Or, if such a\nsimulation could be made real, this would lead to the production of a new\nuniverse. This last direction is argued with a careful speculative\nphilosophical approach, emphasizing the imperative to find a solution to the\nheat death problem in cosmology. The reader is invited to consult Annex 1 for\nan overview of the logical structure of this paper. -- Keywords: far future,\nfuture of science, ALife, simulation, realization, cosmology, heat death,\nfine-tuning, physical eschatology, cosmological natural selection, cosmological\nartificial selection, artificial cosmogenesis, selfish biocosm hypothesis,\nmeduso-anthropic principle, developmental singularity hypothesis, role of\nintelligent life.\n"
  },
  {
    "id": "0803.1207",
    "title": "Serious Flaws in Korf et al.'s Analysis on Time Complexity of A*",
    "abstract": "  This paper has been withdrawn.\n"
  },
  {
    "id": "0803.3192",
    "title": "Eye-Tracking Evolutionary Algorithm to minimize user's fatigue in IEC\n  applied to Interactive One-Max problem",
    "abstract": "  In this paper, we describe a new algorithm that consists in combining an\neye-tracker for minimizing the fatigue of a user during the evaluation process\nof Interactive Evolutionary Computation. The approach is then applied to the\nInteractive One-Max optimization problem.\n"
  },
  {
    "id": "0803.3363",
    "title": "Node discovery in a networked organization",
    "abstract": "  In this paper, I present a method to solve a node discovery problem in a\nnetworked organization. Covert nodes refer to the nodes which are not\nobservable directly. They affect social interactions, but do not appear in the\nsurveillance logs which record the participants of the social interactions.\nDiscovering the covert nodes is defined as identifying the suspicious logs\nwhere the covert nodes would appear if the covert nodes became overt. A\nmathematical model is developed for the maximal likelihood estimation of the\nnetwork behind the social interactions and for the identification of the\nsuspicious logs. Precision, recall, and F measure characteristics are\ndemonstrated with the dataset generated from a real organization and the\ncomputationally synthesized datasets. The performance is close to the\ntheoretical limit for any covert nodes in the networks of any topologies and\nsizes if the ratio of the number of observation to the number of possible\ncommunication patterns is large.\n"
  },
  {
    "id": "0803.3501",
    "title": "Multiagent Approach for the Representation of Information in a Decision\n  Support System",
    "abstract": "  In an emergency situation, the actors need an assistance allowing them to\nreact swiftly and efficiently. In this prospect, we present in this paper a\ndecision support system that aims to prepare actors in a crisis situation\nthanks to a decision-making support. The global architecture of this system is\npresented in the first part. Then we focus on a part of this system which is\ndesigned to represent the information of the current situation. This part is\ncomposed of a multiagent system that is made of factual agents. Each agent\ncarries a semantic feature and aims to represent a partial part of a situation.\nThe agents develop thanks to their interactions by comparing their semantic\nfeatures using proximity measures and according to specific ontologies.\n"
  },
  {
    "id": "0803.4074",
    "title": "Reflective visualization and verbalization of unconscious preference",
    "abstract": "  A new method is presented, that can help a person become aware of his or her\nunconscious preferences, and convey them to others in the form of verbal\nexplanation. The method combines the concepts of reflection, visualization, and\nverbalization. The method was tested in an experiment where the unconscious\npreferences of the subjects for various artworks were investigated. In the\nexperiment, two lessons were learned. The first is that it helps the subjects\nbecome aware of their unconscious preferences to verbalize weak preferences as\ncompared with strong preferences through discussion over preference diagrams.\nThe second is that it is effective to introduce an adjustable factor into\nvisualization to adapt to the differences in the subjects and to foster their\nmutual understanding.\n"
  },
  {
    "id": "0804.0528",
    "title": "Application of Rough Set Theory to Analysis of Hydrocyclone Operation",
    "abstract": "  This paper describes application of rough set theory, on the analysis of\nhydrocyclone operation. In this manner, using Self Organizing Map (SOM) as\npreprocessing step, best crisp granules of data are obtained. Then, using a\ncombining of SOM and rough set theory (RST)-called SORST-, the dominant rules\non the information table, obtained from laboratory tests, are extracted. Based\non these rules, an approximate estimation on decision attribute is fulfilled.\nFinally, a brief comparison of this method with the SOM-NFIS system (briefly\nSONFIS) is highlighted.\n"
  },
  {
    "id": "0804.0558",
    "title": "Agent-Based Perception of an Environment in an Emergency Situation",
    "abstract": "  We are interested in the problem of multiagent systems development for risk\ndetecting and emergency response in an uncertain and partially perceived\nenvironment. The evaluation of the current situation passes by three stages\ninside the multiagent system. In a first time, the situation is represented in\na dynamic way. The second step, consists to characterise the situation and\nfinally, it is compared with other similar known situations. In this paper, we\npresent an information modelling of an observed environment, that we have\napplied on the RoboCupRescue Simulation System. Information coming from the\nenvironment are formatted according to a taxonomy and using semantic features.\nThe latter are defined thanks to a fine ontology of the domain and are managed\nby factual agents that aim to represent dynamically the current situation.\n"
  },
  {
    "id": "0804.0852",
    "title": "On the Influence of Selection Operators on Performances in Cellular\n  Genetic Algorithms",
    "abstract": "  In this paper, we study the influence of the selective pressure on the\nperformance of cellular genetic algorithms. Cellular genetic algorithms are\ngenetic algorithms where the population is embedded on a toroidal grid. This\nstructure makes the propagation of the best so far individual slow down, and\nallows to keep in the population potentially good solutions. We present two\nselective pressure reducing strategies in order to slow down even more the best\nsolution propagation. We experiment these strategies on a hard optimization\nproblem, the quadratic assignment problem, and we show that there is a value\nfor of the control parameter for both which gives the best performance. This\noptimal value does not find explanation on only the selective pressure,\nmeasured either by take over time and diversity evolution. This study makes us\nconclude that we need other tools than the sole selective pressure measures to\nexplain the performances of cellular genetic algorithms.\n"
  },
  {
    "id": "0804.1244",
    "title": "Geometric Data Analysis, From Correspondence Analysis to Structured Data\n  Analysis (book review)",
    "abstract": "  Review of: Brigitte Le Roux and Henry Rouanet, Geometric Data Analysis, From\nCorrespondence Analysis to Structured Data Analysis, Kluwer, Dordrecht, 2004,\nxi+475 pp.\n"
  },
  {
    "id": "0805.0459",
    "title": "Phase transition in SONFIS&SORST",
    "abstract": "  In this study, we introduce general frame of MAny Connected Intelligent\nParticles Systems (MACIPS). Connections and interconnections between particles\nget a complex behavior of such merely simple system (system in\nsystem).Contribution of natural computing, under information granulation\ntheory, are the main topics of this spacious skeleton. Upon this clue, we\norganize two algorithms involved a few prominent intelligent computing and\napproximate reasoning methods: self organizing feature map (SOM), Neuro- Fuzzy\nInference System and Rough Set Theory (RST). Over this, we show how our\nalgorithms can be taken as a linkage of government-society interaction, where\ngovernment catches various fashions of behavior: solid (absolute) or flexible.\nSo, transition of such society, by changing of connectivity parameters (noise)\nfrom order to disorder is inferred. Add to this, one may find an indirect\nmapping among financial systems and eventual market fluctuations with MACIPS.\nKeywords: phase transition, SONFIS, SORST, many connected intelligent particles\nsystem, society-government interaction\n"
  },
  {
    "id": "0805.1096",
    "title": "Adaptive Affinity Propagation Clustering",
    "abstract": "  Affinity propagation clustering (AP) has two limitations: it is hard to know\nwhat value of parameter 'preference' can yield an optimal clustering solution,\nand oscillations cannot be eliminated automatically if occur. The adaptive AP\nmethod is proposed to overcome these limitations, including adaptive scanning\nof preferences to search space of the number of clusters for finding the\noptimal clustering solution, adaptive adjustment of damping factors to\neliminate oscillations, and adaptive escaping from oscillations when the\ndamping adjustment technique fails. Experimental results on simulated and real\ndata sets show that the adaptive AP is effective and can outperform AP in\nquality of clustering results.\n"
  },
  {
    "id": "0805.1288",
    "title": "Assessment of effective parameters on dilution using approximate\n  reasoning methods in longwall mining method, Iran coal mines",
    "abstract": "  Approximately more than 90% of all coal production in Iranian underground\nmines is derived directly longwall mining method. Out of seam dilution is one\nof the essential problems in these mines. Therefore the dilution can impose the\nadditional cost of mining and milling. As a result, recognition of the\neffective parameters on the dilution has a remarkable role in industry. In this\nway, this paper has analyzed the influence of 13 parameters (attributed\nvariables) versus the decision attribute (dilution value), so that using two\napproximate reasoning methods, namely Rough Set Theory (RST) and Self\nOrganizing Neuro- Fuzzy Inference System (SONFIS) the best rules on our\ncollected data sets has been extracted. The other benefit of later methods is\nto predict new unknown cases. So, the reduced sets (reducts) by RST have been\nobtained. Therefore the emerged results by utilizing mentioned methods shows\nthat the high sensitive variables are thickness of layer, length of stope, rate\nof advance, number of miners, type of advancing.\n"
  },
  {
    "id": "0805.2308",
    "title": "Toward Fuzzy block theory",
    "abstract": "  This study, fundamentals of fuzzy block theory, and its application in\nassessment of stability in underground openings, has surveyed. Using fuzzy\ntopics and inserting them in to key block theory, in two ways, fundamentals of\nfuzzy block theory has been presented. In indirect combining, by coupling of\nadaptive Neuro Fuzzy Inference System (NFIS) and classic block theory, we could\nextract possible damage parts around a tunnel. In direct solution, some\nprinciples of block theory, by means of different fuzzy facets theory, were\nrewritten.\n"
  },
  {
    "id": "0805.2440",
    "title": "Analysis of hydrocyclone performance based on information granulation\n  theory",
    "abstract": "  This paper describes application of information granulation theory, on the\nanalysis of hydrocyclone perforamance. In this manner, using a combining of\nSelf Organizing Map (SOM) and Neuro-Fuzzy Inference System (NFIS), crisp and\nfuzzy granules are obtained(briefly called SONFIS). Balancing of crisp granules\nand sub fuzzy granules, within non fuzzy information (initial granulation), is\nrendered in an open-close iteration. Using two criteria, \"simplicity of rules\n\"and \"adaptive threoshold error level\", stability of algorithm is guaranteed.\nValidation of the proposed method, on the data set of the hydrocyclone is\nrendered.\n"
  },
  {
    "id": "0805.3518",
    "title": "Logic programming with social features",
    "abstract": "  In everyday life it happens that a person has to reason about what other\npeople think and how they behave, in order to achieve his goals. In other\nwords, an individual may be required to adapt his behaviour by reasoning about\nthe others' mental state. In this paper we focus on a knowledge representation\nlanguage derived from logic programming which both supports the representation\nof mental states of individual communities and provides each with the\ncapability of reasoning about others' mental states and acting accordingly. The\nproposed semantics is shown to be translatable into stable model semantics of\nlogic programs with aggregates.\n"
  },
  {
    "id": "0805.3747",
    "title": "Constructing Folksonomies from User-specified Relations on Flickr",
    "abstract": "  Many social Web sites allow users to publish content and annotate with\ndescriptive metadata. In addition to flat tags, some social Web sites have\nrecently began to allow users to organize their content and metadata\nhierarchically. The social photosharing site Flickr, for example, allows users\nto group related photos in sets, and related sets in collections. The social\nbookmarking site Del.icio.us similarly lets users group related tags into\nbundles. Although the sites themselves don't impose any constraints on how\nthese hierarchies are used, individuals generally use them to capture\nrelationships between concepts, most commonly the broader/narrower relations.\nCollective annotation of content with hierarchical relations may lead to an\nemergent classification system, called a folksonomy. While some researchers\nhave explored using tags as evidence for learning folksonomies, we believe that\nhierarchical relations described above offer a high-quality source of evidence\nfor this task.\n  We propose a simple approach to aggregate shallow hierarchies created by many\ndistinct Flickr users into a common folksonomy. Our approach uses statistics to\ndetermine if a particular relation should be retained or discarded. The\nrelations are then woven together into larger hierarchies. Although we have not\ncarried out a detailed quantitative evaluation of the approach, it looks very\npromising since it generates very reasonable, non-trivial hierarchies.\n"
  },
  {
    "id": "0805.3799",
    "title": "The Structure of Narrative: the Case of Film Scripts",
    "abstract": "  We analyze the style and structure of story narrative using the case of film\nscripts. The practical importance of this is noted, especially the need to have\nsupport tools for television movie writing. We use the Casablanca film script,\nand scripts from six episodes of CSI (Crime Scene Investigation). For analysis\nof style and structure, we quantify various central perspectives discussed in\nMcKee's book, \"Story: Substance, Structure, Style, and the Principles of\nScreenwriting\". Film scripts offer a useful point of departure for exploration\nof the analysis of more general narratives. Our methodology, using\nCorrespondence Analysis, and hierarchical clustering, is innovative in a range\nof areas that we discuss. In particular this work is groundbreaking in taking\nthe qualitative analysis of McKee and grounding this analysis in a quantitative\nand algorithmic framework.\n"
  },
  {
    "id": "0805.3802",
    "title": "Feature Selection for Bayesian Evaluation of Trauma Death Risk",
    "abstract": "  In the last year more than 70,000 people have been brought to the UK\nhospitals with serious injuries. Each time a clinician has to urgently take a\npatient through a screening procedure to make a reliable decision on the trauma\ntreatment. Typically, such procedure comprises around 20 tests; however the\ncondition of a trauma patient remains very difficult to be tested properly.\nWhat happens if these tests are ambiguously interpreted, and information about\nthe severity of the injury will come misleading? The mistake in a decision can\nbe fatal: using a mild treatment can put a patient at risk of dying from\nposttraumatic shock, while using an overtreatment can also cause death. How can\nwe reduce the risk of the death caused by unreliable decisions? It has been\nshown that probabilistic reasoning, based on the Bayesian methodology of\naveraging over decision models, allows clinicians to evaluate the uncertainty\nin decision making. Based on this methodology, in this paper we aim at\nselecting the most important screening tests, keeping a high performance. We\nassume that the probabilistic reasoning within the Bayesian methodology allows\nus to discover new relationships between the screening tests and uncertainty in\ndecisions. In practice, selection of the most informative tests can also reduce\nthe cost of a screening procedure in trauma care centers. In our experiments we\nuse the UK Trauma data to compare the efficiency of the proposed technique in\nterms of the performance. We also compare the uncertainty in decisions in terms\nof entropy.\n"
  },
  {
    "id": "0805.3935",
    "title": "Fusion for Evaluation of Image Classification in Uncertain Environments",
    "abstract": "  We present in this article a new evaluation method for classification and\nsegmentation of textured images in uncertain environments. In uncertain\nenvironments, real classes and boundaries are known with only a partial\ncertainty given by the experts. Most of the time, in many presented papers,\nonly classification or only segmentation are considered and evaluated. Here, we\npropose to take into account both the classification and segmentation results\naccording to the certainty given by the experts. We present the results of this\nmethod on a fusion of classifiers of sonar images for a seabed\ncharacterization.\n"
  },
  {
    "id": "0805.3972",
    "title": "Intuitive visualization of the intelligence for the run-down of\n  terrorist wire-pullers",
    "abstract": "  The investigation of the terrorist attack is a time-critical task. The\ninvestigators have a limited time window to diagnose the organizational\nbackground of the terrorists, to run down and arrest the wire-pullers, and to\ntake an action to prevent or eradicate the terrorist attack. The intuitive\ninterface to visualize the intelligence data set stimulates the investigators'\nexperience and knowledge, and aids them in decision-making for an immediately\neffective action. This paper presents a computational method to analyze the\nintelligence data set on the collective actions of the perpetrators of the\nattack, and to visualize it into the form of a social network diagram which\npredicts the positions where the wire-pullers conceals themselves.\n"
  },
  {
    "id": "0805.4560",
    "title": "Rock mechanics modeling based on soft granulation theory",
    "abstract": "  This paper describes application of information granulation theory, on the\ndesign of rock engineering flowcharts. Firstly, an overall flowchart, based on\ninformation granulation theory has been highlighted. Information granulation\ntheory, in crisp (non-fuzzy) or fuzzy format, can take into account engineering\nexperiences (especially in fuzzy shape-incomplete information or superfluous),\nor engineering judgments, in each step of designing procedure, while the\nsuitable instruments modeling are employed. In this manner and to extension of\nsoft modeling instruments, using three combinations of Self Organizing Map\n(SOM), Neuro-Fuzzy Inference System (NFIS), and Rough Set Theory (RST) crisp\nand fuzzy granules, from monitored data sets are obtained. The main underlined\ncore of our algorithms are balancing of crisp(rough or non-fuzzy) granules and\nsub fuzzy granules, within non fuzzy information (initial granulation) upon the\nopen-close iterations. Using different criteria on balancing best granules\n(information pockets), are obtained. Validations of our proposed methods, on\nthe data set of in-situ permeability in rock masses in Shivashan dam, Iran have\nbeen highlighted.\n"
  },
  {
    "id": "0806.0526",
    "title": "An Ontology-based Knowledge Management System for Industry Clusters",
    "abstract": "  Knowledge-based economy forces companies in the nation to group together as a\ncluster in order to maintain their competitiveness in the world market. The\ncluster development relies on two key success factors which are knowledge\nsharing and collaboration between the actors in the cluster. Thus, our study\ntries to propose knowledge management system to support knowledge management\nactivities within the cluster. To achieve the objectives of this study,\nontology takes a very important role in knowledge management process in various\nways; such as building reusable and faster knowledge-bases, better way for\nrepresenting the knowledge explicitly. However, creating and representing\nontology create difficulties to organization due to the ambiguity and\nunstructured of source of knowledge. Therefore, the objectives of this paper\nare to propose the methodology to create and represent ontology for the\norganization development by using knowledge engineering approach. The\nhandicraft cluster in Thailand is used as a case study to illustrate our\nproposed methodology.\n"
  },
  {
    "id": "0806.1280",
    "title": "The Role of Artificial Intelligence Technologies in Crisis Response",
    "abstract": "  Crisis response poses many of the most difficult information technology in\ncrisis management. It requires information and communication-intensive efforts,\nutilized for reducing uncertainty, calculating and comparing costs and\nbenefits, and managing resources in a fashion beyond those regularly available\nto handle routine problems. In this paper, we explore the benefits of\nartificial intelligence technologies in crisis response. This paper discusses\nthe role of artificial intelligence technologies; namely, robotics, ontology\nand semantic web, and multi-agent systems in crisis response.\n"
  },
  {
    "id": "0806.1640",
    "title": "Toward a combination rule to deal with partial conflict and specificity\n  in belief functions theory",
    "abstract": "  We present and discuss a mixed conjunctive and disjunctive rule, a\ngeneralization of conflict repartition rules, and a combination of these two\nrules. In the belief functions theory one of the major problem is the conflict\nrepartition enlightened by the famous Zadeh's example. To date, many\ncombination rules have been proposed in order to solve a solution to this\nproblem. Moreover, it can be important to consider the specificity of the\nresponses of the experts. Since few year some unification rules are proposed.\nWe have shown in our previous works the interest of the proportional conflict\nredistribution rule. We propose here a mixed combination rule following the\nproportional conflict redistribution rule modified by a discounting procedure.\nThis rule generalizes many combination rules.\n"
  },
  {
    "id": "0806.1797",
    "title": "A new generalization of the proportional conflict redistribution rule\n  stable in terms of decision",
    "abstract": "  In this chapter, we present and discuss a new generalized proportional\nconflict redistribution rule. The Dezert-Smarandache extension of the\nDemster-Shafer theory has relaunched the studies on the combination rules\nespecially for the management of the conflict. Many combination rules have been\nproposed in the last few years. We study here different combination rules and\ncompare them in terms of decision on didactic example and on generated data.\nIndeed, in real applications, we need a reliable decision and it is the final\nresults that matter. This chapter shows that a fine proportional conflict\nredistribution rule must be preferred for the combination in the belief\nfunction theory.\n"
  },
  {
    "id": "0806.1802",
    "title": "Une nouvelle r\\`egle de combinaison r\\'epartissant le conflit -\n  Applications en imagerie Sonar et classification de cibles Radar",
    "abstract": "  These last years, there were many studies on the problem of the conflict\ncoming from information combination, especially in evidence theory. We can\nsummarise the solutions for manage the conflict into three different\napproaches: first, we can try to suppress or reduce the conflict before the\ncombination step, secondly, we can manage the conflict in order to give no\ninfluence of the conflict in the combination step, and then take into account\nthe conflict in the decision step, thirdly, we can take into account the\nconflict in the combination step. The first approach is certainly the better,\nbut not always feasible. It is difficult to say which approach is the best\nbetween the second and the third. However, the most important is the produced\nresults in applications. We propose here a new combination rule that\ndistributes the conflict proportionally on the element given this conflict. We\ncompare these different combination rules on real data in Sonar imagery and\nRadar target classification.\n"
  },
  {
    "id": "0806.1806",
    "title": "Perfect Derived Propagators",
    "abstract": "  When implementing a propagator for a constraint, one must decide about\nvariants: When implementing min, should one also implement max? Should one\nimplement linear equations both with and without coefficients? Constraint\nvariants are ubiquitous: implementing them requires considerable (if not\nprohibitive) effort and decreases maintainability, but will deliver better\nperformance.\n  This paper shows how to use variable views, previously introduced for an\nimplementation architecture, to derive perfect propagator variants. A model for\nviews and derived propagators is introduced. Derived propagators are proved to\nbe indeed perfect in that they inherit essential properties such as correctness\nand domain and bounds consistency. Techniques for systematically deriving\npropagators such as transformation, generalization, specialization, and\nchanneling are developed for several variable domains. We evaluate the massive\nimpact of derived propagators. Without derived propagators, Gecode would\nrequire 140000 rather than 40000 lines of code for propagators.\n"
  },
  {
    "id": "0806.2140",
    "title": "Defaults and Normality in Causal Structures",
    "abstract": "  A serious defect with the Halpern-Pearl (HP) definition of causality is\nrepaired by combining a theory of causality with a theory of defaults. In\naddition, it is shown that (despite a claim to the contrary) a cause according\nto the HP condition need not be a single conjunct. A definition of causality\nmotivated by Wright's NESS test is shown to always hold for a single conjunct.\nMoreover, conditions that hold for all the examples considered by HP are given\nthat guarantee that causality according to (this version) of the NESS test is\nequivalent to the HP definition.\n"
  },
  {
    "id": "0806.4511",
    "title": "The model of quantum evolution",
    "abstract": "  This paper has been withdrawn by the author due to extremely unscientific\nerrors.\n"
  },
  {
    "id": "0807.0627",
    "title": "Belief decision support and reject for textured images characterization",
    "abstract": "  The textured images' classification assumes to consider the images in terms\nof area with the same texture. In uncertain environment, it could be better to\ntake an imprecise decision or to reject the area corresponding to an unlearning\nclass. Moreover, on the areas that are the classification units, we can have\nmore than one texture. These considerations allows us to develop a belief\ndecision model permitting to reject an area as unlearning and to decide on\nunions and intersections of learning classes. The proposed approach finds all\nits justification in an application of seabed characterization from sonar\nimages, which contributes to an illustration.\n"
  },
  {
    "id": "0807.0908",
    "title": "The Correspondence Analysis Platform for Uncovering Deep Structure in\n  Data and Information",
    "abstract": "  We study two aspects of information semantics: (i) the collection of all\nrelationships, (ii) tracking and spotting anomaly and change. The first is\nimplemented by endowing all relevant information spaces with a Euclidean metric\nin a common projected space. The second is modelled by an induced ultrametric.\nA very general way to achieve a Euclidean embedding of different information\nspaces based on cross-tabulation counts (and from other input data formats) is\nprovided by Correspondence Analysis. From there, the induced ultrametric that\nwe are particularly interested in takes a sequential - e.g. temporal - ordering\nof the data into account. We employ such a perspective to look at narrative,\n\"the flow of thought and the flow of language\" (Chafe). In application to\npolicy decision making, we show how we can focus analysis in a small number of\ndimensions.\n"
  },
  {
    "id": "0807.1906",
    "title": "Extension of Inagaki General Weighted Operators and A New Fusion Rule\n  Class of Proportional Redistribution of Intersection Masses",
    "abstract": "  In this paper we extend Inagaki Weighted Operators fusion rule (WO) in\ninformation fusion by doing redistribution of not only the conflicting mass,\nbut also of masses of non-empty intersections, that we call Double Weighted\nOperators (DWO). Then we propose a new fusion rule Class of Proportional\nRedistribution of Intersection Masses (CPRIM), which generates many interesting\nparticular fusion rules in information fusion. Both formulas are presented for\nany number of sources of information. An application and comparison with other\nfusion rules are given in the last section.\n"
  },
  {
    "id": "0807.3483",
    "title": "Implementing general belief function framework with a practical\n  codification for low complexity",
    "abstract": "  In this chapter, we propose a new practical codification of the elements of\nthe Venn diagram in order to easily manipulate the focal elements. In order to\nreduce the complexity, the eventual constraints must be integrated in the\ncodification at the beginning. Hence, we only consider a reduced hyper power\nset $D_r^\\Theta$ that can be $2^\\Theta$ or $D^\\Theta$. We describe all the\nsteps of a general belief function framework. The step of decision is\nparticularly studied, indeed, when we can decide on intersections of the\nsingletons of the discernment space no actual decision functions are easily to\nuse. Hence, two approaches are proposed, an extension of previous one and an\napproach based on the specificity of the elements on which to decide. The\nprincipal goal of this chapter is to provide practical codes of a general\nbelief function framework for the researchers and users needing the belief\nfunction theory.\n"
  },
  {
    "id": "0807.3669",
    "title": "A new probabilistic transformation of belief mass assignment",
    "abstract": "  In this paper, we propose in Dezert-Smarandache Theory (DSmT) framework, a\nnew probabilistic transformation, called DSmP, in order to build a subjective\nprobability measure from any basic belief assignment defined on any model of\nthe frame of discernment. Several examples are given to show how the DSmP\ntransformation works and we compare it to main existing transformations\nproposed in the literature so far. We show the advantages of DSmP over\nclassical transformations in term of Probabilistic Information Content (PIC).\nThe direct extension of this transformation for dealing with qualitative belief\nassignments is also presented.\n"
  },
  {
    "id": "0807.4417",
    "title": "On Introspection, Metacognitive Control and Augmented Data Mining Live\n  Cycles",
    "abstract": "  We discuss metacognitive modelling as an enhancement to cognitive modelling\nand computing. Metacognitive control mechanisms should enable AI systems to\nself-reflect, reason about their actions, and to adapt to new situations. In\nthis respect, we propose implementation details of a knowledge taxonomy and an\naugmented data mining life cycle which supports a live integration of obtained\nmodels.\n"
  },
  {
    "id": "0807.4680",
    "title": "Hacia una teoria de unificacion para los comportamientos cognitivos",
    "abstract": "  Each cognitive science tries to understand a set of cognitive behaviors. The\nstructuring of knowledge of this nature's aspect is far from what it can be\nexpected about a science. Until now universal standard consistently describing\nthe set of cognitive behaviors has not been found, and there are many questions\nabout the cognitive behaviors for which only there are opinions of members of\nthe scientific community. This article has three proposals. The first proposal\nis to raise to the scientific community the necessity of unified the cognitive\nbehaviors. The second proposal is claim the application of the Newton's\nreasoning rules about nature of his book, Philosophiae Naturalis Principia\nMathematica, to the cognitive behaviors. The third is to propose a scientific\ntheory, currently developing, that follows the rules established by Newton to\nmake sense of nature, and could be the theory to explain all the cognitive\nbehaviors.\n"
  },
  {
    "id": "0808.1125",
    "title": "Verified Null-Move Pruning",
    "abstract": "  In this article we review standard null-move pruning and introduce our\nextended version of it, which we call verified null-move pruning. In verified\nnull-move pruning, whenever the shallow null-move search indicates a fail-high,\ninstead of cutting off the search from the current node, the search is\ncontinued with reduced depth.\n  Our experiments with verified null-move pruning show that on average, it\nconstructs a smaller search tree with greater tactical strength in comparison\nto standard null-move pruning. Moreover, unlike standard null-move pruning,\nwhich fails badly in zugzwang positions, verified null-move pruning manages to\ndetect most zugzwangs and in such cases conducts a re-search to obtain the\ncorrect result. In addition, verified null-move pruning is very easy to\nimplement, and any standard null-move pruning program can use verified\nnull-move pruning by modifying only a few lines of code.\n"
  },
  {
    "id": "0808.3109",
    "title": "n-ary Fuzzy Logic and Neutrosophic Logic Operators",
    "abstract": "  We extend Knuth's 16 Boolean binary logic operators to fuzzy logic and\nneutrosophic logic binary operators. Then we generalize them to n-ary fuzzy\nlogic and neutrosophic logic operators using the smarandache codification of\nthe Venn diagram and a defined vector neutrosophic law. In such way, new\noperators in neutrosophic logic/set/probability are built.\n"
  },
  {
    "id": "0809.0271",
    "title": "Randomised Variable Neighbourhood Search for Multi Objective\n  Optimisation",
    "abstract": "  Various local search approaches have recently been applied to machine\nscheduling problems under multiple objectives. Their foremost consideration is\nthe identification of the set of Pareto optimal alternatives. An important\naspect of successfully solving these problems lies in the definition of an\nappropriate neighbourhood structure. Unclear in this context remains, how\ninterdependencies within the fitness landscape affect the resolution of the\nproblem.\n  The paper presents a study of neighbourhood search operators for multiple\nobjective flow shop scheduling. Experiments have been carried out with twelve\ndifferent combinations of criteria. To derive exact conclusions, small problem\ninstances, for which the optimal solutions are known, have been chosen.\nStatistical tests show that no single neighbourhood operator is able to equally\nidentify all Pareto optimal alternatives. Significant improvements however have\nbeen obtained by hybridising the solution algorithm using a randomised variable\nneighbourhood search technique.\n"
  },
  {
    "id": "0809.0406",
    "title": "Foundations of the Pareto Iterated Local Search Metaheuristic",
    "abstract": "  The paper describes the proposition and application of a local search\nmetaheuristic for multi-objective optimization problems. It is based on two\nmain principles of heuristic search, intensification through variable\nneighborhoods, and diversification through perturbations and successive\niterations in favorable regions of the search space. The concept is\nsuccessfully tested on permutation flow shop scheduling problems under multiple\nobjectives. While the obtained results are encouraging in terms of their\nquality, another positive attribute of the approach is its' simplicity as it\ndoes require the setting of only very few parameters. The implementation of the\nPareto Iterated Local Search metaheuristic is based on the MOOPPS computer\nsystem of local search heuristics for multi-objective scheduling which has been\nawarded the European Academic Software Award 2002 in Ronneby, Sweden\n(http://www.easa-award.net/, http://www.bth.se/llab/easa_2002.nsf)\n"
  },
  {
    "id": "0809.0410",
    "title": "A Computational Study of Genetic Crossover Operators for Multi-Objective\n  Vehicle Routing Problem with Soft Time Windows",
    "abstract": "  The article describes an investigation of the effectiveness of genetic\nalgorithms for multi-objective combinatorial optimization (MOCO) by presenting\nan application for the vehicle routing problem with soft time windows. The work\nis motivated by the question, if and how the problem structure influences the\neffectiveness of different configurations of the genetic algorithm.\nComputational results are presented for different classes of vehicle routing\nproblems, varying in their coverage with time windows, time window size,\ndistribution and number of customers. The results are compared with a simple,\nbut effective local search approach for multi-objective combinatorial\noptimization problems.\n"
  },
  {
    "id": "0809.0416",
    "title": "Genetic Algorithms for multiple objective vehicle routing",
    "abstract": "  The talk describes a general approach of a genetic algorithm for multiple\nobjective optimization problems. A particular dominance relation between the\nindividuals of the population is used to define a fitness operator, enabling\nthe genetic algorithm to adress even problems with efficient, but\nconvex-dominated alternatives. The algorithm is implemented in a multilingual\ncomputer program, solving vehicle routing problems with time windows under\nmultiple objectives. The graphical user interface of the program shows the\nprogress of the genetic algorithm and the main parameters of the approach can\nbe easily modified. In addition to that, the program provides powerful decision\nsupport to the decision maker. The software has proved it's excellence at the\nfinals of the European Academic Software Award EASA, held at the Keble college/\nUniversity of Oxford/ Great Britain.\n"
  },
  {
    "id": "0809.0610",
    "title": "A framework for the interactive resolution of multi-objective vehicle\n  routing problems",
    "abstract": "  The article presents a framework for the resolution of rich vehicle routing\nproblems which are difficult to address with standard optimization techniques.\nWe use local search on the basis on variable neighborhood search for the\nconstruction of the solutions, but embed the techniques in a flexible framework\nthat allows the consideration of complex side constraints of the problem such\nas time windows, multiple depots, heterogeneous fleets, and, in particular,\nmultiple optimization criteria. In order to identify a compromise alternative\nthat meets the requirements of the decision maker, an interactive procedure is\nintegrated in the resolution of the problem, allowing the modification of the\npreference information articulated by the decision maker. The framework is\nprototypically implemented in a computer system. First results of test runs on\nmultiple depot vehicle routing problems with time windows are reported.\n"
  },
  {
    "id": "0809.0662",
    "title": "Improving Local Search for Fuzzy Scheduling Problems",
    "abstract": "  The integration of fuzzy set theory and fuzzy logic into scheduling is a\nrather new aspect with growing importance for manufacturing applications,\nresulting in various unsolved aspects. In the current paper, we investigate an\nimproved local search technique for fuzzy scheduling problems with fitness\nplateaus, using a multi criteria formulation of the problem. We especially\naddress the problem of changing job priorities over time as studied at the\nSherwood Press Ltd, a Nottingham based printing company, who is a collaborator\non the project.\n"
  },
  {
    "id": "0809.0755",
    "title": "Bin Packing Under Multiple Objectives - a Heuristic Approximation\n  Approach",
    "abstract": "  The article proposes a heuristic approximation approach to the bin packing\nproblem under multiple objectives. In addition to the traditional objective of\nminimizing the number of bins, the heterogeneousness of the elements in each\nbin is minimized, leading to a biobjective formulation of the problem with a\ntradeoff between the number of bins and their heterogeneousness. An extension\nof the Best-Fit approximation algorithm is presented to solve the problem.\nExperimental investigations have been carried out on benchmark instances of\ndifferent size, ranging from 100 to 1000 items. Encouraging results have been\nobtained, showing the applicability of the heuristic approach to the described\nproblem.\n"
  },
  {
    "id": "0809.0757",
    "title": "An application of the Threshold Accepting metaheuristic for curriculum\n  based course timetabling",
    "abstract": "  The article presents a local search approach for the solution of timetabling\nproblems in general, with a particular implementation for competition track 3\nof the International Timetabling Competition 2007 (ITC 2007). The heuristic\nsearch procedure is based on Threshold Accepting to overcome local optima. A\nstochastic neighborhood is proposed and implemented, randomly removing and\nreassigning events from the current solution.\n  The overall concept has been incrementally obtained from a series of\nexperiments, which we describe in each (sub)section of the paper. In result, we\nsuccessfully derived a potential candidate solution approach for the finals of\ntrack 3 of the ITC 2007.\n"
  },
  {
    "id": "0809.1077",
    "title": "Variable Neighborhood Search for the University Lecturer-Student\n  Assignment Problem",
    "abstract": "  The paper presents a study of local search heuristics in general and variable\nneighborhood search in particular for the resolution of an assignment problem\nstudied in the practical work of universities. Here, students have to be\nassigned to scientific topics which are proposed and supported by members of\nstaff. The problem involves the optimization under given preferences of\nstudents which may be expressed when applying for certain topics.\n  It is possible to observe that variable neighborhood search leads to superior\nresults for the tested problem instances. One instance is taken from an actual\ncase, while others have been generated based on the real world data to support\nthe analysis with a deeper analysis.\n  An extension of the problem has been formulated by integrating a second\nobjective function that simultaneously balances the workload of the members of\nstaff while maximizing utility of the students. The algorithmic approach has\nbeen prototypically implemented in a computer system. One important aspect in\nthis context is the application of the research work to problems of other\nscientific institutions, and therefore the provision of decision support\nfunctionalities.\n"
  },
  {
    "id": "0809.3204",
    "title": "Extended ASP tableaux and rule redundancy in normal logic programs",
    "abstract": "  We introduce an extended tableau calculus for answer set programming (ASP).\nThe proof system is based on the ASP tableaux defined in [Gebser&Schaub, ICLP\n2006], with an added extension rule. We investigate the power of Extended ASP\nTableaux both theoretically and empirically. We study the relationship of\nExtended ASP Tableaux with the Extended Resolution proof system defined by\nTseitin for sets of clauses, and separate Extended ASP Tableaux from ASP\nTableaux by giving a polynomial-length proof for a family of normal logic\nprograms P_n for which ASP Tableaux has exponential-length minimal proofs with\nrespect to n. Additionally, Extended ASP Tableaux imply interesting insight\ninto the effect of program simplification on the lengths of proofs in ASP.\nClosely related to Extended ASP Tableaux, we empirically investigate the effect\nof redundant rules on the efficiency of ASP solving.\n  To appear in Theory and Practice of Logic Programming (TPLP).\n"
  },
  {
    "id": "0809.4582",
    "title": "Achieving compositionality of the stable model semantics for Smodels\n  programs",
    "abstract": "  In this paper, a Gaifman-Shapiro-style module architecture is tailored to the\ncase of Smodels programs under the stable model semantics. The composition of\nSmodels program modules is suitably limited by module conditions which ensure\nthe compatibility of the module system with stable models. Hence the semantics\nof an entire Smodels program depends directly on stable models assigned to its\nmodules. This result is formalized as a module theorem which truly strengthens\nLifschitz and Turner's splitting-set theorem for the class of Smodels programs.\nTo streamline generalizations in the future, the module theorem is first proved\nfor normal programs and then extended to cover Smodels programs using a\ntranslation from the latter class of programs to the former class. Moreover,\nthe respective notion of module-level equivalence, namely modular equivalence,\nis shown to be a proper congruence relation: it is preserved under\nsubstitutions of modules that are modularly equivalent. Principles for program\ndecomposition are also addressed. The strongly connected components of the\nrespective dependency graph can be exploited in order to extract a module\nstructure when there is no explicit a priori knowledge about the modules of a\nprogram. The paper includes a practical demonstration of tools that have been\ndeveloped for automated (de)composition of Smodels programs.\n  To appear in Theory and Practice of Logic Programming.\n"
  },
  {
    "id": "0810.0139",
    "title": "Determining the Unithood of Word Sequences using a Probabilistic\n  Approach",
    "abstract": "  Most research related to unithood were conducted as part of a larger effort\nfor the determination of termhood. Consequently, novelties are rare in this\nsmall sub-field of term extraction. In addition, existing work were mostly\nempirically motivated and derived. We propose a new probabilistically-derived\nmeasure, independent of any influences of termhood, that provides dedicated\nmeasures to gather linguistic evidence from parsed text and statistical\nevidence from Google search engine for the measurement of unithood. Our\ncomparative study using 1,825 test cases against an existing\nempirically-derived function revealed an improvement in terms of precision,\nrecall and accuracy.\n"
  },
  {
    "id": "0810.0156",
    "title": "Determining the Unithood of Word Sequences using Mutual Information and\n  Independence Measure",
    "abstract": "  Most works related to unithood were conducted as part of a larger effort for\nthe determination of termhood. Consequently, the number of independent research\nthat study the notion of unithood and produce dedicated techniques for\nmeasuring unithood is extremely small. We propose a new approach, independent\nof any influences of termhood, that provides dedicated measures to gather\nlinguistic evidence from parsed text and statistical evidence from Google\nsearch engine for the measurement of unithood. Our evaluations revealed a\nprecision and recall of 98.68% and 91.82% respectively with an accuracy at\n95.42% in measuring the unithood of 1005 test cases.\n"
  },
  {
    "id": "0810.0332",
    "title": "Enhanced Integrated Scoring for Cleaning Dirty Texts",
    "abstract": "  An increasing number of approaches for ontology engineering from text are\ngearing towards the use of online sources such as company intranet and the\nWorld Wide Web. Despite such rise, not much work can be found in aspects of\npreprocessing and cleaning dirty texts from online sources. This paper presents\nan enhancement of an Integrated Scoring for Spelling error correction,\nAbbreviation expansion and Case restoration (ISSAC). ISSAC is implemented as\npart of a text preprocessing phase in an ontology engineering system. New\nevaluations performed on the enhanced ISSAC using 700 chat records reveal an\nimproved accuracy of 98% as compared to 96.5% and 71% based on the use of only\nbasic ISSAC and of Aspell, respectively.\n"
  },
  {
    "id": "0810.1186",
    "title": "On-the-fly Macros",
    "abstract": "  We present a domain-independent algorithm that computes macros in a novel\nway. Our algorithm computes macros \"on-the-fly\" for a given set of states and\ndoes not require previously learned or inferred information, nor prior domain\nknowledge. The algorithm is used to define new domain-independent tractable\nclasses of classical planning that are proved to include \\emph{Blocksworld-arm}\nand \\emph{Towers of Hanoi}.\n"
  },
  {
    "id": "0810.2046",
    "title": "Modeling of Social Transitions Using Intelligent Systems",
    "abstract": "  In this study, we reproduce two new hybrid intelligent systems, involve three\nprominent intelligent computing and approximate reasoning methods: Self\nOrganizing feature Map (SOM), Neruo-Fuzzy Inference System and Rough Set Theory\n(RST),called SONFIS and SORST. We show how our algorithms can be construed as a\nlinkage of government-society interactions, where government catches various\nstates of behaviors: solid (absolute) or flexible. So, transition of society,\nby changing of connectivity parameters (noise) from order to disorder is\ninferred.\n"
  },
  {
    "id": "0810.3865",
    "title": "Relationship between Diversity and Perfomance of Multiple Classifiers\n  for Decision Support",
    "abstract": "  The paper presents the investigation and implementation of the relationship\nbetween diversity and the performance of multiple classifiers on classification\naccuracy. The study is critical as to build classifiers that are strong and can\ngeneralize better. The parameters of the neural network within the committee\nwere varied to induce diversity; hence structural diversity is the focus for\nthis study. The hidden nodes and the activation function are the parameters\nthat were varied. The diversity measures that were adopted from ecology such as\nShannon and Simpson were used to quantify diversity. Genetic algorithm is used\nto find the optimal ensemble by using the accuracy as the cost function. The\nresults observed shows that there is a relationship between structural\ndiversity and accuracy. It is observed that the classification accuracy of an\nensemble increases as the diversity increases. There was an increase of 3%-6%\nin the classification accuracy.\n"
  },
  {
    "id": "0811.0131",
    "title": "Balancing Exploration and Exploitation by an Elitist Ant System with\n  Exponential Pheromone Deposition Rule",
    "abstract": "  The paper presents an exponential pheromone deposition rule to modify the\nbasic ant system algorithm which employs constant deposition rule. A stability\nanalysis using differential equation is carried out to find out the values of\nparameters that make the ant system dynamics stable for both kinds of\ndeposition rule. A roadmap of connected cities is chosen as the problem\nenvironment where the shortest route between two given cities is required to be\ndiscovered. Simulations performed with both forms of deposition approach using\nElitist Ant System model reveal that the exponential deposition approach\noutperforms the classical one by a large extent. Exhaustive experiments are\nalso carried out to find out the optimum setting of different controlling\nparameters for exponential deposition approach and an empirical relationship\nbetween the major controlling parameters of the algorithm and some features of\nproblem environment.\n"
  },
  {
    "id": "0811.0134",
    "title": "A Novel Parser Design Algorithm Based on Artificial Ants",
    "abstract": "  This article presents a unique design for a parser using the Ant Colony\nOptimization algorithm. The paper implements the intuitive thought process of\nhuman mind through the activities of artificial ants. The scheme presented here\nuses a bottom-up approach and the parsing program can directly use ambiguous or\nredundant grammars. We allocate a node corresponding to each production rule\npresent in the given grammar. Each node is connected to all other nodes\n(representing other production rules), thereby establishing a completely\nconnected graph susceptible to the movement of artificial ants. Each ant tries\nto modify this sentential form by the production rule present in the node and\nupgrades its position until the sentential form reduces to the start symbol S.\nSuccessful ants deposit pheromone on the links that they have traversed\nthrough. Eventually, the optimum path is discovered by the links carrying\nmaximum amount of pheromone concentration. The design is simple, versatile,\nrobust and effective and obviates the calculation of the above mentioned sets\nand precedence relation tables. Further advantages of our scheme lie in i)\nascertaining whether a given string belongs to the language represented by the\ngrammar, and ii) finding out the shortest possible path from the given string\nto the start symbol S in case multiple routes exist.\n"
  },
  {
    "id": "0811.0136",
    "title": "Extension of Max-Min Ant System with Exponential Pheromone Deposition\n  Rule",
    "abstract": "  The paper presents an exponential pheromone deposition approach to improve\nthe performance of classical Ant System algorithm which employs uniform\ndeposition rule. A simplified analysis using differential equations is carried\nout to study the stability of basic ant system dynamics with both exponential\nand constant deposition rules. A roadmap of connected cities, where the\nshortest path between two specified cities are to be found out, is taken as a\nplatform to compare Max-Min Ant System model (an improved and popular model of\nAnt System algorithm) with exponential and constant deposition rules. Extensive\nsimulations are performed to find the best parameter settings for non-uniform\ndeposition approach and experiments with these parameter settings revealed that\nthe above approach outstripped the traditional one by a large extent in terms\nof both solution quality and convergence time.\n"
  },
  {
    "id": "0811.0340",
    "title": "Document stream clustering: experimenting an incremental algorithm and\n  AR-based tools for highlighting dynamic trends",
    "abstract": "  We address here two major challenges presented by dynamic data mining: 1) the\nstability challenge: we have implemented a rigorous incremental density-based\nclustering algorithm, independent from any initial conditions and ordering of\nthe data-vectors stream, 2) the cognitive challenge: we have implemented a\nstringent selection process of association rules between clusters at time t-1\nand time t for directly generating the main conclusions about the dynamics of a\ndata-stream. We illustrate these points with an application to a two years and\n2600 documents scientific information database.\n"
  },
  {
    "id": "0811.0602",
    "title": "Classification dynamique d'un flux documentaire : une \\'evaluation\n  statique pr\\'ealable de l'algorithme GERMEN",
    "abstract": "  Data-stream clustering is an ever-expanding subdomain of knowledge\nextraction. Most of the past and present research effort aims at efficient\nscaling up for the huge data repositories. Our approach focuses on qualitative\nimprovement, mainly for \"weak signals\" detection and precise tracking of\ntopical evolutions in the framework of information watch - though scalability\nis intrinsically guaranteed in a possibly distributed implementation. Our\nGERMEN algorithm exhaustively picks up the whole set of density peaks of the\ndata at time t, by identifying the local perturbations induced by the current\ndocument vector, such as changing cluster borders, or new/vanishing clusters.\nOptimality yields from the uniqueness 1) of the density landscape for any value\nof our zoom parameter, 2) of the cluster allocation operated by our border\npropagation rule. This results in a rigorous independence from the data\npresentation ranking or any initialization parameter. We present here as a\nfirst step the only assessment of a static view resulting from one year of the\nCNRS/INIST Pascal database in the field of geotechnics.\n"
  },
  {
    "id": "0811.0942",
    "title": "\\'Etude longitudinale d'une proc\\'edure de mod\\'elisation de\n  connaissances en mati\\`ere de gestion du territoire agricole",
    "abstract": "  This paper gives an introduction to this issue, and presents the framework\nand the main steps of the Rosa project. Four teams of researchers, agronomists,\ncomputer scientists, psychologists and linguists were involved during five\nyears within this project that aimed at the development of a knowledge based\nsystem. The purpose of the Rosa system is the modelling and the comparison of\nfarm spatial organizations. It relies on a formalization of agronomical\nknowledge and thus induces a joint knowledge building process involving both\nthe agronomists and the computer scientists. The paper describes the steps of\nthe modelling process as well as the filming procedures set up by the\npsychologists and linguists in order to make explicit and to analyze the\nunderlying knowledge building process.\n"
  },
  {
    "id": "0811.1319",
    "title": "Modeling Social Annotation: a Bayesian Approach",
    "abstract": "  Collaborative tagging systems, such as Delicious, CiteULike, and others,\nallow users to annotate resources, e.g., Web pages or scientific papers, with\ndescriptive labels called tags. The social annotations contributed by thousands\nof users, can potentially be used to infer categorical knowledge, classify\ndocuments or recommend new relevant information. Traditional text inference\nmethods do not make best use of social annotation, since they do not take into\naccount variations in individual users' perspectives and vocabulary. In a\nprevious work, we introduced a simple probabilistic model that takes interests\nof individual annotators into account in order to find hidden topics of\nannotated resources. Unfortunately, that approach had one major shortcoming:\nthe number of topics and interests must be specified a priori. To address this\ndrawback, we extend the model to a fully Bayesian framework, which offers a way\nto automatically estimate these numbers. In particular, the model allows the\nnumber of interests and topics to change as suggested by the structure of the\ndata. We evaluate the proposed model in detail on the synthetic and real-world\ndata by comparing its performance to Latent Dirichlet Allocation on the topic\nextraction task. For the latter evaluation, we apply the model to infer topics\nof Web resources from social annotations obtained from Delicious in order to\ndiscover new resources similar to a specified one. Our empirical results\ndemonstrate that the proposed model is a promising method for exploiting social\nknowledge contained in user-generated annotations.\n"
  },
  {
    "id": "0811.1618",
    "title": "Airport Gate Assignment: New Model and Implementation",
    "abstract": "  Airport gate assignment is of great importance in airport operations. In this\npaper, we study the Airport Gate Assignment Problem (AGAP), propose a new model\nand implement the model with Optimization Programming language (OPL). With the\nobjective to minimize the number of conflicts of any two adjacent aircrafts\nassigned to the same gate, we build a mathematical model with logical\nconstraints and the binary constraints, which can provide an efficient\nevaluation criterion for the Airlines to estimate the current gate assignment.\nTo illustrate the feasibility of the model we construct experiments with the\ndata obtained from Continental Airlines, Houston Gorge Bush Intercontinental\nAirport IAH, which indicate that our model is both energetic and effective.\nMoreover, we interpret experimental results, which further demonstrate that our\nproposed model can provide a powerful tool for airline companies to estimate\nthe efficiency of their current work of gate assignment.\n"
  },
  {
    "id": "0811.1711",
    "title": "Artificial Intelligence Techniques for Steam Generator Modelling",
    "abstract": "  This paper investigates the use of different Artificial Intelligence methods\nto predict the values of several continuous variables from a Steam Generator.\nThe objective was to determine how the different artificial intelligence\nmethods performed in making predictions on the given dataset. The artificial\nintelligence methods evaluated were Neural Networks, Support Vector Machines,\nand Adaptive Neuro-Fuzzy Inference Systems. The types of neural networks\ninvestigated were Multi-Layer Perceptions, and Radial Basis Function. Bayesian\nand committee techniques were applied to these neural networks. Each of the AI\nmethods considered was simulated in Matlab. The results of the simulations\nshowed that all the AI methods were capable of predicting the Steam Generator\ndata reasonably accurately. However, the Adaptive Neuro-Fuzzy Inference system\nout performed the other methods in terms of accuracy and ease of\nimplementation, while still achieving a fast execution time as well as a\nreasonable training time.\n"
  },
  {
    "id": "0812.0885",
    "title": "Elementary epistemological features of machine intelligence",
    "abstract": "  Theoretical analysis of machine intelligence (MI) is useful for defining a\ncommon platform in both theoretical and applied artificial intelligence (AI).\nThe goal of this paper is to set canonical definitions that can assist\npragmatic research in both strong and weak AI. Described epistemological\nfeatures of machine intelligence include relationship between intelligent\nbehavior, intelligent and unintelligent machine characteristics, observable and\nunobservable entities and classification of intelligence. The paper also\nestablishes algebraic definitions of efficiency and accuracy of MI tests as\ntheir quality measure. The last part of the paper addresses the learning\nprocess with respect to the traditional epistemology and the epistemology of MI\ndescribed here. The proposed views on MI positively correlate to the Hegelian\nmonistic epistemology and contribute towards amalgamating idealistic\ndeliberations with the AI theory, particularly in a local frame of reference.\n"
  },
  {
    "id": "0812.1462",
    "title": "Logic programs with propositional connectives and aggregates",
    "abstract": "  Answer set programming (ASP) is a logic programming paradigm that can be used\nto solve complex combinatorial search problems. Aggregates are an ASP construct\nthat plays an important role in many applications. Defining a satisfactory\nsemantics of aggregates turned out to be a difficult problem, and in this paper\nwe propose a new approach, based on an analogy between aggregates and\npropositional connectives. First, we extend the definition of an answer\nset/stable model to cover arbitrary propositional theories; then we define\naggregates on top of them both as primitive constructs and as abbreviations for\nformulas. Our definition of an aggregate combines expressiveness and\nsimplicity, and it inherits many theorems about programs with nested\nexpressions, such as theorems about strong equivalence and splitting.\n"
  },
  {
    "id": "0812.1843",
    "title": "Identification of parameters underlying emotions and a classification of\n  emotions",
    "abstract": "  The standard classification of emotions involves categorizing the expression\nof emotions. In this paper, parameters underlying some emotions are identified\nand a new classification based on these parameters is suggested.\n"
  },
  {
    "id": "0812.2785",
    "title": "Prediction of Platinum Prices Using Dynamically Weighted Mixture of\n  Experts",
    "abstract": "  Neural networks are powerful tools for classification and regression in\nstatic environments. This paper describes a technique for creating an ensemble\nof neural networks that adapts dynamically to changing conditions. The model\nseparates the input space into four regions and each network is given a weight\nin each region based on its performance on samples from that region. The\nensemble adapts dynamically by constantly adjusting these weights based on the\ncurrent performance of the networks. The data set used is a collection of\nfinancial indicators with the goal of predicting the future platinum price. An\nensemble with no weightings does not improve on the naive estimate of no weekly\nchange; our weighting algorithm gives an average percentage error of 63% for\ntwenty weeks of prediction.\n"
  },
  {
    "id": "0812.2991",
    "title": "Analyse et structuration automatique des guides de bonnes pratiques\n  cliniques : essai d'\\'evaluation",
    "abstract": "  Health Practice Guideliens are supposed to unify practices and propose\nrecommendations to physicians. This paper describes GemFrame, a system capable\nof semi-automatically filling an XML template from free texts in the clinical\ndomain. The XML template includes semantic information not explicitly encoded\nin the text (pairs of conditions and ac-tions/recommendations). Therefore,\nthere is a need to compute the exact scope of condi-tions over text sequences\nexpressing the re-quired actions. We present a system developped for this task.\nWe show that it yields good performance when applied to the analysis of French\npractice guidelines. We conclude with a precise evaluation of the tool.\n"
  },
  {
    "id": "0812.3478",
    "title": "Automatic Construction of Lightweight Domain Ontologies for Chemical\n  Engineering Risk Management",
    "abstract": "  The need for domain ontologies in mission critical applications such as risk\nmanagement and hazard identification is becoming more and more pressing. Most\nresearch on ontology learning conducted in the academia remains unrealistic for\nreal-world applications. One of the main problems is the dependence on\nnon-incremental, rare knowledge and textual resources, and manually-crafted\npatterns and rules. This paper reports work in progress aiming to address such\nundesirable dependencies during ontology construction. Initial experiments\nusing a working prototype of the system revealed promising potentials in\nautomatically constructing high-quality domain ontologies using real-world\ntexts.\n"
  },
  {
    "id": "0901.0786",
    "title": "Approximate inference on planar graphs using Loop Calculus and Belief\n  Propagation",
    "abstract": "  We introduce novel results for approximate inference on planar graphical\nmodels using the loop calculus framework. The loop calculus (Chertkov and\nChernyak, 2006) allows to express the exact partition function of a graphical\nmodel as a finite sum of terms that can be evaluated once the belief\npropagation (BP) solution is known. In general, full summation over all\ncorrection terms is intractable. We develop an algorithm for the approach\npresented in (Certkov et al., 2008) which represents an efficient truncation\nscheme on planar graphs and a new representation of the series in terms of\nPfaffians of matrices. We analyze the performance of the algorithm for the\npartition function approximation for models with binary variables and pairwise\ninteractions on grids and other planar graphs. We study in detail both the loop\nseries and the equivalent Pfaffian series and show that the first term of the\nPfaffian series for the general, intractable planar model, can provide very\naccurate approximations. The algorithm outperforms previous truncation schemes\nof the loop series and is competitive with other state-of-the-art methods for\napproximate inference.\n"
  },
  {
    "id": "0901.1289",
    "title": "N-norm and N-conorm in Neutrosophic Logic and Set, and the Neutrosophic\n  Topologies",
    "abstract": "  In this paper we present the N-norms/N-conorms in neutrosophic logic and set\nas extensions of T-norms/T-conorms in fuzzy logic and set. Also, as an\nextension of the Intuitionistic Fuzzy Topology we present the Neutrosophic\nTopologies.\n"
  },
  {
    "id": "0901.3769",
    "title": "Deceptiveness and Neutrality - the ND family of fitness landscapes",
    "abstract": "  When a considerable number of mutations have no effects on fitness values,\nthe fitness landscape is said neutral. In order to study the interplay between\nneutrality, which exists in many real-world applications, and performances of\nmetaheuristics, it is useful to design landscapes which make it possible to\ntune precisely neutral degree distribution. Even though many neutral landscape\nmodels have already been designed, none of them are general enough to create\nlandscapes with specific neutral degree distributions. We propose three steps\nto design such landscapes: first using an algorithm we construct a landscape\nwhose distribution roughly fits the target one, then we use a simulated\nannealing heuristic to bring closer the two distributions and finally we affect\nfitness values to each neutral network. Then using this new family of fitness\nlandscapes we are able to highlight the interplay between deceptiveness and\nneutrality.\n"
  },
  {
    "id": "0901.4004",
    "title": "Mining for adverse drug events with formal concept analysis",
    "abstract": "  The pharmacovigilance databases consist of several case reports involving\ndrugs and adverse events (AEs). Some methods are applied consistently to\nhighlight all signals, i.e. all statistically significant associations between\na drug and an AE. These methods are appropriate for verification of more\ncomplex relationships involving one or several drug(s) and AE(s) (e.g;\nsyndromes or interactions) but do not address the identification of them. We\npropose a method for the extraction of these relationships based on Formal\nConcept Analysis (FCA) associated with disproportionality measures. This method\nidentifies all sets of drugs and AEs which are potential signals, syndromes or\ninteractions. Compared to a previous experience of disproportionality analysis\nwithout FCA, the addition of FCA was more efficient for identifying false\npositives related to concomitant drugs.\n"
  },
  {
    "id": "0901.4761",
    "title": "A Knowledge Discovery Framework for Learning Task Models from User\n  Interactions in Intelligent Tutoring Systems",
    "abstract": "  Domain experts should provide relevant domain knowledge to an Intelligent\nTutoring System (ITS) so that it can guide a learner during problemsolving\nlearning activities. However, for many ill-defined domains, the domain\nknowledge is hard to define explicitly. In previous works, we showed how\nsequential pattern mining can be used to extract a partial problem space from\nlogged user interactions, and how it can support tutoring services during\nproblem-solving exercises. This article describes an extension of this approach\nto extract a problem space that is richer and more adapted for supporting\ntutoring services. We combined sequential pattern mining with (1) dimensional\npattern mining (2) time intervals, (3) the automatic clustering of valued\nactions and (4) closed sequences mining. Some tutoring services have been\nimplemented and an experiment has been conducted in a tutoring system.\n"
  },
  {
    "id": "0901.4963",
    "title": "How Emotional Mechanism Helps Episodic Learning in a Cognitive Agent",
    "abstract": "  In this paper we propose the CTS (Concious Tutoring System) technology, a\nbiologically plausible cognitive agent based on human brain functions.This\nagent is capable of learning and remembering events and any related information\nsuch as corresponding procedures, stimuli and their emotional valences. Our\nproposed episodic memory and episodic learning mechanism are closer to the\ncurrent multiple-trace theory in neuroscience, because they are inspired by it\n[5] contrary to other mechanisms that are incorporated in cognitive agents.\nThis is because in our model emotions play a role in the encoding and\nremembering of events. This allows the agent to improve its behavior by\nremembering previously selected behaviors which are influenced by its emotional\nmechanism. Moreover, the architecture incorporates a realistic memory\nconsolidation process based on a data mining algorithm.\n"
  },
  {
    "id": "0902.0798",
    "title": "Alleviating Media Bias Through Intelligent Agent Blogging",
    "abstract": "  Consumers of mass media must have a comprehensive, balanced and plural\nselection of news to get an unbiased perspective; but achieving this goal can\nbe very challenging, laborious and time consuming. News stories development\nover time, its (in)consistency, and different level of coverage across the\nmedia outlets are challenges that a conscientious reader has to overcome in\norder to alleviate bias.\n  In this paper we present an intelligent agent framework currently\nfacilitating analysis of the main sources of on-line news in El Salvador. We\nshow how prior tools of text analysis and Web 2.0 technologies can be combined\nwith minimal manual intervention to help individuals on their rational decision\nprocess, while holding media outlets accountable for their work.\n"
  },
  {
    "id": "0902.0899",
    "title": "Comparative concept similarity over Minspaces: Axiomatisation and\n  Tableaux Calculus",
    "abstract": "  We study the logic of comparative concept similarity $\\CSL$ introduced by\nSheremet, Tishkovsky, Wolter and Zakharyaschev to capture a form of qualitative\nsimilarity comparison. In this logic we can formulate assertions of the form \"\nobjects A are more similar to B than to C\". The semantics of this logic is\ndefined by structures equipped by distance functions evaluating the similarity\ndegree of objects. We consider here the particular case of the semantics\ninduced by \\emph{minspaces}, the latter being distance spaces where the minimum\nof a set of distances always exists. It turns out that the semantics over\narbitrary minspaces can be equivalently specified in terms of preferential\nstructures, typical of conditional logics. We first give a direct\naxiomatisation of this logic over Minspaces. We next define a decision\nprocedure in the form of a tableaux calculus. Both the calculus and the\naxiomatisation take advantage of the reformulation of the semantics in terms of\npreferential structures.\n"
  },
  {
    "id": "0902.1080",
    "title": "A Model for Managing Collections of Patterns",
    "abstract": "  Data mining algorithms are now able to efficiently deal with huge amount of\ndata. Various kinds of patterns may be discovered and may have some great\nimpact on the general development of knowledge. In many domains, end users may\nwant to have their data mined by data mining tools in order to extract patterns\nthat could impact their business. Nevertheless, those users are often\noverwhelmed by the large quantity of patterns extracted in such a situation.\nMoreover, some privacy issues, or some commercial one may lead the users not to\nbe able to mine the data by themselves. Thus, the users may not have the\npossibility to perform many experiments integrating various constraints in\norder to focus on specific patterns they would like to extract. Post processing\nof patterns may be an answer to that drawback. Thus, in this paper we present a\nframework that could allow end users to manage collections of patterns. We\npropose to use an efficient data structure on which some algebraic operators\nmay be used in order to retrieve or access patterns in pattern bases.\n"
  },
  {
    "id": "0902.2206",
    "title": "Feature Hashing for Large Scale Multitask Learning",
    "abstract": "  Empirical evidence suggests that hashing is an effective strategy for\ndimensionality reduction and practical nonparametric estimation. In this paper\nwe provide exponential tail bounds for feature hashing and show that the\ninteraction between random subspaces is negligible with high probability. We\ndemonstrate the feasibility of this approach with experimental results for a\nnew use case -- multitask learning with hundreds of thousands of tasks.\n"
  },
  {
    "id": "0902.2362",
    "title": "XML Representation of Constraint Networks: Format XCSP 2.1",
    "abstract": "  We propose a new extended format to represent constraint networks using XML.\nThis format allows us to represent constraints defined either in extension or\nin intension. It also allows us to reference global constraints. Any instance\nof the problems CSP (Constraint Satisfaction Problem), QCSP (Quantified CSP)\nand WCSP (Weighted CSP) can be represented using this format.\n"
  },
  {
    "id": "0902.2871",
    "title": "The Semantics of Kalah Game",
    "abstract": "  The present work consisted in developing a plateau game. There are the\ntraditional ones (monopoly, cluedo, ect.) but those which interest us leave\nless place at the chance (luck) than to the strategy such that the chess game.\nKallah is an old African game, its rules are simple but the strategies to be\nused are very complex to implement. Of course, they are based on a strongly\nmathematical basis as in the film \"Rain-Man\" where one can see that gambling\ncan be payed with strategies based on mathematical theories. The Artificial\nIntelligence gives the possibility \"of thinking\" to a machine and, therefore,\nallows it to make decisions. In our work, we use it to give the means to the\ncomputer choosing its best movement.\n"
  },
  {
    "id": "0903.0041",
    "title": "Learning DTW Global Constraint for Time Series Classification",
    "abstract": "  1-Nearest Neighbor with the Dynamic Time Warping (DTW) distance is one of the\nmost effective classifiers on time series domain. Since the global constraint\nhas been introduced in speech community, many global constraint models have\nbeen proposed including Sakoe-Chiba (S-C) band, Itakura Parallelogram, and\nRatanamahatana-Keogh (R-K) band. The R-K band is a general global constraint\nmodel that can represent any global constraints with arbitrary shape and size\neffectively. However, we need a good learning algorithm to discover the most\nsuitable set of R-K bands, and the current R-K band learning algorithm still\nsuffers from an 'overfitting' phenomenon. In this paper, we propose two new\nlearning algorithms, i.e., band boundary extraction algorithm and iterative\nlearning algorithm. The band boundary extraction is calculated from the bound\nof all possible warping paths in each class, and the iterative learning is\nadjusted from the original R-K band learning. We also use a Silhouette index, a\nwell-known clustering validation technique, as a heuristic function, and the\nlower bound function, LB_Keogh, to enhance the prediction speed. Twenty\ndatasets, from the Workshop and Challenge on Time Series Classification, held\nin conjunction of the SIGKDD 2007, are used to evaluate our approach.\n"
  },
  {
    "id": "0903.0211",
    "title": "Range and Roots: Two Common Patterns for Specifying and Propagating\n  Counting and Occurrence Constraints",
    "abstract": "  We propose Range and Roots which are two common patterns useful for\nspecifying a wide range of counting and occurrence constraints. We design\nspecialised propagation algorithms for these two patterns. Counting and\noccurrence constraints specified using these patterns thus directly inherit a\npropagation algorithm. To illustrate the capabilities of the Range and Roots\nconstraints, we specify a number of global constraints taken from the\nliterature. Preliminary experiments demonstrate that propagating counting and\noccurrence constraints using these two patterns leads to a small loss in\nperformance when compared to specialised global constraints and is competitive\nwith alternative decompositions using elementary constraints.\n"
  },
  {
    "id": "0903.0279",
    "title": "An introduction to DSmT",
    "abstract": "  The management and combination of uncertain, imprecise, fuzzy and even\nparadoxical or high conflicting sources of information has always been, and\nstill remains today, of primal importance for the development of reliable\nmodern information systems involving artificial reasoning. In this\nintroduction, we present a survey of our recent theory of plausible and\nparadoxical reasoning, known as Dezert-Smarandache Theory (DSmT), developed for\ndealing with imprecise, uncertain and conflicting sources of information. We\nfocus our presentation on the foundations of DSmT and on its most important\nrules of combination, rather than on browsing specific applications of DSmT\navailable in literature. Several simple examples are given throughout this\npresentation to show the efficiency and the generality of this new approach.\n"
  },
  {
    "id": "0903.0314",
    "title": "Granularity-Adaptive Proof Presentation",
    "abstract": "  When mathematicians present proofs they usually adapt their explanations to\ntheir didactic goals and to the (assumed) knowledge of their addressees. Modern\nautomated theorem provers, in contrast, present proofs usually at a fixed level\nof detail (also called granularity). Often these presentations are neither\nintended nor suitable for human use. A challenge therefore is to develop user-\nand goal-adaptive proof presentation techniques that obey common mathematical\npractice. We present a flexible and adaptive approach to proof presentation\nthat exploits machine learning techniques to extract a model of the specific\ngranularity of proof examples and employs this model for the automated\ngeneration of further proofs at an adapted level of granularity.\n"
  },
  {
    "id": "0903.0465",
    "title": "Breaking Value Symmetry",
    "abstract": "  Symmetry is an important factor in solving many constraint satisfaction\nproblems. One common type of symmetry is when we have symmetric values. In a\nrecent series of papers, we have studied methods to break value symmetries. Our\nresults identify computational limits on eliminating value symmetry. For\ninstance, we prove that pruning all symmetric values is NP-hard in general.\nNevertheless, experiments show that much value symmetry can be broken in\npractice. These results may be useful to researchers in planning, scheduling\nand other areas as value symmetry occurs in many different domains.\n"
  },
  {
    "id": "0903.0475",
    "title": "Reformulating Global Grammar Constraints",
    "abstract": "  An attractive mechanism to specify global constraints in rostering and other\ndomains is via formal languages. For instance, the Regular and Grammar\nconstraints specify constraints in terms of the languages accepted by an\nautomaton and a context-free grammar respectively. Taking advantage of the\nfixed length of the constraint, we give an algorithm to transform a\ncontext-free grammar into an automaton. We then study the use of minimization\ntechniques to reduce the size of such automata and speed up propagation. We\nshow that minimizing such automata after they have been unfolded and domains\ninitially reduced can give automata that are more compact than minimizing\nbefore unfolding and reducing. Experimental results show that such\ntransformations can improve the size of rostering problems that we can 'model\nand run'.\n"
  },
  {
    "id": "0903.0479",
    "title": "Combining Symmetry Breaking and Global Constraints",
    "abstract": "  We propose a new family of constraints which combine together lexicographical\nordering constraints for symmetry breaking with other common global\nconstraints. We give a general purpose propagator for this family of\nconstraints, and show how to improve its complexity by exploiting properties of\nthe included global constraints.\n"
  },
  {
    "id": "0903.0695",
    "title": "Online Estimation of SAT Solving Runtime",
    "abstract": "  We present an online method for estimating the cost of solving SAT problems.\nModern SAT solvers present several challenges to estimate search cost including\nnon-chronological backtracking, learning and restarts. Our method uses a linear\nmodel trained on data gathered at the start of search. We show the\neffectiveness of this method using random and structured problems. We\ndemonstrate that predictions made in early restarts can be used to improve\nlater predictions. We also show that we can use such cost estimations to select\na solver from a portfolio.\n"
  },
  {
    "id": "0903.0786",
    "title": "On Requirements for Programming Exercises from an E-learning Perspective",
    "abstract": "  In this work, we deal with the question of modeling programming exercises for\nnovices pointing to an e-learning scenario. Our purpose is to identify basic\nrequirements, raise some key questions and propose potential answers from a\nconceptual perspective. Presented as a general picture, we hypothetically\nsituate our work in a general context where e-learning instructional material\nneeds to be adapted to form part of an introductory Computer Science (CS)\ne-learning course at the CS1-level. Meant is a potential course which aims at\nimproving novices skills and knowledge on the essentials of programming by\nusing e-learning based approaches in connection (at least conceptually) with a\ngeneral host framework like Activemath (www.activemath.org). Our elaboration\ncovers contextual and, particularly, cognitive elements preparing the terrain\nfor eventual research stages in a derived project, as indicated. We concentrate\nour main efforts on reasoning mechanisms about exercise complexity that can\neventually offer tool support for the task of exercise authoring. We base our\nrequirements analysis on our own perception of the exercise subsystem provided\nby Activemath especially within the domain reasoner area. We enrich the\nanalysis by bringing to the discussion several relevant contextual elements\nfrom the CS1 courses, its definition and implementation. Concerning cognitive\nmodels and exercises, we build upon the principles of Bloom's Taxonomy as a\nrelatively standardized basis and use them as a framework for study and\nanalysis of complexity in basic programming exercises. Our analysis includes\nrequirements for the domain reasoner which are necessary for the exercise\nanalysis. We propose for such a purpose a three-layered conceptual model\nconsidering exercise evaluation, programming and metaprogramming.\n"
  },
  {
    "id": "0903.0829",
    "title": "Tagging multimedia stimuli with ontologies",
    "abstract": "  Successful management of emotional stimuli is a pivotal issue concerning\nAffective Computing (AC) and the related research. As a subfield of Artificial\nIntelligence, AC is concerned not only with the design of computer systems and\nthe accompanying hardware that can recognize, interpret, and process human\nemotions, but also with the development of systems that can trigger human\nemotional response in an ordered and controlled manner. This requires the\nmaximum attainable precision and efficiency in the extraction of data from\nemotionally annotated databases While these databases do use keywords or tags\nfor description of the semantic content, they do not provide either the\nnecessary flexibility or leverage needed to efficiently extract the pertinent\nemotional content. Therefore, to this extent we propose an introduction of\nontologies as a new paradigm for description of emotionally annotated data. The\nability to select and sequence data based on their semantic attributes is vital\nfor any study involving metadata, semantics and ontological sorting like the\nSemantic Web or the Social Semantic Desktop, and the approach described in the\npaper facilitates reuse in these areas as well.\n"
  },
  {
    "id": "0903.1150",
    "title": "Stochastic Constraint Programming: A Scenario-Based Approach",
    "abstract": "  To model combinatorial decision problems involving uncertainty and\nprobability, we introduce scenario based stochastic constraint programming.\nStochastic constraint programs contain both decision variables, which we can\nset, and stochastic variables, which follow a discrete probability\ndistribution. We provide a semantics for stochastic constraint programs based\non scenario trees. Using this semantics, we can compile stochastic constraint\nprograms down into conventional (non-stochastic) constraint programs. This\nallows us to exploit the full power of existing constraint solvers. We have\nimplemented this framework for decision making under uncertainty in stochastic\nOPL, a language which is based on the OPL constraint modelling language\n[Hentenryck et al., 1999]. To illustrate the potential of this framework, we\nmodel a wide range of problems in areas as diverse as portfolio\ndiversification, agricultural planning and production/inventory management.\n"
  },
  {
    "id": "0903.1152",
    "title": "Stochastic Constraint Programming",
    "abstract": "  To model combinatorial decision problems involving uncertainty and\nprobability, we introduce stochastic constraint programming. Stochastic\nconstraint programs contain both decision variables (which we can set) and\nstochastic variables (which follow a probability distribution). They combine\ntogether the best features of traditional constraint satisfaction, stochastic\ninteger programming, and stochastic satisfiability. We give a semantics for\nstochastic constraint programs, and propose a number of complete algorithms and\napproximation procedures. Finally, we discuss a number of extensions of\nstochastic constraint programming to relax various assumptions like the\nindependence between stochastic variables, and compare with other approaches\nfor decision making under uncertainty.\n"
  },
  {
    "id": "0903.3926",
    "title": "Designing a GUI for Proofs - Evaluation of an HCI Experiment",
    "abstract": "  Often user interfaces of theorem proving systems focus on assisting\nparticularly trained and skilled users, i.e., proof experts. As a result, the\nsystems are difficult to use for non-expert users. This paper describes a paper\nand pencil HCI experiment, in which (non-expert) students were asked to make\nsuggestions for a GUI for an interactive system for mathematical proofs. They\nhad to explain the usage of the GUI by applying it to construct a proof sketch\nfor a given theorem. The evaluation of the experiment provides insights for the\ninteraction design for non-expert users and the needs and wants of this user\ngroup.\n"
  },
  {
    "id": "0903.5054",
    "title": "Flow of Activity in the Ouroboros Model",
    "abstract": "  The Ouroboros Model is a new conceptual proposal for an algorithmic structure\nfor efficient data processing in living beings as well as for artificial\nagents. Its central feature is a general repetitive loop where one iteration\ncycle sets the stage for the next. Sensory input activates data structures\n(schemata) with similar constituents encountered before, thus expectations are\nkindled. This corresponds to the highlighting of empty slots in the selected\nschema, and these expectations are compared with the actually encountered\ninput. Depending on the outcome of this consumption analysis different next\nsteps like search for further data or a reset, i.e. a new attempt employing\nanother schema, are triggered. Monitoring of the whole process, and in\nparticular of the flow of activation directed by the consumption analysis,\nyields valuable feedback for the optimum allocation of attention and resources\nincluding the selective establishment of useful new memory entries.\n"
  },
  {
    "id": "0903.5289",
    "title": "Heterogeneous knowledge representation using a finite automaton and\n  first order logic: a case study in electromyography",
    "abstract": "  In a certain number of situations, human cognitive functioning is difficult\nto represent with classical artificial intelligence structures. Such a\ndifficulty arises in the polyneuropathy diagnosis which is based on the spatial\ndistribution, along the nerve fibres, of lesions, together with the synthesis\nof several partial diagnoses. Faced with this problem while building up an\nexpert system (NEUROP), we developed a heterogeneous knowledge representation\nassociating a finite automaton with first order logic. A number of knowledge\nrepresentation problems raised by the electromyography test features are\nexamined in this study and the expert system architecture allowing such a\nknowledge modeling are laid out.\n"
  },
  {
    "id": "0904.0029",
    "title": "Learning for Dynamic subsumption",
    "abstract": "  In this paper a new dynamic subsumption technique for Boolean CNF formulae is\nproposed. It exploits simple and sufficient conditions to detect during\nconflict analysis, clauses from the original formula that can be reduced by\nsubsumption. During the learnt clause derivation, and at each step of the\nresolution process, we simply check for backward subsumption between the\ncurrent resolvent and clauses from the original formula and encoded in the\nimplication graph. Our approach give rise to a strong and dynamic\nsimplification technique that exploits learning to eliminate literals from the\noriginal clauses. Experimental results show that the integration of our dynamic\nsubsumption approach within the state-of-the-art SAT solvers Minisat and Rsat\nachieves interesting improvements particularly on crafted instances.\n"
  },
  {
    "id": "0904.2827",
    "title": "Principle of development",
    "abstract": "  Today, science have a powerful tool for the description of reality - the\nnumbers. However, the concept of number was not immediately, lets try to trace\nthe evolution of the concept. The numbers emerged as the need for accurate\nestimates of the amount in order to permit a comparison of some objects. So if\nyou see to it how many times a day a person uses the numbers and compare, it\nbecomes evident that the comparison is used much more frequently. However, the\ncomparison is not possible without two opposite basic standards. Thus, to\nintroduce the concept of comparison, must have two opposing standards, in turn,\nthe operation of comparison is necessary to introduce the concept of number.\nArguably, the scientific description of reality is impossible without the\nconcept of opposites.\n  In this paper analyzes the concept of opposites, as the basis for the\nintroduction of the principle of development.\n"
  },
  {
    "id": "0904.3701",
    "title": "Semantic Social Network Analysis",
    "abstract": "  Social Network Analysis (SNA) tries to understand and exploit the key\nfeatures of social networks in order to manage their life cycle and predict\ntheir evolution. Increasingly popular web 2.0 sites are forming huge social\nnetwork. Classical methods from social network analysis (SNA) have been applied\nto such online networks. In this paper, we propose leveraging semantic web\ntechnologies to merge and exploit the best features of each domain. We present\nhow to facilitate and enhance the analysis of online social networks,\nexploiting the power of semantic social network analysis.\n"
  },
  {
    "id": "0904.3953",
    "title": "Guarded resolution for answer set programming",
    "abstract": "  We describe a variant of resolution rule of proof and show that it is\ncomplete for stable semantics of logic programs. We show applications of this\nresult.\n"
  },
  {
    "id": "0905.0192",
    "title": "Fuzzy Mnesors",
    "abstract": "  A fuzzy mnesor space is a semimodule over the positive real numbers. It can\nbe used as theoretical framework for fuzzy sets. Hence we can prove a great\nnumber of properties for fuzzy sets without refering to the membership\nfunctions.\n"
  },
  {
    "id": "0905.0197",
    "title": "An Application of Proof-Theory in Answer Set Programming",
    "abstract": "  We apply proof-theoretic techniques in answer Set Programming. The main\nresults include: 1. A characterization of continuity properties of\nGelfond-Lifschitz operator for logic program. 2. A propositional\ncharacterization of stable models of logic programs (without referring to loop\nformulas.\n"
  },
  {
    "id": "0905.3755",
    "title": "Decompositions of All Different, Global Cardinality and Related\n  Constraints",
    "abstract": "  We show that some common and important global constraints like ALL-DIFFERENT\nand GCC can be decomposed into simple arithmetic constraints on which we\nachieve bound or range consistency, and in some cases even greater pruning.\nThese decompositions can be easily added to new solvers. They also provide\nother constraints with access to the state of the propagator by sharing of\nvariables. Such sharing can be used to improve propagation between constraints.\nWe report experiments with our decomposition in a pseudo-Boolean solver.\n"
  },
  {
    "id": "0905.3763",
    "title": "Scenario-based Stochastic Constraint Programming",
    "abstract": "  To model combinatorial decision problems involving uncertainty and\nprobability, we extend the stochastic constraint programming framework proposed\nin [Walsh, 2002] along a number of important dimensions (e.g. to multiple\nchance constraints and to a range of new objectives). We also provide a new\n(but equivalent) semantics based on scenarios. Using this semantics, we can\ncompile stochastic constraint programs down into conventional (nonstochastic)\nconstraint programs. This allows us to exploit the full power of existing\nconstraint solvers. We have implemented this framework for decision making\nunder uncertainty in stochastic OPL, a language which is based on the OPL\nconstraint modelling language [Hentenryck et al., 1999]. To illustrate the\npotential of this framework, we model a wide range of problems in areas as\ndiverse as finance, agriculture and production.\n"
  },
  {
    "id": "0905.3766",
    "title": "Reasoning about soft constraints and conditional preferences: complexity\n  results and approximation techniques",
    "abstract": "  Many real life optimization problems contain both hard and soft constraints,\nas well as qualitative conditional preferences. However, there is no single\nformalism to specify all three kinds of information. We therefore propose a\nframework, based on both CP-nets and soft constraints, that handles both hard\nand soft constraints as well as conditional preferences efficiently and\nuniformly. We study the complexity of testing the consistency of preference\nstatements, and show how soft constraints can faithfully approximate the\nsemantics of conditional preference statements whilst improving the\ncomputational complexity\n"
  },
  {
    "id": "0905.3769",
    "title": "Multiset Ordering Constraints",
    "abstract": "  We identify a new and important global (or non-binary) constraint. This\nconstraint ensures that the values taken by two vectors of variables, when\nviewed as multisets, are ordered. This constraint is useful for a number of\ndifferent applications including breaking symmetry and fuzzy constraint\nsatisfaction. We propose and implement an efficient linear time algorithm for\nenforcing generalised arc consistency on such a multiset ordering constraint.\nExperimental results on several problem domains show considerable promise.\n"
  },
  {
    "id": "0905.3830",
    "title": "Tag Clouds for Displaying Semantics: The Case of Filmscripts",
    "abstract": "  We relate tag clouds to other forms of visualization, including planar or\nreduced dimensionality mapping, and Kohonen self-organizing maps. Using a\nmodified tag cloud visualization, we incorporate other information into it,\nincluding text sequence and most pertinent words. Our notion of word pertinence\ngoes beyond just word frequency and instead takes a word in a mathematical\nsense as located at the average of all of its pairwise relationships. We\ncapture semantics through context, taken as all pairwise relationships. Our\ndomain of application is that of filmscript analysis. The analysis of\nfilmscripts, always important for cinema, is experiencing a major gain in\nimportance in the context of television. Our objective in this work is to\nvisualize the semantics of filmscript, and beyond filmscript any other\npartially structured, time-ordered, sequence of text segments. In particular we\ndevelop an innovative approach to plot characterization.\n"
  },
  {
    "id": "0905.4601",
    "title": "Considerations on Construction Ontologies",
    "abstract": "  The paper proposes an analysis on some existent ontologies, in order to point\nout ways to resolve semantic heterogeneity in information systems. Authors are\nhighlighting the tasks in a Knowledge Acquisiton System and identifying aspects\nrelated to the addition of new information to an intelligent system. A solution\nis proposed, as a combination of ontology reasoning services and natural\nlanguages generation. A multi-agent system will be conceived with an extractor\nagent, a reasoner agent and a competence management agent.\n"
  },
  {
    "id": "0905.4614",
    "title": "A Logic Programming Approach to Activity Recognition",
    "abstract": "  We have been developing a system for recognising human activity given a\nsymbolic representation of video content. The input of our system is a set of\ntime-stamped short-term activities detected on video frames. The output of our\nsystem is a set of recognised long-term activities, which are pre-defined\ntemporal combinations of short-term activities. The constraints on the\nshort-term activities that, if satisfied, lead to the recognition of a\nlong-term activity, are expressed using a dialect of the Event Calculus. We\nillustrate the expressiveness of the dialect by showing the representation of\nseveral typical complex activities. Furthermore, we present a detailed\nevaluation of the system through experimentation on a benchmark dataset of\nsurveillance videos.\n"
  },
  {
    "id": "0906.1673",
    "title": "Knowledge Management in Economic Intelligence with Reasoning on Temporal\n  Attributes",
    "abstract": "  People have to make important decisions within a time frame. Hence, it is\nimperative to employ means or strategy to aid effective decision making.\nConsequently, Economic Intelligence (EI) has emerged as a field to aid\nstrategic and timely decision making in an organization. In the course of\nattaining this goal: it is indispensable to be more optimistic towards\nprovision for conservation of intellectual resource invested into the process\nof decision making. This intellectual resource is nothing else but the\nknowledge of the actors as well as that of the various processes for effecting\ndecision making. Knowledge has been recognized as a strategic economic resource\nfor enhancing productivity and a key for innovation in any organization or\ncommunity. Thus, its adequate management with cognizance of its temporal\nproperties is highly indispensable. Temporal properties of knowledge refer to\nthe date and time (known as timestamp) such knowledge is created as well as the\nduration or interval between related knowledge. This paper focuses on the needs\nfor a user-centered knowledge management approach as well as exploitation of\nassociated temporal properties. Our perspective of knowledge is with respect to\ndecision-problems projects in EI. Our hypothesis is that the possibility of\nreasoning about temporal properties in exploitation of knowledge in EI projects\nshould foster timely decision making through generation of useful inferences\nfrom available and reusable knowledge for a new project.\n"
  },
  {
    "id": "0906.1694",
    "title": "Toward a Category Theory Design of Ontological Knowledge Bases",
    "abstract": "  I discuss (ontologies_and_ontological_knowledge_bases /\nformal_methods_and_theories) duality and its category theory extensions as a\nstep toward a solution to Knowledge-Based Systems Theory. In particular I focus\non the example of the design of elements of ontologies and ontological\nknowledge bases of next three electronic courses: Foundations of Research\nActivities, Virtual Modeling of Complex Systems and Introduction to String\nTheory.\n"
  },
  {
    "id": "0906.3036",
    "title": "Mnesors for automatic control",
    "abstract": "  Mnesors are defined as elements of a semimodule over the min-plus integers.\nThis two-sorted structure is able to merge graduation properties of vectors and\nidempotent properties of boolean numbers, which makes it appropriate for hybrid\nsystems. We apply it to the control of an inverted pendulum and design a full\nlogical controller, that is, without the usual algebra of real numbers.\n"
  },
  {
    "id": "0906.3149",
    "title": "Semi-Myopic Sensing Plans for Value Optimization",
    "abstract": "  We consider the following sequential decision problem. Given a set of items\nof unknown utility, we need to select one of as high a utility as possible\n(``the selection problem''). Measurements (possibly noisy) of item values prior\nto selection are allowed, at a known cost. The goal is to optimize the overall\nsequential decision process of measurements and selection.\n  Value of information (VOI) is a well-known scheme for selecting measurements,\nbut the intractability of the problem typically leads to using myopic VOI\nestimates. In the selection problem, myopic VOI frequently badly underestimates\nthe value of information, leading to inferior sensing plans. We relax the\nstrict myopic assumption into a scheme we term semi-myopic, providing a\nspectrum of methods that can improve the performance of sensing plans. In\nparticular, we propose the efficiently computable method of ``blinkered'' VOI,\nand examine theoretical bounds for special cases. Empirical evaluation of\n``blinkered'' VOI in the selection problem with normally distributed item\nvalues shows that is performs much better than pure myopic VOI.\n"
  },
  {
    "id": "0906.4332",
    "title": "Updating Sets of Probabilities",
    "abstract": "  There are several well-known justifications for conditioning as the\nappropriate method for updating a single probability measure, given an\nobservation. However, there is a significant body of work arguing for sets of\nprobability measures, rather than single measures, as a more realistic model of\nuncertainty. Conditioning still makes sense in this context--we can simply\ncondition each measure in the set individually, then combine the results--and,\nindeed, it seems to be the preferred updating procedure in the literature. But\nhow justified is conditioning in this richer setting? Here we show, by\nconsidering an axiomatic account of conditioning given by van Fraassen, that\nthe single-measure and sets-of-measures cases are very different. We show that\nvan Fraassen's axiomatization for the former case is nowhere near sufficient\nfor updating sets of measures. We give a considerably longer (and not as\ncompelling) list of axioms that together force conditioning in this setting,\nand describe other update methods that are allowed once any of these axioms is\ndropped.\n"
  },
  {
    "id": "0906.5038",
    "title": "A Novel Two-Stage Dynamic Decision Support based Optimal Threat\n  Evaluation and Defensive Resource Scheduling Algorithm for Multi Air-borne\n  threats",
    "abstract": "  This paper presents a novel two-stage flexible dynamic decision support based\noptimal threat evaluation and defensive resource scheduling algorithm for\nmulti-target air-borne threats. The algorithm provides flexibility and\noptimality by swapping between two objective functions, i.e. the preferential\nand subtractive defense strategies as and when required. To further enhance the\nsolution quality, it outlines and divides the critical parameters used in\nThreat Evaluation and Weapon Assignment (TEWA) into three broad categories\n(Triggering, Scheduling and Ranking parameters). Proposed algorithm uses a\nvariant of many-to-many Stable Marriage Algorithm (SMA) to solve Threat\nEvaluation (TE) and Weapon Assignment (WA) problem. In TE stage, Threat Ranking\nand Threat-Asset pairing is done. Stage two is based on a new flexible dynamic\nweapon scheduling algorithm, allowing multiple engagements using\nshoot-look-shoot strategy, to compute near-optimal solution for a range of\nscenarios. Analysis part of this paper presents the strengths and weaknesses of\nthe proposed algorithm over an alternative greedy algorithm as applied to\ndifferent offline scenarios.\n"
  },
  {
    "id": "0906.5119",
    "title": "General combination rules for qualitative and quantitative beliefs",
    "abstract": "  Martin and Osswald \\cite{Martin07} have recently proposed many\ngeneralizations of combination rules on quantitative beliefs in order to manage\nthe conflict and to consider the specificity of the responses of the experts.\nSince the experts express themselves usually in natural language with\nlinguistic labels, Smarandache and Dezert \\cite{Li07} have introduced a\nmathematical framework for dealing directly also with qualitative beliefs. In\nthis paper we recall some element of our previous works and propose the new\ncombination rules, developed for the fusion of both qualitative or quantitative\nbeliefs.\n"
  },
  {
    "id": "0907.0067",
    "title": "A Novel Two-Staged Decision Support based Threat Evaluation and Weapon\n  Assignment Algorithm, Asset-based Dynamic Weapon Scheduling using Artificial\n  Intelligence Techinques",
    "abstract": "  Surveillance control and reporting (SCR) system for air threats play an\nimportant role in the defense of a country. SCR system corresponds to air and\nground situation management/processing along with information fusion,\ncommunication, coordination, simulation and other critical defense oriented\ntasks. Threat Evaluation and Weapon Assignment (TEWA) sits at the core of SCR\nsystem. In such a system, maximal or near maximal utilization of constrained\nresources is of extreme importance. Manual TEWA systems cannot provide\noptimality because of different limitations e.g.surface to air missile (SAM)\ncan fire from a distance of 5Km, but manual TEWA systems are constrained by\nhuman vision range and other constraints. Current TEWA systems usually work on\ntarget-by-target basis using some type of greedy algorithm thus affecting the\noptimality of the solution and failing in multi-target scenario. his paper\nrelates to a novel two-staged flexible dynamic decision support based optimal\nthreat evaluation and weapon assignment algorithm for multi-target air-borne\nthreats.\n"
  },
  {
    "id": "0907.0589",
    "title": "Generalized Collective Inference with Symmetric Clique Potentials",
    "abstract": "  Collective graphical models exploit inter-instance associative dependence to\noutput more accurate labelings. However existing models support very limited\nkind of associativity which restricts accuracy gains. This paper makes two\nmajor contributions. First, we propose a general collective inference framework\nthat biases data instances to agree on a set of {\\em properties} of their\nlabelings. Agreement is encouraged through symmetric clique potentials. We show\nthat rich properties leads to bigger gains, and present a systematic inference\nprocedure for a large class of such properties. The procedure performs message\npassing on the cluster graph, where property-aware messages are computed with\ncluster specific algorithms. This provides an inference-only solution for\ndomain adaptation. Our experiments on bibliographic information extraction\nillustrate significant test error reduction over unseen domains. Our second\nmajor contribution consists of algorithms for computing outgoing messages from\nclique clusters with symmetric clique potentials. Our algorithms are exact for\narbitrary symmetric potentials on binary labels and for max-like and\nmajority-like potentials on multiple labels. For majority potentials, we also\nprovide an efficient Lagrangian Relaxation based algorithm that compares\nfavorably with the exact algorithm. We present a 13/15-approximation algorithm\nfor the NP-hard Potts potential, with runtime sub-quadratic in the clique size.\nIn contrast, the best known previous guarantee for graphs with Potts potentials\nis only 1/2. We empirically show that our method for Potts potentials is an\norder of magnitude faster than the best alternatives, and our Lagrangian\nRelaxation based algorithm for majority potentials beats the best applicable\nheuristic -- ICM.\n"
  },
  {
    "id": "0907.0939",
    "title": "The Soft Cumulative Constraint",
    "abstract": "  This research report presents an extension of Cumulative of Choco constraint\nsolver, which is useful to encode over-constrained cumulative problems. This\nnew global constraint uses sweep and task interval violation-based algorithms.\n"
  },
  {
    "id": "0907.2775",
    "title": "Modelling Concurrent Behaviors in the Process Specification Language",
    "abstract": "  In this paper, we propose a first-order ontology for generalized stratified\norder structure. We then classify the models of the theory using\nmodel-theoretic techniques. An ontology mapping from this ontology to the core\ntheory of Process Specification Language is also discussed.\n"
  },
  {
    "id": "0907.2990",
    "title": "The Single Machine Total Weighted Tardiness Problem - Is it (for\n  Metaheuristics) a Solved Problem ?",
    "abstract": "  The article presents a study of rather simple local search heuristics for the\nsingle machine total weighted tardiness problem (SMTWTP), namely hillclimbing\nand Variable Neighborhood Search. In particular, we revisit these approaches\nfor the SMTWTP as there appears to be a lack of appropriate/challenging\nbenchmark instances in this case. The obtained results are impressive indeed.\nOnly few instances remain unsolved, and even those are approximated within 1%\nof the optimal/best known solutions. Our experiments support the claim that\nmetaheuristics for the SMTWTP are very likely to lead to good results, and\nthat, before refining search strategies, more work must be done with regard to\nthe proposition of benchmark data. Some recommendations for the construction of\nsuch data sets are derived from our investigations.\n"
  },
  {
    "id": "0907.2993",
    "title": "Improvements for multi-objective flow shop scheduling by Pareto Iterated\n  Local Search",
    "abstract": "  The article describes the proposition and application of a local search\nmetaheuristic for multi-objective optimization problems. It is based on two\nmain principles of heuristic search, intensification through variable\nneighborhoods, and diversification through perturbations and successive\niterations in favorable regions of the search space. The concept is\nsuccessfully tested on permutation flow shop scheduling problems under multiple\nobjectives and compared to other local search approaches. While the obtained\nresults are encouraging in terms of their quality, another positive attribute\nof the approach is its simplicity as it does require the setting of only very\nfew parameters.\n"
  },
  {
    "id": "0907.4100",
    "title": "Beyond Turing Machines",
    "abstract": "  This paper discusses \"computational\" systems capable of \"computing\" functions\nnot computable by predefined Turing machines if the systems are not isolated\nfrom their environment. Roughly speaking, these systems can change their finite\ndescriptions by interacting with their environment.\n"
  },
  {
    "id": "0907.4509",
    "title": "Pattern Recognition Theory of Mind",
    "abstract": "  I propose that pattern recognition, memorization and processing are key\nconcepts that can be a principle set for the theoretical modeling of the mind\nfunction. Most of the questions about the mind functioning can be answered by a\ndescriptive modeling and definitions from these principles. An understandable\nconsciousness definition can be drawn based on the assumption that a pattern\nrecognition system can recognize its own patterns of activity. The principles,\ndescriptive modeling and definitions can be a basis for theoretical and applied\nresearch on cognitive sciences, particularly at artificial intelligence\nstudies.\n"
  },
  {
    "id": "0907.4561",
    "title": "Fact Sheet on Semantic Web",
    "abstract": "  The report gives an overview about activities on the topic Semantic Web. It\nhas been released as technical report for the project \"KTweb -- Connecting\nKnowledge Technologies Communities\" in 2003.\n"
  },
  {
    "id": "0907.5032",
    "title": "Restart Strategy Selection using Machine Learning Techniques",
    "abstract": "  Restart strategies are an important factor in the performance of\nconflict-driven Davis Putnam style SAT solvers. Selecting a good restart\nstrategy for a problem instance can enhance the performance of a solver.\nInspired by recent success applying machine learning techniques to predict the\nruntime of SAT solvers, we present a method which uses machine learning to\nboost solver performance through a smart selection of the restart strategy.\nBased on easy to compute features, we train both a satisfiability classifier\nand runtime models. We use these models to choose between restart strategies.\nWe present experimental results comparing this technique with the most commonly\nused restart strategies. Our results demonstrate that machine learning is\neffective in improving solver performance.\n"
  },
  {
    "id": "0907.5033",
    "title": "Online Search Cost Estimation for SAT Solvers",
    "abstract": "  We present two different methods for estimating the cost of solving SAT\nproblems. The methods focus on the online behaviour of the backtracking solver,\nas well as the structure of the problem. Modern SAT solvers present several\nchallenges to estimate search cost including coping with nonchronological\nbacktracking, learning and restarts. Our first method adapt an existing\nalgorithm for estimating the size of a search tree to deal with these\nchallenges. We then suggest a second method that uses a linear model trained on\ndata gathered online at the start of search. We compare the effectiveness of\nthese two methods using random and structured problems. We also demonstrate\nthat predictions made in early restarts can be used to improve later\npredictions. We conclude by showing that the cost of solving a set of problems\ncan be reduced by selecting a solver from a portfolio based on such cost\nestimations.\n"
  },
  {
    "id": "0907.5155",
    "title": "On Classification from Outlier View",
    "abstract": "  Classification is the basis of cognition. Unlike other solutions, this study\napproaches it from the view of outliers. We present an expanding algorithm to\ndetect outliers in univariate datasets, together with the underlying\nfoundation. The expanding algorithm runs in a holistic way, making it a rather\nrobust solution. Synthetic and real data experiments show its power.\nFurthermore, an application for multi-class problems leads to the introduction\nof the oscillator algorithm. The corresponding result implies the potential\nwide use of the expanding algorithm.\n"
  },
  {
    "id": "0907.5598",
    "title": "Convergence of Expected Utility for Universal AI",
    "abstract": "  We consider a sequence of repeated interactions between an agent and an\nenvironment. Uncertainty about the environment is captured by a probability\ndistribution over a space of hypotheses, which includes all computable\nfunctions. Given a utility function, we can evaluate the expected utility of\nany computational policy for interaction with the environment. After making\nsome plausible assumptions (and maybe one not-so-plausible assumption), we show\nthat if the utility function is unbounded, then the expected utility of any\npolicy is undefined.\n"
  },
  {
    "id": "0908.0089",
    "title": "Knowledge Discovery of Hydrocyclone s Circuit Based on SONFIS and SORST",
    "abstract": "  This study describes application of some approximate reasoning methods to\nanalysis of hydrocyclone performance. In this manner, using a combining of Self\nOrganizing Map (SOM), Neuro-Fuzzy Inference System (NFIS)-SONFIS- and Rough Set\nTheory (RST)-SORST-crisp and fuzzy granules are obtained. Balancing of crisp\ngranules and non-crisp granules can be implemented in close-open iteration.\nUsing different criteria and based on granulation level balance point\n(interval) or a pseudo-balance point is estimated. Validation of the proposed\nmethods, on the data set of the hydrocyclone is rendered.\n"
  },
  {
    "id": "0908.0100",
    "title": "A Class of DSm Conditional Rules",
    "abstract": "  In this paper we introduce two new DSm fusion conditioning rules with\nexample, and as a generalization of them a class of DSm fusion conditioning\nrules, and then extend them to a class of DSm conditioning rules.\n"
  },
  {
    "id": "0908.2050",
    "title": "View-based Propagator Derivation",
    "abstract": "  When implementing a propagator for a constraint, one must decide about\nvariants: When implementing min, should one also implement max? Should one\nimplement linear constraints both with unit and non-unit coefficients?\nConstraint variants are ubiquitous: implementing them requires considerable (if\nnot prohibitive) effort and decreases maintainability, but will deliver better\nperformance than resorting to constraint decomposition.\n  This paper shows how to use views to derive perfect propagator variants. A\nmodel for views and derived propagators is introduced. Derived propagators are\nproved to be indeed perfect in that they inherit essential properties such as\ncorrectness and domain and bounds consistency. Techniques for systematically\nderiving propagators such as transformation, generalization, specialization,\nand type conversion are developed. The paper introduces an implementation\narchitecture for views that is independent of the underlying constraint\nprogramming system. A detailed evaluation of views implemented in Gecode shows\nthat derived propagators are efficient and that views often incur no overhead.\nWithout views, Gecode would either require 180 000 rather than 40 000 lines of\npropagator code, or would lack many efficient propagator variants. Compared to\n8 000 lines of code for views, the reduction in code for propagators yields a\n1750% return on investment.\n"
  },
  {
    "id": "0908.3394",
    "title": "A Cognitive Mind-map Framework to Foster Trust",
    "abstract": "  The explorative mind-map is a dynamic framework, that emerges automatically\nfrom the input, it gets. It is unlike a verificative modeling system where\nexisting (human) thoughts are placed and connected together. In this regard,\nexplorative mind-maps change their size continuously, being adaptive with\nconnectionist cells inside; mind-maps process data input incrementally and\noffer lots of possibilities to interact with the user through an appropriate\ncommunication interface. With respect to a cognitive motivated situation like a\nconversation between partners, mind-maps become interesting as they are able to\nprocess stimulating signals whenever they occur. If these signals are close to\nan own understanding of the world, then the conversational partner becomes\nautomatically more trustful than if the signals do not or less match the own\nknowledge scheme. In this (position) paper, we therefore motivate explorative\nmind-maps as a cognitive engine and propose these as a decision support engine\nto foster trust.\n"
  },
  {
    "id": "0908.3999",
    "title": "An improved axiomatic definition of information granulation",
    "abstract": "  To capture the uncertainty of information or knowledge in information\nsystems, various information granulations, also known as knowledge\ngranulations, have been proposed. Recently, several axiomatic definitions of\ninformation granulation have been introduced. In this paper, we try to improve\nthese axiomatic definitions and give a universal construction of information\ngranulation by relating information granulations with a class of functions of\nmultiple variables. We show that the improved axiomatic definition has some\nconcrete information granulations in the literature as instances.\n"
  },
  {
    "id": "0909.0122",
    "title": "Reasoning with Topological and Directional Spatial Information",
    "abstract": "  Current research on qualitative spatial representation and reasoning mainly\nfocuses on one single aspect of space. In real world applications, however,\nmultiple spatial aspects are often involved simultaneously.\n  This paper investigates problems arising in reasoning with combined\ntopological and directional information. We use the RCC8 algebra and the\nRectangle Algebra (RA) for expressing topological and directional information\nrespectively. We give examples to show that the bipath-consistency algorithm\nBIPATH is incomplete for solving even basic RCC8 and RA constraints. If\ntopological constraints are taken from some maximal tractable subclasses of\nRCC8, and directional constraints are taken from a subalgebra, termed DIR49, of\nRA, then we show that BIPATH is able to separate topological constraints from\ndirectional ones. This means, given a set of hybrid topological and directional\nconstraints from the above subclasses of RCC8 and RA, we can transfer the joint\nsatisfaction problem in polynomial time to two independent satisfaction\nproblems in RCC8 and RA. For general RA constraints, we give a method to\ncompute solutions that satisfy all topological constraints and approximately\nsatisfy each RA constraint to any prescribed precision.\n"
  },
  {
    "id": "0909.0138",
    "title": "Reasoning about Cardinal Directions between Extended Objects",
    "abstract": "  Direction relations between extended spatial objects are important\ncommonsense knowledge. Recently, Goyal and Egenhofer proposed a formal model,\nknown as Cardinal Direction Calculus (CDC), for representing direction\nrelations between connected plane regions. CDC is perhaps the most expressive\nqualitative calculus for directional information, and has attracted increasing\ninterest from areas such as artificial intelligence, geographical information\nscience, and image retrieval. Given a network of CDC constraints, the\nconsistency problem is deciding if the network is realizable by connected\nregions in the real plane. This paper provides a cubic algorithm for checking\nconsistency of basic CDC constraint networks, and proves that reasoning with\nCDC is in general an NP-Complete problem. For a consistent network of basic CDC\nconstraints, our algorithm also returns a 'canonical' solution in cubic time.\nThis cubic algorithm is also adapted to cope with cardinal directions between\npossibly disconnected regions, in which case currently the best algorithm is of\ntime complexity O(n^5).\n"
  },
  {
    "id": "0909.0682",
    "title": "On Planning with Preferences in HTN",
    "abstract": "  In this paper, we address the problem of generating preferred plans by\ncombining the procedural control knowledge specified by Hierarchical Task\nNetworks (HTNs) with rich qualitative user preferences. The outcome of our work\nis a language for specifyin user preferences, tailored to HTN planning,\ntogether with a provably optimal preference-based planner, HTNPLAN, that is\nimplemented as an extension of SHOP2. To compute preferred plans, we propose an\napproach based on forward-chaining heuristic search. Our heuristic uses an\nadmissible evaluation function measuring the satisfaction of preferences over\npartial plans. Our empirical evaluation demonstrates the effectiveness of our\nHTNPLAN heuristics. We prove our approach sound and optimal with respect to the\nplans it generates by appealing to a situation calculus semantics of our\npreference language and of HTN planning. While our implementation builds on\nSHOP2, the language and techniques proposed here are relevant to a broad range\nof HTN planners.\n"
  },
  {
    "id": "0909.0901",
    "title": "Assessing the Impact of Informedness on a Consultant's Profit",
    "abstract": "  We study the notion of informedness in a client-consultant setting. Using a\nsoftware simulator, we examine the extent to which it pays off for consultants\nto provide their clients with advice that is well-informed, or with advice that\nis merely meant to appear to be well-informed. The latter strategy is\nbeneficial in that it costs less resources to keep up-to-date, but carries the\nrisk of a decreased reputation if the clients discover the low level of\ninformedness of the consultant. Our experimental results indicate that under\ndifferent circumstances, different strategies yield the optimal results (net\nprofit) for the consultants.\n"
  },
  {
    "id": "0909.1021",
    "title": "A multiagent urban traffic simulation Part I: dealing with the ordinary",
    "abstract": "  We describe in this article a multiagent urban traffic simulation, as we\nbelieve individual-based modeling is necessary to encompass the complex\ninfluence the actions of an individual vehicle can have on the overall flow of\nvehicles. We first describe how we build a graph description of the network\nfrom purely geometric data, ESRI shapefiles. We then explain how we include\ntraffic related data to this graph. We go on after that with the model of the\nvehicle agents: origin and destination, driving behavior, multiple lanes,\ncrossroads, and interactions with the other vehicles in day-to-day, ?ordinary?\ntraffic. We conclude with the presentation of the resulting simulation of this\nmodel on the Rouen agglomeration.\n"
  },
  {
    "id": "0909.1151",
    "title": "n-Opposition theory to structure debates",
    "abstract": "  2007 was the first international congress on the ?square of oppositions?. A\nfirst attempt to structure debate using n-opposition theory was presented along\nwith the results of a first experiment on the web. Our proposal for this paper\nis to define relations between arguments through a structure of opposition\n(square of oppositions is one structure of opposition). We will be trying to\nanswer the following questions: How to organize debates on the web 2.0? How to\nstructure them in a logical way? What is the role of n-opposition theory, in\nthis context? We present in this paper results of three experiments\n(Betapolitique 2007, ECAP 2008, Intermed 2008).\n"
  },
  {
    "id": "0909.2091",
    "title": "Paired Comparisons-based Interactive Differential Evolution",
    "abstract": "  We propose Interactive Differential Evolution (IDE) based on paired\ncomparisons for reducing user fatigue and evaluate its convergence speed in\ncomparison with Interactive Genetic Algorithms (IGA) and tournament IGA. User\ninterface and convergence performance are two big keys for reducing Interactive\nEvolutionary Computation (IEC) user fatigue. Unlike IGA and conventional IDE,\nusers of the proposed IDE and tournament IGA do not need to compare whole\nindividuals each other but compare pairs of individuals, which largely\ndecreases user fatigue. In this paper, we design a pseudo-IEC user and evaluate\nanother factor, IEC convergence performance, using IEC simulators and show that\nour proposed IDE converges significantly faster than IGA and tournament IGA,\ni.e. our proposed one is superior to others from both user interface and\nconvergence performance points of view.\n"
  },
  {
    "id": "0909.2339",
    "title": "Back analysis based on SOM-RST system",
    "abstract": "  This paper describes application of information granulation theory, on the\nback analysis of Jeffrey mine southeast wall Quebec. In this manner, using a\ncombining of Self Organizing Map (SOM) and rough set theory (RST), crisp and\nrough granules are obtained. Balancing of crisp granules and sub rough granules\nis rendered in close-open iteration. Combining of hard and soft computing,\nnamely finite difference method (FDM) and computational intelligence and taking\nin to account missing information are two main benefits of the proposed method.\nAs a practical example, reverse analysis on the failure of the southeast wall\nJeffrey mine is accomplished.\n"
  },
  {
    "id": "0909.2375",
    "title": "Similarity Matching Techniques for Fault Diagnosis in Automotive\n  Infotainment Electronics",
    "abstract": "  Fault diagnosis has become a very important area of research during the last\ndecade due to the advancement of mechanical and electrical systems in\nindustries. The automobile is a crucial field where fault diagnosis is given a\nspecial attention. Due to the increasing complexity and newly added features in\nvehicles, a comprehensive study has to be performed in order to achieve an\nappropriate diagnosis model. A diagnosis system is capable of identifying the\nfaults of a system by investigating the observable effects (or symptoms). The\nsystem categorizes the fault into a diagnosis class and identifies a probable\ncause based on the supplied fault symptoms. Fault categorization and\nidentification are done using similarity matching techniques. The development\nof diagnosis classes is done by making use of previous experience, knowledge or\ninformation within an application area. The necessary information used may come\nfrom several sources of knowledge, such as from system analysis. In this paper\nsimilarity matching techniques for fault diagnosis in automotive infotainment\napplications are discussed.\n"
  },
  {
    "id": "0909.2376",
    "title": "Performing Hybrid Recommendation in Intermodal Transportation-the\n  FTMarket System's Recommendation Module",
    "abstract": "  Diverse recommendation techniques have been already proposed and encapsulated\ninto several e-business applications, aiming to perform a more accurate\nevaluation of the existing information and accordingly augment the assistance\nprovided to the users involved. This paper reports on the development and\nintegration of a recommendation module in an agent-based transportation\ntransactions management system. The module is built according to a novel hybrid\nrecommendation technique, which combines the advantages of collaborative\nfiltering and knowledge-based approaches. The proposed technique and supporting\nmodule assist customers in considering in detail alternative transportation\ntransactions that satisfy their requests, as well as in evaluating completed\ntransactions. The related services are invoked through a software agent that\nconstructs the appropriate knowledge rules and performs a synthesis of the\nrecommendation policy.\n"
  },
  {
    "id": "0909.3273",
    "title": "Decomposition of the NVALUE constraint",
    "abstract": "  We study decompositions of NVALUE, a global constraint that can be used to\nmodel a wide range of problems where values need to be counted. Whilst\ndecomposition typically hinders propagation, we identify one decomposition that\nmaintains a global view as enforcing bound consistency on the decomposition\nachieves bound consistency on the original global NVALUE constraint. Such\ndecompositions offer the prospect for advanced solving techniques like nogood\nlearning and impact based branching heuristics. They may also help SAT and IP\nsolvers take advantage of the propagation of global constraints.\n"
  },
  {
    "id": "0909.3276",
    "title": "Symmetries of Symmetry Breaking Constraints",
    "abstract": "  Symmetry is an important feature of many constraint programs. We show that\nany symmetry acting on a set of symmetry breaking constraints can be used to\nbreak symmetry. Different symmetries pick out different solutions in each\nsymmetry class. We use these observations in two methods for eliminating\nsymmetry from a problem. These methods are designed to have many of the\nadvantages of symmetry breaking methods that post static symmetry breaking\nconstraint without some of the disadvantages. In particular, the two methods\nprune the search space using fast and efficient propagation of posted\nconstraints, whilst reducing the conflict between symmetry breaking and\nbranching heuristics. Experimental results show that the two methods perform\nwell on some standard benchmarks.\n"
  },
  {
    "id": "0909.4446",
    "title": "Elicitation strategies for fuzzy constraint problems with missing\n  preferences: algorithms and experimental studies",
    "abstract": "  Fuzzy constraints are a popular approach to handle preferences and\nover-constrained problems in scenarios where one needs to be cautious, such as\nin medical or space applications. We consider here fuzzy constraint problems\nwhere some of the preferences may be missing. This models, for example,\nsettings where agents are distributed and have privacy issues, or where there\nis an ongoing preference elicitation process. In this setting, we study how to\nfind a solution which is optimal irrespective of the missing preferences. In\nthe process of finding such a solution, we may elicit preferences from the user\nif necessary. However, our goal is to ask the user as little as possible. We\ndefine a combined solving and preference elicitation scheme with a large number\nof different instantiations, each corresponding to a concrete algorithm which\nwe compare experimentally. We compute both the number of elicited preferences\nand the \"user effort\", which may be larger, as it contains all the preference\nvalues the user has to compute to be able to respond to the elicitation\nrequests. While the number of elicited preferences is important when the\nconcern is to communicate as little information as possible, the user effort\nmeasures also the hidden work the user has to do to be able to communicate the\nelicited preferences. Our experimental results show that some of our algorithms\nare very good at finding a necessarily optimal solution while asking the user\nfor only a very small fraction of the missing preferences. The user effort is\nalso very small for the best algorithms. Finally, we test these algorithms on\nhard constraint problems with possibly missing constraints, where the aim is to\nfind feasible solutions irrespective of the missing constraints.\n"
  },
  {
    "id": "0909.4452",
    "title": "Flow-Based Propagators for the SEQUENCE and Related Global Constraints",
    "abstract": "  We propose new filtering algorithms for the SEQUENCE constraint and some\nextensions of the SEQUENCE constraint based on network flows. We enforce domain\nconsistency on the SEQUENCE constraint in $O(n^2)$ time down a branch of the\nsearch tree. This improves upon the best existing domain consistency algorithm\nby a factor of $O(\\log n)$. The flows used in these algorithms are derived from\na linear program. Some of them differ from the flows used to propagate global\nconstraints like GCC since the domains of the variables are encoded as costs on\nthe edges rather than capacities. Such flows are efficient for maintaining\nbounds consistency over large domains and may be useful for other global\nconstraints.\n"
  },
  {
    "id": "0909.4456",
    "title": "The Weighted CFG Constraint",
    "abstract": "  We introduce the weighted CFG constraint and propose a propagation algorithm\nthat enforces domain consistency in $O(n^3|G|)$ time. We show that this\nalgorithm can be decomposed into a set of primitive arithmetic constraints\nwithout hindering propagation.\n"
  },
  {
    "id": "0910.1014",
    "title": "Building upon Fast Multipole Methods to Detect and Model Organizations",
    "abstract": "  Many models in natural and social sciences are comprised of sets of\ninter-acting entities whose intensity of interaction decreases with distance.\nThis often leads to structures of interest in these models composed of dense\npacks of entities. Fast Multipole Methods are a family of methods developed to\nhelp with the calculation of a number of computable models such as described\nabove. We propose a method that builds upon FMM to detect and model the dense\nstructures of these systems.\n"
  },
  {
    "id": "0910.1026",
    "title": "A multiagent urban traffic simulation. Part II: dealing with the\n  extraordinary",
    "abstract": "  In Probabilistic Risk Management, risk is characterized by two quantities:\nthe magnitude (or severity) of the adverse consequences that can potentially\nresult from the given activity or action, and by the likelihood of occurrence\nof the given adverse consequences. But a risk seldom exists in isolation: chain\nof consequences must be examined, as the outcome of one risk can increase the\nlikelihood of other risks. Systemic theory must complement classic PRM. Indeed\nthese chains are composed of many different elements, all of which may have a\ncritical importance at many different levels. Furthermore, when urban\ncatastrophes are envisioned, space and time constraints are key determinants of\nthe workings and dynamics of these chains of catastrophes: models must include\na correct spatial topology of the studied risk. Finally, literature insists on\nthe importance small events can have on the risk on a greater scale: urban\nrisks management models belong to self-organized criticality theory. We chose\nmultiagent systems to incorporate this property in our model: the behavior of\nan agent can transform the dynamics of important groups of them.\n"
  },
  {
    "id": "0910.1238",
    "title": "A Local Search Modeling for Constrained Optimum Paths Problems (Extended\n  Abstract)",
    "abstract": "  Constrained Optimum Path (COP) problems appear in many real-life\napplications, especially on communication networks. Some of these problems have\nbeen considered and solved by specific techniques which are usually difficult\nto extend. In this paper, we introduce a novel local search modeling for\nsolving some COPs by local search. The modeling features the compositionality,\nmodularity, reuse and strengthens the benefits of Constrained-Based Local\nSearch. We also apply the modeling to the edge-disjoint paths problem (EDP). We\nshow that side constraints can easily be added in the model. Computational\nresults show the significance of the approach.\n"
  },
  {
    "id": "0910.1239",
    "title": "Dynamic Demand-Capacity Balancing for Air Traffic Management Using\n  Constraint-Based Local Search: First Results",
    "abstract": "  Using constraint-based local search, we effectively model and efficiently\nsolve the problem of balancing the traffic demands on portions of the European\nairspace while ensuring that their capacity constraints are satisfied. The\ntraffic demand of a portion of airspace is the hourly number of flights planned\nto enter it, and its capacity is the upper bound on this number under which\nair-traffic controllers can work. Currently, the only form of demand-capacity\nbalancing we allow is ground holding, that is the changing of the take-off\ntimes of not yet airborne flights. Experiments with projected European flight\nplans of the year 2030 show that already this first form of demand-capacity\nbalancing is feasible without incurring too much total delay and that it can\nlead to a significantly better demand-capacity balance.\n"
  },
  {
    "id": "0910.1244",
    "title": "On Improving Local Search for Unsatisfiability",
    "abstract": "  Stochastic local search (SLS) has been an active field of research in the\nlast few years, with new techniques and procedures being developed at an\nastonishing rate. SLS has been traditionally associated with satisfiability\nsolving, that is, finding a solution for a given problem instance, as its\nintrinsic nature does not address unsatisfiable problems. Unsatisfiable\ninstances were therefore commonly solved using backtrack search solvers. For\nthis reason, in the late 90s Selman, Kautz and McAllester proposed a challenge\nto use local search instead to prove unsatisfiability. More recently, two SLS\nsolvers - Ranger and Gunsat - have been developed, which are able to prove\nunsatisfiability albeit being SLS solvers. In this paper, we first compare\nRanger with Gunsat and then propose to improve Ranger performance using some of\nGunsat's techniques, namely unit propagation look-ahead and extended\nresolution.\n"
  },
  {
    "id": "0910.1247",
    "title": "Integrating Conflict Driven Clause Learning to Local Search",
    "abstract": "  This article introduces SatHyS (SAT HYbrid Solver), a novel hybrid approach\nfor propositional satisfiability. It combines local search and conflict driven\nclause learning (CDCL) scheme. Each time the local search part reaches a local\nminimum, the CDCL is launched. For SAT problems it behaves like a tabu list,\nwhereas for UNSAT ones, the CDCL part tries to focus on minimum unsatisfiable\nsub-formula (MUS). Experimental results show good performances on many classes\nof SAT instances from the last SAT competitions.\n"
  },
  {
    "id": "0910.1253",
    "title": "A Constraint-directed Local Search Approach to Nurse Rostering Problems",
    "abstract": "  In this paper, we investigate the hybridization of constraint programming and\nlocal search techniques within a large neighbourhood search scheme for solving\nhighly constrained nurse rostering problems. As identified by the research, a\ncrucial part of the large neighbourhood search is the selection of the fragment\n(neighbourhood, i.e. the set of variables), to be relaxed and re-optimized\niteratively. The success of the large neighbourhood search depends on the\nadequacy of this identified neighbourhood with regard to the problematic part\nof the solution assignment and the choice of the neighbourhood size. We\ninvestigate three strategies to choose the fragment of different sizes within\nthe large neighbourhood search scheme. The first two strategies are tailored\nconcerning the problem properties. The third strategy is more general, using\nthe information of the cost from the soft constraint violations and their\npropagation as the indicator to choose the variables added into the fragment.\nThe three strategies are analyzed and compared upon a benchmark nurse rostering\nproblem. Promising results demonstrate the possibility of future work in the\nhybrid approach.\n"
  },
  {
    "id": "0910.1255",
    "title": "Sonet Network Design Problems",
    "abstract": "  This paper presents a new method and a constraint-based objective function to\nsolve two problems related to the design of optical telecommunication networks,\nnamely the Synchronous Optical Network Ring Assignment Problem (SRAP) and the\nIntra-ring Synchronous Optical Network Design Problem (IDP). These network\ntopology problems can be represented as a graph partitioning with capacity\nconstraints as shown in previous works. We present here a new objective\nfunction and a new local search algorithm to solve these problems. Experiments\nconducted in Comet allow us to compare our method to previous ones and show\nthat we obtain better results.\n"
  },
  {
    "id": "0910.1264",
    "title": "Parallel local search for solving Constraint Problems on the Cell\n  Broadband Engine (Preliminary Results)",
    "abstract": "  We explore the use of the Cell Broadband Engine (Cell/BE for short) for\ncombinatorial optimization applications: we present a parallel version of a\nconstraint-based local search algorithm that has been implemented on a\nmultiprocessor BladeCenter machine with twin Cell/BE processors (total of 16\nSPUs per blade). This algorithm was chosen because it fits very well the\nCell/BE architecture and requires neither shared memory nor communication\nbetween processors, while retaining a compact memory footprint. We study the\nperformance on several large optimization benchmarks and show that this\nachieves mostly linear time speedups, even sometimes super-linear. This is\npossible because the parallel implementation might explore simultaneously\ndifferent parts of the search space and therefore converge faster towards the\nbest sub-space and thus towards a solution. Besides getting speedups, the\nresulting times exhibit a much smaller variance, which benefits applications\nwhere a timely reply is critical.\n"
  },
  {
    "id": "0910.1266",
    "title": "Toward an automaton Constraint for Local Search",
    "abstract": "  We explore the idea of using finite automata to implement new constraints for\nlocal search (this is already a successful technique in constraint-based global\nsearch). We show how it is possible to maintain incrementally the violations of\na constraint and its decision variables from an automaton that describes a\nground checker for that constraint. We establish the practicality of our\napproach idea on real-life personnel rostering problems, and show that it is\ncompetitive with the approach of [Pralong, 2007].\n"
  },
  {
    "id": "0910.1404",
    "title": "Proceedings 6th International Workshop on Local Search Techniques in\n  Constraint Satisfaction",
    "abstract": "  LSCS is a satellite workshop of the international conference on principles\nand practice of Constraint Programming (CP), since 2004. It is devoted to local\nsearch techniques in constraint satisfaction, and focuses on all aspects of\nlocal search techniques, including: design and implementation of new\nalgorithms, hybrid stochastic-systematic search, reactive search optimization,\nadaptive search, modeling for local-search, global constraints, flexibility and\nrobustness, learning methods, and specific applications.\n"
  },
  {
    "id": "0910.1433",
    "title": "Tracking object's type changes with fuzzy based fusion rule",
    "abstract": "  In this paper the behavior of three combinational rules for\ntemporal/sequential attribute data fusion for target type estimation are\nanalyzed. The comparative analysis is based on: Dempster's fusion rule proposed\nin Dempster-Shafer Theory; Proportional Conflict Redistribution rule no. 5\n(PCR5), proposed in Dezert-Smarandache Theory and one alternative class fusion\nrule, connecting the combination rules for information fusion with particular\nfuzzy operators, focusing on the t-norm based Conjunctive rule as an analog of\nthe ordinary conjunctive rule and t-conorm based Disjunctive rule as an analog\nof the ordinary disjunctive rule. The way how different t-conorms and t-norms\nfunctions within TCN fusion rule influence over target type estimation\nperformance is studied and estimated.\n"
  },
  {
    "id": "0910.2217",
    "title": "Finite element model selection using Particle Swarm Optimization",
    "abstract": "  This paper proposes the application of particle swarm optimization (PSO) to\nthe problem of finite element model (FEM) selection. This problem arises when a\nchoice of the best model for a system has to be made from set of competing\nmodels, each developed a priori from engineering judgment. PSO is a\npopulation-based stochastic search algorithm inspired by the behaviour of\nbiological entities in nature when they are foraging for resources. Each\npotentially correct model is represented as a particle that exhibits both\nindividualistic and group behaviour. Each particle moves within the model\nsearch space looking for the best solution by updating the parameters values\nthat define it. The most important step in the particle swarm algorithm is the\nmethod of representing models which should take into account the number,\nlocation and variables of parameters to be updated. One example structural\nsystem is used to show the applicability of PSO in finding an optimal FEM. An\noptimal model is defined as the model that has the least number of updated\nparameters and has the smallest parameter variable variation from the mean\nmaterial properties. Two different objective functions are used to compare\nperformance of the PSO algorithm.\n"
  },
  {
    "id": "0910.3485",
    "title": "A Fuzzy Petri Nets Model for Computing With Words",
    "abstract": "  Motivated by Zadeh's paradigm of computing with words rather than numbers,\nseveral formal models of computing with words have recently been proposed.\nThese models are based on automata and thus are not well-suited for concurrent\ncomputing. In this paper, we incorporate the well-known model of concurrent\ncomputing, Petri nets, together with fuzzy set theory and thereby establish a\nconcurrency model of computing with words--fuzzy Petri nets for computing with\nwords (FPNCWs). The new feature of such fuzzy Petri nets is that the labels of\ntransitions are some special words modeled by fuzzy sets. By employing the\nmethodology of fuzzy reasoning, we give a faithful extension of an FPNCW which\nmakes it possible for computing with more words. The language expressiveness of\nthe two formal models of computing with words, fuzzy automata for computing\nwith words and FPNCWs, is compared as well. A few small examples are provided\nto illustrate the theoretical development.\n"
  },
  {
    "id": "0911.2405",
    "title": "Emotion: Appraisal-coping model for the \"Cascades\" problem",
    "abstract": "  Modelling emotion has become a challenge nowadays. Therefore, several models\nhave been produced in order to express human emotional activity. However, only\na few of them are currently able to express the close relationship existing\nbetween emotion and cognition. An appraisal-coping model is presented here,\nwith the aim to simulate the emotional impact caused by the evaluation of a\nparticular situation (appraisal), along with the consequent cognitive reaction\nintended to face the situation (coping). This model is applied to the\n\"Cascades\" problem, a small arithmetical exercise designed for ten-year-old\npupils. The goal is to create a model corresponding to a child's behaviour when\nsolving the problem using his own strategies.\n"
  },
  {
    "id": "0911.2501",
    "title": "Emotion : mod\\`ele d'appraisal-coping pour le probl\\`eme des Cascades",
    "abstract": "  Modeling emotion has become a challenge nowadays. Therefore, several models\nhave been produced in order to express human emotional activity. However, only\na few of them are currently able to express the close relationship existing\nbetween emotion and cognition. An appraisal-coping model is presented here,\nwith the aim to simulate the emotional impact caused by the evaluation of a\nparticular situation (appraisal), along with the consequent cognitive reaction\nintended to face the situation (coping). This model is applied to the\n?Cascades? problem, a small arithmetical exercise designed for ten-year-old\npupils. The goal is to create a model corresponding to a child's behavior when\nsolving the problem using his own strategies.\n"
  },
  {
    "id": "0911.5394",
    "title": "Covering rough sets based on neighborhoods: An approach without using\n  neighborhoods",
    "abstract": "  Rough set theory, a mathematical tool to deal with inexact or uncertain\nknowledge in information systems, has originally described the indiscernibility\nof elements by equivalence relations. Covering rough sets are a natural\nextension of classical rough sets by relaxing the partitions arising from\nequivalence relations to coverings. Recently, some topological concepts such as\nneighborhood have been applied to covering rough sets. In this paper, we\nfurther investigate the covering rough sets based on neighborhoods by\napproximation operations. We show that the upper approximation based on\nneighborhoods can be defined equivalently without using neighborhoods. To\nanalyze the coverings themselves, we introduce unary and composition operations\non coverings. A notion of homomorphismis provided to relate two covering\napproximation spaces. We also examine the properties of approximations\npreserved by the operations and homomorphisms, respectively.\n"
  },
  {
    "id": "0911.5395",
    "title": "An axiomatic approach to the roughness measure of rough sets",
    "abstract": "  In Pawlak's rough set theory, a set is approximated by a pair of lower and\nupper approximations. To measure numerically the roughness of an approximation,\nPawlak introduced a quantitative measure of roughness by using the ratio of the\ncardinalities of the lower and upper approximations. Although the roughness\nmeasure is effective, it has the drawback of not being strictly monotonic with\nrespect to the standard ordering on partitions. Recently, some improvements\nhave been made by taking into account the granularity of partitions. In this\npaper, we approach the roughness measure in an axiomatic way. After\naxiomatically defining roughness measure and partition measure, we provide a\nunified construction of roughness measure, called strong Pawlak roughness\nmeasure, and then explore the properties of this measure. We show that the\nimproved roughness measures in the literature are special instances of our\nstrong Pawlak roughness measure and introduce three more strong Pawlak\nroughness measures as well. The advantage of our axiomatic approach is that\nsome properties of a roughness measure follow immediately as soon as the\nmeasure satisfies the relevant axiomatic definition.\n"
  },
  {
    "id": "0912.0132",
    "title": "Opportunistic Adaptation Knowledge Discovery",
    "abstract": "  Adaptation has long been considered as the Achilles' heel of case-based\nreasoning since it requires some domain-specific knowledge that is difficult to\nacquire. In this paper, two strategies are combined in order to reduce the\nknowledge engineering cost induced by the adaptation knowledge (CA) acquisition\ntask: CA is learned from the case base by the means of knowledge discovery\ntechniques, and the CA acquisition sessions are opportunistically triggered,\ni.e., at problem-solving time.\n"
  },
  {
    "id": "0912.3228",
    "title": "On Backtracking in Real-time Heuristic Search",
    "abstract": "  Real-time heuristic search algorithms are suitable for situated agents that\nneed to make their decisions in constant time. Since the original work by Korf\nnearly two decades ago, numerous extensions have been suggested. One of the\nmost intriguing extensions is the idea of backtracking wherein the agent\ndecides to return to a previously visited state as opposed to moving forward\ngreedily. This idea has been empirically shown to have a significant impact on\nvarious performance measures. The studies have been carried out in particular\nempirical testbeds with specific real-time search algorithms that use\nbacktracking. Consequently, the extent to which the trends observed are\ncharacteristic of backtracking in general is unclear. In this paper, we present\nthe first entirely theoretical study of backtracking in real-time heuristic\nsearch. In particular, we present upper bounds on the solution cost exponential\nand linear in a parameter regulating the amount of backtracking. The results\nhold for a wide class of real-time heuristic search algorithms that includes\nmany existing algorithms as a small subclass.\n"
  },
  {
    "id": "0912.3309",
    "title": "New Generalization Bounds for Learning Kernels",
    "abstract": "  This paper presents several novel generalization bounds for the problem of\nlearning kernels based on the analysis of the Rademacher complexity of the\ncorresponding hypothesis sets. Our bound for learning kernels with a convex\ncombination of p base kernels has only a log(p) dependency on the number of\nkernels, p, which is considerably more favorable than the previous best bound\ngiven for the same problem. We also give a novel bound for learning with a\nlinear combination of p base kernels with an L_2 regularization whose\ndependency on p is only in p^{1/4}.\n"
  },
  {
    "id": "0912.4584",
    "title": "A Necessary and Sufficient Condition for Graph Matching Being Equivalent\n  to the Maximum Weight Clique Problem",
    "abstract": "  This paper formulates a necessary and sufficient condition for a generic\ngraph matching problem to be equivalent to the maximum vertex and edge weight\nclique problem in a derived association graph. The consequences of this results\nare threefold: first, the condition is general enough to cover a broad range of\npractical graph matching problems; second, a proof to establish equivalence\nbetween graph matching and clique search reduces to showing that a given graph\nmatching problem satisfies the proposed condition; and third, the result sets\nthe scene for generic continuous solutions for a broad range of graph matching\nproblems. To illustrate the mathematical framework, we apply it to a number of\ngraph matching problems, including the problem of determining the graph edit\ndistance.\n"
  },
  {
    "id": "0912.4598",
    "title": "Elkan's k-Means for Graphs",
    "abstract": "  This paper extends k-means algorithms from the Euclidean domain to the domain\nof graphs. To recompute the centroids, we apply subgradient methods for solving\nthe optimization-based formulation of the sample mean of graphs. To accelerate\nthe k-means algorithm for graphs without trading computational time against\nsolution quality, we avoid unnecessary graph distance calculations by\nexploiting the triangle inequality of the underlying distance metric following\nElkan's k-means algorithm proposed in \\cite{Elkan03}. In experiments we show\nthat the accelerated k-means algorithm are faster than the standard k-means\nalgorithm for graphs provided there is a cluster structure in the data.\n"
  },
  {
    "id": "0912.4879",
    "title": "Similarit\\'e en intension vs en extension : \\`a la crois\\'ee de\n  l'informatique et du th\\'e\\^atre",
    "abstract": "  Traditional staging is based on a formal approach of similarity leaning on\ndramaturgical ontologies and instanciation variations. Inspired by interactive\ndata mining, that suggests different approaches, we give an overview of\ncomputer science and theater researches using computers as partners of the\nactor to escape the a priori specification of roles.\n"
  },
  {
    "id": "0912.5511",
    "title": "A general approach to belief change in answer set programming",
    "abstract": "  We address the problem of belief change in (nonmonotonic) logic programming\nunder answer set semantics. Unlike previous approaches to belief change in\nlogic programming, our formal techniques are analogous to those of\ndistance-based belief revision in propositional logic. In developing our\nresults, we build upon the model theory of logic programs furnished by SE\nmodels. Since SE models provide a formal, monotonic characterisation of logic\nprograms, we can adapt techniques from the area of belief revision to belief\nchange in logic programs. We introduce methods for revising and merging logic\nprograms, respectively. For the former, we study both subset-based revision as\nwell as cardinality-based revision, and we show that they satisfy the majority\nof the AGM postulates for revision. For merging, we consider operators\nfollowing arbitration merging and IC merging, respectively. We also present\nencodings for computing the revision as well as the merging of logic programs\nwithin the same logic programming framework, giving rise to a direct\nimplementation of our approach in terms of off-the-shelf answer set solvers.\nThese encodings reflect in turn the fact that our change operators do not\nincrease the complexity of the base formalism.\n"
  },
  {
    "id": "0912.5533",
    "title": "Oriented Straight Line Segment Algebra: Qualitative Spatial Reasoning\n  about Oriented Objects",
    "abstract": "  Nearly 15 years ago, a set of qualitative spatial relations between oriented\nstraight line segments (dipoles) was suggested by Schlieder. This work received\nsubstantial interest amongst the qualitative spatial reasoning community.\nHowever, it turned out to be difficult to establish a sound constraint calculus\nbased on these relations. In this paper, we present the results of a new\ninvestigation into dipole constraint calculi which uses algebraic methods to\nderive sound results on the composition of relations and other properties of\ndipole calculi. Our results are based on a condensed semantics of the dipole\nrelations.\n  In contrast to the points that are normally used, dipoles are extended and\nhave an intrinsic direction. Both features are important properties of natural\nobjects. This allows for a straightforward representation of prototypical\nreasoning tasks for spatial agents. As an example, we show how to generate\nsurvey knowledge from local observations in a street network. The example\nillustrates the fast constraint-based reasoning capabilities of the dipole\ncalculus. We integrate our results into two reasoning tools which are publicly\navailable.\n"
  },
  {
    "id": "1001.0063",
    "title": "On a Model for Integrated Information",
    "abstract": "  In this paper we give a thorough presentation of a model proposed by Tononi\net al. for modeling \\emph{integrated information}, i.e. how much information is\ngenerated in a system transitioning from one state to the next one by the\ncausal interaction of its parts and \\emph{above and beyond} the information\ngiven by the sum of its parts. We also provides a more general formulation of\nsuch a model, independent from the time chosen for the analysis and from the\nuniformity of the probability distribution at the initial time instant.\nFinally, we prove that integrated information is null for disconnected systems.\n"
  },
  {
    "id": "1001.0921",
    "title": "Graph Quantization",
    "abstract": "  Vector quantization(VQ) is a lossy data compression technique from signal\nprocessing, which is restricted to feature vectors and therefore inapplicable\nfor combinatorial structures. This contribution presents a theoretical\nfoundation of graph quantization (GQ) that extends VQ to the domain of\nattributed graphs. We present the necessary Lloyd-Max conditions for optimality\nof a graph quantizer and consistency results for optimal GQ design based on\nempirical distortion measures and stochastic optimization. These results\nstatistically justify existing clustering algorithms in the domain of graphs.\nThe proposed approach provides a template of how to link structural pattern\nrecognition methods other than GQ to statistical pattern recognition.\n"
  },
  {
    "id": "1001.1257",
    "title": "Decisional Processes with Boolean Neural Network: the Emergence of\n  Mental Schemes",
    "abstract": "  Human decisional processes result from the employment of selected quantities\nof relevant information, generally synthesized from environmental incoming data\nand stored memories. Their main goal is the production of an appropriate and\nadaptive response to a cognitive or behavioral task. Different strategies of\nresponse production can be adopted, among which haphazard trials, formation of\nmental schemes and heuristics. In this paper, we propose a model of Boolean\nneural network that incorporates these strategies by recurring to global\noptimization strategies during the learning session. The model characterizes as\nwell the passage from an unstructured/chaotic attractor neural network typical\nof data-driven processes to a faster one, forward-only and representative of\nschema-driven processes. Moreover, a simplified version of the Iowa Gambling\nTask (IGT) is introduced in order to test the model. Our results match with\nexperimental data and point out some relevant knowledge coming from\npsychological domain.\n"
  },
  {
    "id": "1001.1836",
    "title": "Web-Based Expert System for Civil Service Regulations: RCSES",
    "abstract": "  Internet and expert systems have offered new ways of sharing and distributing\nknowledge, but there is a lack of researches in the area of web based expert\nsystems. This paper introduces a development of a web-based expert system for\nthe regulations of civil service in the Kingdom of Saudi Arabia named as RCSES.\nIt is the first time to develop such system (application of civil service\nregulations) as well the development of it using web based approach. The\nproposed system considers 17 regulations of the civil service system. The\ndifferent phases of developing the RCSES system are presented, as knowledge\nacquiring and selection, ontology and knowledge representations using XML\nformat. XML Rule-based knowledge sources and the inference mechanisms were\nimplemented using ASP.net technique. An interactive tool for entering the\nontology and knowledge base, and the inferencing was built. It gives the\nability to use, modify, update, and extend the existing knowledge base in an\neasy way. The knowledge was validated by experts in the domain of civil service\nregulations, and the proposed RCSES was tested, verified, and validated by\ndifferent technical users and the developers staff. The RCSES system is\ncompared with other related web based expert systems, that comparison proved\nthe goodness, usability, and high performance of RCSES.\n"
  },
  {
    "id": "1001.2277",
    "title": "Application of a Fuzzy Programming Technique to Production Planning in\n  the Textile Industry",
    "abstract": "  Many engineering optimization problems can be considered as linear\nprogramming problems where all or some of the parameters involved are\nlinguistic in nature. These can only be quantified using fuzzy sets. The aim of\nthis paper is to solve a fuzzy linear programming problem in which the\nparameters involved are fuzzy quantities with logistic membership functions. To\nexplore the applicability of the method a numerical example is considered to\ndetermine the monthly production planning quotas and profit of a home textile\ngroup.\n"
  },
  {
    "id": "1001.2279",
    "title": "The Application of Mamdani Fuzzy Model for Auto Zoom Function of a\n  Digital Camera",
    "abstract": "  Mamdani Fuzzy Model is an important technique in Computational Intelligence\n(CI) study. This paper presents an implementation of a supervised learning\nmethod based on membership function training in the context of Mamdani fuzzy\nmodels. Specifically, auto zoom function of a digital camera is modelled using\nMamdani technique. The performance of control method is verified through a\nseries of simulation and numerical results are provided as illustrations.\n"
  },
  {
    "id": "1002.0102",
    "title": "$\\alpha$-Discounting Multi-Criteria Decision Making ($\\alpha$-D MCDM)",
    "abstract": "  In this book we introduce a new procedure called \\alpha-Discounting Method\nfor Multi-Criteria Decision Making (\\alpha-D MCDM), which is as an alternative\nand extension of Saaty Analytical Hierarchy Process (AHP). It works for any\nnumber of preferences that can be transformed into a system of homogeneous\nlinear equations. A degree of consistency (and implicitly a degree of\ninconsistency) of a decision-making problem are defined. \\alpha-D MCDM is\nafterwards generalized to a set of preferences that can be transformed into a\nsystem of linear and or non-linear homogeneous and or non-homogeneous equations\nand or inequalities. The general idea of \\alpha-D MCDM is to assign non-null\npositive parameters \\alpha_1, \\alpha_2, and so on \\alpha_p to the coefficients\nin the right-hand side of each preference that diminish or increase them in\norder to transform the above linear homogeneous system of equations which has\nonly the null-solution, into a system having a particular non-null solution.\nAfter finding the general solution of this system, the principles used to\nassign particular values to all parameters \\alpha is the second important part\nof \\alpha-D, yet to be deeper investigated in the future. In the current book\nwe propose the Fairness Principle, i.e. each coefficient should be discounted\nwith the same percentage (we think this is fair: not making any favoritism or\nunfairness to any coefficient), but the reader can propose other principles.\nFor consistent decision-making problems with pairwise comparisons,\n\\alpha-Discounting Method together with the Fairness Principle give the same\nresult as AHP. But for weak inconsistent decision-making problem,\n\\alpha-Discounting together with the Fairness Principle give a different result\nfrom AHP. Many consistent, weak inconsistent, and strong inconsistent examples\nare given in this book.\n"
  },
  {
    "id": "1002.0136",
    "title": "Dominion -- A constraint solver generator",
    "abstract": "  This paper proposes a design for a system to generate constraint solvers that\nare specialised for specific problem models. It describes the design in detail\nand gives preliminary experimental results showing the feasibility and\neffectiveness of the approach.\n"
  },
  {
    "id": "1002.0177",
    "title": "Logical Evaluation of Consciousness: For Incorporating Consciousness\n  into Machine Architecture",
    "abstract": "  Machine Consciousness is the study of consciousness in a biological,\nphilosophical, mathematical and physical perspective and designing a model that\ncan fit into a programmable system architecture. Prime objective of the study\nis to make the system architecture behave consciously like a biological model\ndoes. Present work has developed a feasible definition of consciousness, that\ncharacterizes consciousness with four parameters i.e., parasitic, symbiotic,\nself referral and reproduction. Present work has also developed a biologically\ninspired consciousness architecture that has following layers: quantum layer,\ncellular layer, organ layer and behavioral layer and traced the characteristics\nof consciousness at each layer. Finally, the work has estimated physical and\nalgorithmic architecture to devise a system that can behave consciously.\n"
  },
  {
    "id": "1002.0449",
    "title": "Some improved results on communication between information systems",
    "abstract": "  To study the communication between information systems, Wang et al. [C. Wang,\nC. Wu, D. Chen, Q. Hu, and C. Wu, Communicating between information systems,\nInformation Sciences 178 (2008) 3228-3239] proposed two concepts of type-1 and\ntype-2 consistent functions. Some properties of such functions and induced\nrelation mappings have been investigated there. In this paper, we provide an\nimprovement of the aforementioned work by disclosing the symmetric relationship\nbetween type-1 and type-2 consistent functions. We present more properties of\nconsistent functions and induced relation mappings and improve upon several\ndeficient assertions in the original work. In particular, we unify and extend\ntype-1 and type-2 consistent functions into the so-called\nneighborhood-consistent functions. This provides a convenient means for\nstudying the communication between information systems based on various\nneighborhoods.\n"
  },
  {
    "id": "1002.0908",
    "title": "Homomorphisms between fuzzy information systems revisited",
    "abstract": "  Recently, Wang et al. discussed the properties of fuzzy information systems\nunder homomorphisms in the paper [C. Wang, D. Chen, L. Zhu, Homomorphisms\nbetween fuzzy information systems, Applied Mathematics Letters 22 (2009)\n1045-1050], where homomorphisms are based upon the concepts of consistent\nfunctions and fuzzy relation mappings. In this paper, we classify consistent\nfunctions as predecessor-consistent and successor-consistent, and then proceed\nto present more properties of consistent functions. In addition, we improve\nsome characterizations of fuzzy relation mappings provided by Wang et al.\n"
  },
  {
    "id": "1002.1157",
    "title": "Establishment of Relationships between Material Design and Product\n  Design Domains by Hybrid FEM-ANN Technique",
    "abstract": "  In this paper, research on AI based modeling technique to optimize\ndevelopment of new alloys with necessitated improvements in properties and\nchemical mixture over existing alloys as per functional requirements of product\nis done. The current research work novels AI in lieu of predictions to\nestablish association between material and product customary. Advanced\ncomputational simulation techniques like CFD, FEA interrogations are made\nviable to authenticate product dynamics in context to experimental\ninvestigations. Accordingly, the current research is focused towards binding\nrelationships between material design and product design domains. The input to\nfeed forward back propagation prediction network model constitutes of material\ndesign features. Parameters relevant to product design strategies are furnished\nas target outputs. The outcomes of ANN shows good sign of correlation between\nmaterial and product design domains. The study enriches a new path to\nillustrate material factors at the time of new product development.\n"
  },
  {
    "id": "1002.2202",
    "title": "Modeling of Human Criminal Behavior using Probabilistic Networks",
    "abstract": "  Currently, criminals profile (CP) is obtained from investigators or forensic\npsychologists interpretation, linking crime scene characteristics and an\noffenders behavior to his or her characteristics and psychological profile.\nThis paper seeks an efficient and systematic discovery of nonobvious and\nvaluable patterns between variables from a large database of solved cases via a\nprobabilistic network (PN) modeling approach. The PN structure can be used to\nextract behavioral patterns and to gain insight into what factors influence\nthese behaviors. Thus, when a new case is being investigated and the profile\nvariables are unknown because the offender has yet to be identified, the\nobserved crime scene variables are used to infer the unknown variables based on\ntheir connections in the structure and the corresponding numerical\n(probabilistic) weights. The objective is to produce a more systematic and\nempirical approach to profiling, and to use the resulting PN model as a\ndecision tool.\n"
  },
  {
    "id": "1002.2897",
    "title": "Model-Driven Constraint Programming",
    "abstract": "  Constraint programming can definitely be seen as a model-driven paradigm. The\nusers write programs for modeling problems. These programs are mapped to\nexecutable models to calculate the solutions. This paper focuses on efficient\nmodel management (definition and transformation). From this point of view, we\npropose to revisit the design of constraint-programming systems. A model-driven\narchitecture is introduced to map solving-independent constraint models to\nsolving-dependent decision models. Several important questions are examined,\nsuch as the need for a visual highlevel modeling language, and the quality of\nmetamodeling techniques to implement the transformations. A main result is the\ns-COMMA platform that efficiently implements the chain from modeling to solving\nconstraint problems\n"
  },
  {
    "id": "1002.3023",
    "title": "Rewriting Constraint Models with Metamodels",
    "abstract": "  An important challenge in constraint programming is to rewrite constraint\nmodels into executable programs calculat- ing the solutions. This phase of\nconstraint processing may require translations between constraint programming\nlan- guages, transformations of constraint representations, model\noptimizations, and tuning of solving strategies. In this paper, we introduce a\npivot metamodel describing the common fea- tures of constraint models including\ndifferent kinds of con- straints, statements like conditionals and loops, and\nother first-class elements like object classes and predicates. This metamodel\nis general enough to cope with the constructions of many languages, from\nobject-oriented modeling languages to logic languages, but it is independent\nfrom them. The rewriting operations manipulate metamodel instances apart from\nlanguages. As a consequence, the rewriting operations apply whatever languages\nare selected and they are able to manage model semantic information. A bridge\nis created between the metamodel space and languages using parsing techniques.\nTools from the software engineering world can be useful to implement this\nframework.\n"
  },
  {
    "id": "1002.3078",
    "title": "Using ATL to define advanced and flexible constraint model\n  transformations",
    "abstract": "  Transforming constraint models is an important task in re- cent constraint\nprogramming systems. User-understandable models are defined during the modeling\nphase but rewriting or tuning them is manda- tory to get solving-efficient\nmodels. We propose a new architecture al- lowing to define bridges between any\n(modeling or solver) languages and to implement model optimizations. This\narchitecture follows a model- driven approach where the constraint modeling\nprocess is seen as a set of model transformations. Among others, an interesting\nfeature is the def- inition of transformations as concept-oriented rules, i.e.\nbased on types of model elements where the types are organized into a hierarchy\ncalled a metamodel.\n"
  },
  {
    "id": "1002.4522",
    "title": "Feature Importance in Bayesian Assessment of Newborn Brain Maturity from\n  EEG",
    "abstract": "  The methodology of Bayesian Model Averaging (BMA) is applied for assessment\nof newborn brain maturity from sleep EEG. In theory this methodology provides\nthe most accurate assessments of uncertainty in decisions. However, the\nexisting BMA techniques have been shown providing biased assessments in the\nabsence of some prior information enabling to explore model parameter space in\ndetails within a reasonable time. The lack in details leads to disproportional\nsampling from the posterior distribution. In case of the EEG assessment of\nbrain maturity, BMA results can be biased because of the absence of information\nabout EEG feature importance. In this paper we explore how the posterior\ninformation about EEG features can be used in order to reduce a negative impact\nof disproportional sampling on BMA performance. We use EEG data recorded from\nsleeping newborns to test the efficiency of the proposed BMA technique.\n"
  },
  {
    "id": "1003.0590",
    "title": "A new model for solution of complex distributed constrained problems",
    "abstract": "  In this paper we describe an original computational model for solving\ndifferent types of Distributed Constraint Satisfaction Problems (DCSP). The\nproposed model is called Controller-Agents for Constraints Solving (CACS). This\nmodel is intended to be used which is an emerged field from the integration\nbetween two paradigms of different nature: Multi-Agent Systems (MAS) and the\nConstraint Satisfaction Problem paradigm (CSP) where all constraints are\ntreated in central manner as a black-box. This model allows grouping\nconstraints to form a subset that will be treated together as a local problem\ninside the controller. Using this model allows also handling non-binary\nconstraints easily and directly so that no translating of constraints into\nbinary ones is needed. This paper presents the implementation outlines of a\nprototype of DCSP solver, its usage methodology and overview of the CACS\napplication for timetabling problems.\n"
  },
  {
    "id": "1003.0746",
    "title": "Automatically Discovering Hidden Transformation Chaining Constraints",
    "abstract": "  Model transformations operate on models conforming to precisely defined\nmetamodels. Consequently, it often seems relatively easy to chain them: the\noutput of a transformation may be given as input to a second one if metamodels\nmatch. However, this simple rule has some obvious limitations. For instance, a\ntransformation may only use a subset of a metamodel. Therefore, chaining\ntransformations appropriately requires more information. We present here an\napproach that automatically discovers more detailed information about actual\nchaining constraints by statically analyzing transformations. The objective is\nto provide developers who decide to chain transformations with more data on\nwhich to base their choices. This approach has been successfully applied to the\ncase of a library of endogenous transformations. They all have the same source\nand target metamodel but have some hidden chaining constraints. In such a case,\nthe simple metamodel matching rule given above does not provide any useful\ninformation.\n"
  },
  {
    "id": "1003.1493",
    "title": "Integration of Rule Based Expert Systems and Case Based Reasoning in an\n  Acute Bacterial Meningitis Clinical Decision Support System",
    "abstract": "  This article presents the results of the research carried out on the\ndevelopment of a medical diagnostic system applied to the Acute Bacterial\nMeningitis, using the Case Based Reasoning methodology. The research was\nfocused on the implementation of the adaptation stage, from the integration of\nCase Based Reasoning and Rule Based Expert Systems. In this adaptation stage we\nuse a higher level RBC that stores and allows reutilizing change experiences,\ncombined with a classic rule-based inference engine. In order to take into\naccount the most evident clinical situation, a pre-diagnosis stage is\nimplemented using a rule engine that, given an evident situation, emits the\ncorresponding diagnosis and avoids the complete process.\n"
  },
  {
    "id": "1003.1504",
    "title": "Indexer Based Dynamic Web Services Discovery",
    "abstract": "  Recent advancement in web services plays an important role in business to\nbusiness and business to consumer interaction. Discovery mechanism is not only\nused to find a suitable service but also provides collaboration between service\nproviders and consumers by using standard protocols. A static web service\ndiscovery mechanism is not only time consuming but requires continuous human\ninteraction. This paper proposed an efficient dynamic web services discovery\nmechanism that can locate relevant and updated web services from service\nregistries and repositories with timestamp based on indexing value and\ncategorization for faster and efficient discovery of service. The proposed\nprototype focuses on quality of service issues and introduces concept of local\ncache, categorization of services, indexing mechanism, CSP (Constraint\nSatisfaction Problem) solver, aging and usage of translator. Performance of\nproposed framework is evaluated by implementing the algorithm and correctness\nof our method is shown. The results of proposed framework shows greater\nperformance and accuracy in dynamic discovery mechanism of web services\nresolving the existing issues of flexibility, scalability, based on quality of\nservice, and discovers updated and most relevant services with ease of usage.\n"
  },
  {
    "id": "1003.1588",
    "title": "On the Failure of the Finite Model Property in some Fuzzy Description\n  Logics",
    "abstract": "  Fuzzy Description Logics (DLs) are a family of logics which allow the\nrepresentation of (and the reasoning with) structured knowledge affected by\nvagueness. Although most of the not very expressive crisp DLs, such as ALC,\nenjoy the Finite Model Property (FMP), this is not the case once we move into\nthe fuzzy case. In this paper we show that if we allow arbitrary knowledge\nbases, then the fuzzy DLs ALC under Lukasiewicz and Product fuzzy logics do not\nverify the FMP even if we restrict to witnessed models; in other words, finite\nsatisfiability and witnessed satisfiability are different for arbitrary\nknowledge bases. The aim of this paper is to point out the failure of FMP\nbecause it affects several algorithms published in the literature for reasoning\nunder fuzzy ALC.\n"
  },
  {
    "id": "1003.1658",
    "title": "A multivalued knowledge-base model",
    "abstract": "  The basic aim of our study is to give a possible model for handling uncertain\ninformation. This model is worked out in the framework of DATALOG. At first the\nconcept of fuzzy Datalog will be summarized, then its extensions for\nintuitionistic- and interval-valued fuzzy logic is given and the concept of\nbipolar fuzzy Datalog is introduced. Based on these ideas the concept of\nmultivalued knowledge-base will be defined as a quadruple of any background\nknowledge; a deduction mechanism; a connecting algorithm, and a function set of\nthe program, which help us to determine the uncertainty levels of the results.\nAt last a possible evaluation strategy is given.\n"
  },
  {
    "id": "1003.2641",
    "title": "Release ZERO.0.1 of package RefereeToolbox",
    "abstract": "  RefereeToolbox is a java package implementing combination operators for\nfusing evidences. It is downloadable from:\nhttp://refereefunction.fredericdambreville.com/releases RefereeToolbox is based\non an interpretation of the fusion rules by means of Referee Functions. This\napproach implies a dissociation between the definition of the combination and\nits actual implementation, which is common to all referee-based combinations.\nAs a result, RefereeToolbox is designed with the aim to be generic and\nevolutive.\n"
  },
  {
    "id": "1003.5173",
    "title": "LEXSYS: Architecture and Implication for Intelligent Agent systems",
    "abstract": "  LEXSYS, (Legume Expert System) was a project conceived at IITA (International\nInstitute of Tropical Agriculture) Ibadan Nigeria. It was initiated by the\nCOMBS (Collaborative Group on Maize-Based Systems Research in the 1990. It was\nmeant for a general framework for characterizing on-farm testing for technology\ndesign for sustainable cereal-based cropping system. LEXSYS is not a true\nexpert system as the name would imply, but simply a user-friendly information\nsystem. This work is an attempt to give a formal representation of the existing\nsystem and then present areas where intelligent agent can be applied.\n"
  },
  {
    "id": "1003.5305",
    "title": "Rational Value of Information Estimation for Measurement Selection",
    "abstract": "  Computing value of information (VOI) is a crucial task in various aspects of\ndecision-making under uncertainty, such as in meta-reasoning for search; in\nselecting measurements to make, prior to choosing a course of action; and in\nmanaging the exploration vs. exploitation tradeoff. Since such applications\ntypically require numerous VOI computations during a single run, it is\nessential that VOI be computed efficiently. We examine the issue of anytime\nestimation of VOI, as frequently it suffices to get a crude estimate of the\nVOI, thus saving considerable computational resources. As a case study, we\nexamine VOI estimation in the measurement selection problem. Empirical\nevaluation of the proposed scheme in this domain shows that computational\nresources can indeed be significantly reduced, at little cost in expected\nrewards achieved in the overall decision problem.\n"
  },
  {
    "id": "1003.5899",
    "title": "Geometric Algebra Model of Distributed Representations",
    "abstract": "  Formalism based on GA is an alternative to distributed representation models\ndeveloped so far --- Smolensky's tensor product, Holographic Reduced\nRepresentations (HRR) and Binary Spatter Code (BSC). Convolutions are replaced\nby geometric products, interpretable in terms of geometry which seems to be the\nmost natural language for visualization of higher concepts. This paper recalls\nthe main ideas behind the GA model and investigates recognition test results\nusing both inner product and a clipped version of matrix representation. The\ninfluence of accidental blade equality on recognition is also studied. Finally,\nthe efficiency of the GA model is compared to that of previously developed\nmodels.\n"
  },
  {
    "id": "1004.1540",
    "title": "Importance of Sources using the Repeated Fusion Method and the\n  Proportional Conflict Redistribution Rules #5 and #6",
    "abstract": "  We present in this paper some examples of how to compute by hand the PCR5\nfusion rule for three sources, so the reader will better understand its\nmechanism. We also take into consideration the importance of sources, which is\ndifferent from the classical discounting of sources.\n"
  },
  {
    "id": "1004.1772",
    "title": "Terrorism Event Classification Using Fuzzy Inference Systems",
    "abstract": "  Terrorism has led to many problems in Thai societies, not only property\ndamage but also civilian casualties. Predicting terrorism activities in advance\ncan help prepare and manage risk from sabotage by these activities. This paper\nproposes a framework focusing on event classification in terrorism domain using\nfuzzy inference systems (FISs). Each FIS is a decision-making model combining\nfuzzy logic and approximate reasoning. It is generated in five main parts: the\ninput interface, the fuzzification interface, knowledge base unit, decision\nmaking unit and output defuzzification interface. Adaptive neuro-fuzzy\ninference system (ANFIS) is a FIS model adapted by combining the fuzzy logic\nand neural network. The ANFIS utilizes automatic identification of fuzzy logic\nrules and adjustment of membership function (MF). Moreover, neural network can\ndirectly learn from data set to construct fuzzy logic rules and MF implemented\nin various applications. FIS settings are evaluated based on two comparisons.\nThe first evaluation is the comparison between unstructured and structured\nevents using the same FIS setting. The second comparison is the model settings\nbetween FIS and ANFIS for classifying structured events. The data set consists\nof news articles related to terrorism events in three southern provinces of\nThailand. The experimental results show that the classification performance of\nthe FIS resulting from structured events achieves satisfactory accuracy and is\nbetter than the unstructured events. In addition, the classification of\nstructured events using ANFIS gives higher performance than the events using\nonly FIS in the prediction of terrorism events.\n"
  },
  {
    "id": "1004.1794",
    "title": "Probabilistic Semantic Web Mining Using Artificial Neural Analysis",
    "abstract": "  Most of the web user's requirements are search or navigation time and getting\ncorrectly matched result. These constrains can be satisfied with some\nadditional modules attached to the existing search engines and web servers.\nThis paper proposes that powerful architecture for search engines with the\ntitle of Probabilistic Semantic Web Mining named from the methods used. With\nthe increase of larger and larger collection of various data resources on the\nWorld Wide Web (WWW), Web Mining has become one of the most important\nrequirements for the web users. Web servers will store various formats of data\nincluding text, image, audio, video etc., but servers can not identify the\ncontents of the data. These search techniques can be improved by adding some\nspecial techniques including semantic web mining and probabilistic analysis to\nget more accurate results. Semantic web mining technique can provide meaningful\nsearch of data resources by eliminating useless information with mining\nprocess. In this technique web servers will maintain Meta information of each\nand every data resources available in that particular web server. This will\nhelp the search engine to retrieve information that is relevant to user given\ninput string. This paper proposing the idea of combing these two techniques\nSemantic web mining and Probabilistic analysis for efficient and accurate\nsearch results of web mining. SPF can be calculated by considering both\nsemantic accuracy and syntactic accuracy of data with the input string. This\nwill be the deciding factor for producing results.\n"
  },
  {
    "id": "1004.2008",
    "title": "Matrix Coherence and the Nystrom Method",
    "abstract": "  The Nystrom method is an efficient technique to speed up large-scale learning\napplications by generating low-rank approximations. Crucial to the performance\nof this technique is the assumption that a matrix can be well approximated by\nworking exclusively with a subset of its columns. In this work we relate this\nassumption to the concept of matrix coherence and connect matrix coherence to\nthe performance of the Nystrom method. Making use of related work in the\ncompressed sensing and the matrix completion literature, we derive novel\ncoherence-based bounds for the Nystrom method in the low-rank setting. We then\npresent empirical results that corroborate these theoretical bounds. Finally,\nwe present more general empirical results for the full-rank setting that\nconvincingly demonstrate the ability of matrix coherence to measure the degree\nto which information can be extracted from a subset of columns.\n"
  },
  {
    "id": "1004.2624",
    "title": "Symmetry within Solutions",
    "abstract": "  We define the concept of an internal symmetry. This is a symmety within a\nsolution of a constraint satisfaction problem. We compare this to solution\nsymmetry, which is a mapping between different solutions of the same problem.\nWe argue that we may be able to exploit both types of symmetry when finding\nsolutions. We illustrate the potential of exploiting internal symmetries on two\nbenchmark domains: Van der Waerden numbers and graceful graphs. By identifying\ninternal symmetries we are able to extend the state of the art in both cases.\n"
  },
  {
    "id": "1004.2626",
    "title": "Propagating Conjunctions of AllDifferent Constraints",
    "abstract": "  We study propagation algorithms for the conjunction of two AllDifferent\nconstraints. Solutions of an AllDifferent constraint can be seen as perfect\nmatchings on the variable/value bipartite graph. Therefore, we investigate the\nproblem of finding simultaneous bipartite matchings. We present an extension of\nthe famous Hall theorem which characterizes when simultaneous bipartite\nmatchings exists. Unfortunately, finding such matchings is NP-hard in general.\nHowever, we prove a surprising result that finding a simultaneous matching on a\nconvex bipartite graph takes just polynomial time. Based on this theoretical\nresult, we provide the first polynomial time bound consistency algorithm for\nthe conjunction of two AllDifferent constraints. We identify a pathological\nproblem on which this propagator is exponentially faster compared to existing\npropagators. Our experiments show that this new propagator can offer\nsignificant benefits over existing methods.\n"
  },
  {
    "id": "1004.3260",
    "title": "Decision Support Systems (DSS) in Construction Tendering Processes",
    "abstract": "  The successful execution of a construction project is heavily impacted by\nmaking the right decision during tendering processes. Managing tender\nprocedures is very complex and uncertain involving coordination of many tasks\nand individuals with different priorities and objectives. Bias and inconsistent\ndecision are inevitable if the decision-making process is totally depends on\nintuition, subjective judgement or emotion. In making transparent decision and\nhealthy competition tendering, there exists a need for flexible guidance tool\nfor decision support. Aim of this paper is to give a review on current\npractices of Decision Support Systems (DSS) technology in construction\ntendering processes. Current practices of general tendering processes as\napplied to the most countries in different regions such as United States,\nEurope, Middle East and Asia are comprehensively discussed. Applications of\nWeb-based tendering processes is also summarised in terms of its properties.\nBesides that, a summary of Decision Support System (DSS) components is included\nin the next section. Furthermore, prior researches on implementation of DSS\napproaches in tendering processes are discussed in details. Current issues\narise from both of paper-based and Web-based tendering processes are outlined.\nFinally, conclusion is included at the end of this paper.\n"
  },
  {
    "id": "1004.4342",
    "title": "Towards Closed World Reasoning in Dynamic Open Worlds (Extended Version)",
    "abstract": "  The need for integration of ontologies with nonmonotonic rules has been\ngaining importance in a number of areas, such as the Semantic Web. A number of\nresearchers addressed this problem by proposing a unified semantics for hybrid\nknowledge bases composed of both an ontology (expressed in a fragment of\nfirst-order logic) and nonmonotonic rules. These semantics have matured over\nthe years, but only provide solutions for the static case when knowledge does\nnot need to evolve. In this paper we take a first step towards addressing the\ndynamics of hybrid knowledge bases. We focus on knowledge updates and,\nconsidering the state of the art of belief update, ontology update and rule\nupdate, we show that current solutions are only partial and difficult to\ncombine. Then we extend the existing work on ABox updates with rules, provide a\nsemantics for such evolving hybrid knowledge bases and study its basic\nproperties. To the best of our knowledge, this is the first time that an update\noperator is proposed for hybrid knowledge bases.\n"
  },
  {
    "id": "1004.4734",
    "title": "On the comparison of plans: Proposition of an instability measure for\n  dynamic machine scheduling",
    "abstract": "  On the basis of an analysis of previous research, we present a generalized\napproach for measuring the difference of plans with an exemplary application to\nmachine scheduling. Our work is motivated by the need for such measures, which\nare used in dynamic scheduling and planning situations. In this context,\nquantitative approaches are needed for the assessment of the robustness and\nstability of schedules. Obviously, any `robustness' or `stability' of plans has\nto be defined w. r. t. the particular situation and the requirements of the\nhuman decision maker. Besides the proposition of an instability measure, we\ntherefore discuss possibilities of obtaining meaningful information from the\ndecision maker for the implementation of the introduced approach.\n"
  },
  {
    "id": "1004.4801",
    "title": "Ontology-based inference for causal explanation",
    "abstract": "  We define an inference system to capture explanations based on causal\nstatements, using an ontology in the form of an IS-A hierarchy. We first\nintroduce a simple logical language which makes it possible to express that a\nfact causes another fact and that a fact explains another fact. We present a\nset of formal inference patterns from causal statements to explanation\nstatements. We introduce an elementary ontology which gives greater\nexpressiveness to the system while staying close to propositional reasoning. We\nprovide an inference system that captures the patterns discussed, firstly in a\npurely propositional framework, then in a datalog (limited predicate)\nframework.\n"
  },
  {
    "id": "1005.0089",
    "title": "The Exact Closest String Problem as a Constraint Satisfaction Problem",
    "abstract": "  We report (to our knowledge) the first evaluation of Constraint Satisfaction\nas a computational framework for solving closest string problems. We show that\ncareful consideration of symbol occurrences can provide search heuristics that\nprovide several orders of magnitude speedup at and above the optimal distance.\nWe also report (to our knowledge) the first analysis and evaluation -- using\nany technique -- of the computational difficulties involved in the\nidentification of all closest strings for a given input set. We describe\nalgorithms for web-scale distributed solution of closest string problems, both\npurely based on AI backtrack search and also hybrid numeric-AI methods.\n"
  },
  {
    "id": "1005.0104",
    "title": "Joint Structured Models for Extraction from Overlapping Sources",
    "abstract": "  We consider the problem of jointly training structured models for extraction\nfrom sources whose instances enjoy partial overlap. This has important\napplications like user-driven ad-hoc information extraction on the web. Such\napplications present new challenges in terms of the number of sources and their\narbitrary pattern of overlap not seen by earlier collective training schemes\napplied on two sources. We present an agreement-based learning framework and\nalternatives within it to trade-off tractability, robustness to noise, and\nextent of agreement. We provide a principled scheme to discover low-noise\nagreement sets in unlabeled data across the sources. Through extensive\nexperiments over 58 real datasets, we establish that our method of additively\nrewarding agreement over maximal segments of text provides the best trade-offs,\nand also scores over alternatives such as collective inference, staged\ntraining, and multi-view learning.\n"
  },
  {
    "id": "1005.0605",
    "title": "An approach to visualize the course of solving of a research task in\n  humans",
    "abstract": "  A technique to study the dynamics of solving of a research task is suggested.\nThe research task was based on specially developed software Right- Wrong\nResponder (RWR), with the participants having to reveal the response logic of\nthe program. The participants interacted with the program in the form of a\nsemi-binary dialogue, which implies the feedback responses of only two kinds -\n\"right\" or \"wrong\". The technique has been applied to a small pilot group of\nvolunteer participants. Some of them have successfully solved the task\n(solvers) and some have not (non-solvers). In the beginning of the work, the\nsolvers did more wrong moves than non-solvers, and they did less wrong moves\ncloser to the finish of the work. A phase portrait of the work both in solvers\nand non-solvers showed definite cycles that may correspond to sequences of\npartially true hypotheses that may be formulated by the participants during the\nsolving of the task.\n"
  },
  {
    "id": "1005.0608",
    "title": "Informal Concepts in Machines",
    "abstract": "  This paper constructively proves the existence of an effective procedure\ngenerating a computable (total) function that is not contained in any given\neffectively enumerable set of such functions. The proof implies the existence\nof machines that process informal concepts such as computable (total) functions\nbeyond the limits of any given Turing machine or formal system, that is, these\nmachines can, in a certain sense, \"compute\" function values beyond these\nlimits. We call these machines creative. We argue that any \"intelligent\"\nmachine should be capable of processing informal concepts such as computable\n(total) functions, that is, it should be creative. Finally, we introduce\nhypotheses on creative machines which were developed on the basis of\ntheoretical investigations and experiments with computer programs. The\nhypotheses say that machine intelligence is the execution of a self-developing\nprocedure starting from any universal programming language and any input.\n"
  },
  {
    "id": "1005.0896",
    "title": "A two-step fusion process for multi-criteria decision applied to natural\n  hazards in mountains",
    "abstract": "  Mountain river torrents and snow avalanches generate human and material\ndamages with dramatic consequences. Knowledge about natural phenomenona is\noften lacking and expertise is required for decision and risk management\npurposes using multi-disciplinary quantitative or qualitative approaches.\nExpertise is considered as a decision process based on imperfect information\ncoming from more or less reliable and conflicting sources. A methodology mixing\nthe Analytic Hierarchy Process (AHP), a multi-criteria aid-decision method, and\ninformation fusion using Belief Function Theory is described. Fuzzy Sets and\nPossibilities theories allow to transform quantitative and qualitative criteria\ninto a common frame of discernment for decision in Dempster-Shafer Theory (DST\n) and Dezert-Smarandache Theory (DSmT) contexts. Main issues consist in basic\nbelief assignments elicitation, conflict identification and management, fusion\nrule choices, results validation but also in specific needs to make a\ndifference between importance and reliability and uncertainty in the fusion\nprocess.\n"
  },
  {
    "id": "1005.0917",
    "title": "On Building a Knowledge Base for Stability Theory",
    "abstract": "  A lot of mathematical knowledge has been formalized and stored in\nrepositories by now: different mathematical theorems and theories have been\ntaken into consideration and included in mathematical repositories.\nApplications more distant from pure mathematics, however --- though based on\nthese theories --- often need more detailed knowledge about the underlying\ntheories. In this paper we present an example Mizar formalization from the area\nof electrical engineering focusing on stability theory which is based on\ncomplex analysis. We discuss what kind of special knowledge is necessary here\nand which amount of this knowledge is included in existing repositories.\n"
  },
  {
    "id": "1005.1518",
    "title": "Recognizability of Individual Creative Style Within and Across Domains:\n  Preliminary Studies",
    "abstract": "  It is hypothesized that creativity arises from the self-mending capacity of\nan internal model of the world, or worldview. The uniquely honed worldview of a\ncreative individual results in a distinctive style that is recognizable within\nand across domains. It is further hypothesized that creativity is domaingeneral\nin the sense that there exist multiple avenues by which the distinctiveness of\none's worldview can be expressed. These hypotheses were tested using art\nstudents and creative writing students. Art students guessed significantly\nabove chance both which painting was done by which of five famous artists, and\nwhich artwork was done by which of their peers. Similarly, creative writing\nstudents guessed significantly above chance both which passage was written by\nwhich of five famous writers, and which passage was written by which of their\npeers. These findings support the hypothesis that creative style is\nrecognizable. Moreover, creative writing students guessed significantly above\nchance which of their peers produced particular works of art, supporting the\nhypothesis that creative style is recognizable not just within but across\ndomains.\n"
  },
  {
    "id": "1005.1860",
    "title": "Feature Selection Using Regularization in Approximate Linear Programs\n  for Markov Decision Processes",
    "abstract": "  Approximate dynamic programming has been used successfully in a large variety\nof domains, but it relies on a small set of provided approximation features to\ncalculate solutions reliably. Large and rich sets of features can cause\nexisting algorithms to overfit because of a limited number of samples. We\naddress this shortcoming using $L_1$ regularization in approximate linear\nprogramming. Because the proposed method can automatically select the\nappropriate richness of features, its performance does not degrade with an\nincreasing number of features. These results rely on new and stronger sampling\nbounds for regularized approximate linear programs. We also propose a\ncomputationally efficient homotopy method. The empirical evaluation of the\napproach shows that the proposed method performs well on simple MDPs and\nstandard benchmark problems.\n"
  },
  {
    "id": "1005.2815",
    "title": "Evolving Genes to Balance a Pole",
    "abstract": "  We discuss how to use a Genetic Regulatory Network as an evolutionary\nrepresentation to solve a typical GP reinforcement problem, the pole balancing.\nThe network is a modified version of an Artificial Regulatory Network proposed\na few years ago, and the task could be solved only by finding a proper way of\nconnecting inputs and outputs to the network. We show that the representation\nis able to generalize well over the problem domain, and discuss the performance\nof different models of this kind.\n"
  },
  {
    "id": "1005.3502",
    "title": "Using machine learning to make constraint solver implementation\n  decisions",
    "abstract": "  Programs to solve so-called constraint problems are complex pieces of\nsoftware which require many design decisions to be made more or less\narbitrarily by the implementer. These decisions affect the performance of the\nfinished solver significantly. Once a design decision has been made, it cannot\neasily be reversed, although a different decision may be more appropriate for a\nparticular problem.\n  We investigate using machine learning to make these decisions automatically\ndepending on the problem to solve with the alldifferent constraint as an\nexample. Our system is capable of making non-trivial, multi-level decisions\nthat improve over always making a default choice.\n"
  },
  {
    "id": "1005.4025",
    "title": "A Soft Computing Model for Physicians' Decision Process",
    "abstract": "  In this paper the author presents a kind of Soft Computing Technique, mainly\nan application of fuzzy set theory of Prof. Zadeh [16], on a problem of Medical\nExperts Systems. The choosen problem is on design of a physician's decision\nmodel which can take crisp as well as fuzzy data as input, unlike the\ntraditional models. The author presents a mathematical model based on fuzzy set\ntheory for physician aided evaluation of a complete representation of\ninformation emanating from the initial interview including patient past\nhistory, present symptoms, and signs observed upon physical examination and\nresults of clinical and diagnostic tests.\n"
  },
  {
    "id": "1005.4159",
    "title": "The Complexity of Manipulating $k$-Approval Elections",
    "abstract": "  An important problem in computational social choice theory is the complexity\nof undesirable behavior among agents, such as control, manipulation, and\nbribery in election systems. These kinds of voting strategies are often\ntempting at the individual level but disastrous for the agents as a whole.\nCreating election systems where the determination of such strategies is\ndifficult is thus an important goal.\n  An interesting set of elections is that of scoring protocols. Previous work\nin this area has demonstrated the complexity of misuse in cases involving a\nfixed number of candidates, and of specific election systems on unbounded\nnumber of candidates such as Borda. In contrast, we take the first step in\ngeneralizing the results of computational complexity of election misuse to\ncases of infinitely many scoring protocols on an unbounded number of\ncandidates. Interesting families of systems include $k$-approval and $k$-veto\nelections, in which voters distinguish $k$ candidates from the candidate set.\n  Our main result is to partition the problems of these families based on their\ncomplexity. We do so by showing they are polynomial-time computable, NP-hard,\nor polynomial-time equivalent to another problem of interest. We also\ndemonstrate a surprising connection between manipulation in election systems\nand some graph theory problems.\n"
  },
  {
    "id": "1005.4272",
    "title": "Inaccuracy Minimization by Partioning Fuzzy Data Sets - Validation of\n  Analystical Methodology",
    "abstract": "  In the last two decades, a number of methods have been proposed for\nforecasting based on fuzzy time series. Most of the fuzzy time series methods\nare presented for forecasting of car road accidents. However, the forecasting\naccuracy rates of the existing methods are not good enough. In this paper, we\ncompared our proposed new method of fuzzy time series forecasting with existing\nmethods. Our method is based on means based partitioning of the historical data\nof car road accidents. The proposed method belongs to the kth order and\ntime-variant methods. The proposed method can get the best forecasting accuracy\nrate for forecasting the car road accidents than the existing methods.\n"
  },
  {
    "id": "1005.4496",
    "title": "Combining Naive Bayes and Decision Tree for Adaptive Intrusion Detection",
    "abstract": "  In this paper, a new learning algorithm for adaptive network intrusion\ndetection using naive Bayesian classifier and decision tree is presented, which\nperforms balance detections and keeps false positives at acceptable level for\ndifferent types of network attacks, and eliminates redundant attributes as well\nas contradictory examples from training data that make the detection model\ncomplex. The proposed algorithm also addresses some difficulties of data mining\nsuch as handling continuous attribute, dealing with missing attribute values,\nand reducing noise in training data. Due to the large volumes of security audit\ndata as well as the complex and dynamic properties of intrusion behaviours,\nseveral data miningbased intrusion detection techniques have been applied to\nnetwork-based traffic data and host-based data in the last decades. However,\nthere remain various issues needed to be examined towards current intrusion\ndetection systems (IDS). We tested the performance of our proposed algorithm\nwith existing learning algorithms by employing on the KDD99 benchmark intrusion\ndetection dataset. The experimental results prove that the proposed algorithm\nachieved high detection rates (DR) and significant reduce false positives (FP)\nfor different types of network intrusions using limited computational\nresources.\n"
  },
  {
    "id": "1005.4592",
    "title": "Automated Reasoning and Presentation Support for Formalizing Mathematics\n  in Mizar",
    "abstract": "  This paper presents a combination of several automated reasoning and proof\npresentation tools with the Mizar system for formalization of mathematics. The\ncombination forms an online service called MizAR, similar to the SystemOnTPTP\nservice for first-order automated reasoning. The main differences to\nSystemOnTPTP are the use of the Mizar language that is oriented towards human\nmathematicians (rather than the pure first-order logic used in SystemOnTPTP),\nand setting the service in the context of the large Mizar Mathematical Library\nof previous theorems,definitions, and proofs (rather than the isolated problems\nthat are solved in SystemOnTPTP). These differences poses new challenges and\nnew opportunities for automated reasoning and for proof presentation tools.\nThis paper describes the overall structure of MizAR, and presents the automated\nreasoning systems and proof presentation tools that are combined to make MizAR\na useful mathematical service.\n"
  },
  {
    "id": "1005.4963",
    "title": "Integrating Structured Metadata with Relational Affinity Propagation",
    "abstract": "  Structured and semi-structured data describing entities, taxonomies and\nontologies appears in many domains. There is a huge interest in integrating\nstructured information from multiple sources; however integrating structured\ndata to infer complex common structures is a difficult task because the\nintegration must aggregate similar structures while avoiding structural\ninconsistencies that may appear when the data is combined. In this work, we\nstudy the integration of structured social metadata: shallow personal\nhierarchies specified by many individual users on the SocialWeb, and focus on\ninferring a collection of integrated, consistent taxonomies. We frame this task\nas an optimization problem with structural constraints. We propose a new\ninference algorithm, which we refer to as Relational Affinity Propagation (RAP)\nthat extends affinity propagation (Frey and Dueck 2007) by introducing\nstructural constraints. We validate the approach on a real-world social media\ndataset, collected from the photosharing website Flickr. Our empirical results\nshow that our proposed approach is able to construct deeper and denser\nstructures compared to an approach using only the standard affinity propagation\nalgorithm.\n"
  },
  {
    "id": "1005.4989",
    "title": "A Formalization of the Turing Test",
    "abstract": "  The paper offers a mathematical formalization of the Turing test. This\nformalization makes it possible to establish the conditions under which some\nTuring machine will pass the Turing test and the conditions under which every\nTuring machine (or every Turing machine of the special class) will fail the\nTuring test.\n"
  },
  {
    "id": "1005.5114",
    "title": "Growing a Tree in the Forest: Constructing Folksonomies by Integrating\n  Structured Metadata",
    "abstract": "  Many social Web sites allow users to annotate the content with descriptive\nmetadata, such as tags, and more recently to organize content hierarchically.\nThese types of structured metadata provide valuable evidence for learning how a\ncommunity organizes knowledge. For instance, we can aggregate many personal\nhierarchies into a common taxonomy, also known as a folksonomy, that will aid\nusers in visualizing and browsing social content, and also to help them in\norganizing their own content. However, learning from social metadata presents\nseveral challenges, since it is sparse, shallow, ambiguous, noisy, and\ninconsistent. We describe an approach to folksonomy learning based on\nrelational clustering, which exploits structured metadata contained in personal\nhierarchies. Our approach clusters similar hierarchies using their structure\nand tag statistics, then incrementally weaves them into a deeper, bushier tree.\nWe study folksonomy learning using social metadata extracted from the\nphoto-sharing site Flickr, and demonstrate that the proposed approach addresses\nthe challenges. Moreover, comparing to previous work, the approach produces\nlarger, more accurate folksonomies, and in addition, scales better.\n"
  },
  {
    "id": "1005.5270",
    "title": "Symmetries of Symmetry Breaking Constraints",
    "abstract": "  Symmetry is an important feature of many constraint programs. We show that\nany problem symmetry acting on a set of symmetry breaking constraints can be\nused to break symmetry. Different symmetries pick out different solutions in\neach symmetry class. This simple but powerful idea can be used in a number of\ndifferent ways. We describe one application within model restarts, a search\ntechnique designed to reduce the conflict between symmetry breaking and the\nbranching heuristic. In model restarts, we restart search periodically with a\nrandom symmetry of the symmetry breaking constraints. Experimental results show\nthat this symmetry breaking technique is effective in practice on some standard\nbenchmark problems.\n"
  },
  {
    "id": "1006.0274",
    "title": "Learning Probabilistic Hierarchical Task Networks to Capture User\n  Preferences",
    "abstract": "  We propose automatically learning probabilistic Hierarchical Task Networks\n(pHTNs) in order to capture a user's preferences on plans, by observing only\nthe user's behavior. HTNs are a common choice of representation for a variety\nof purposes in planning, including work on learning in planning. Our\ncontributions are (a) learning structure and (b) representing preferences. In\ncontrast, prior work employing HTNs considers learning method preconditions\n(instead of structure) and representing domain physics or search control\nknowledge (rather than preferences). Initially we will assume that the observed\ndistribution of plans is an accurate representation of user preference, and\nthen generalize to the situation where feasibility constraints frequently\nprevent the execution of preferred plans. In order to learn a distribution on\nplans we adapt an Expectation-Maximization (EM) technique from the discipline\nof (probabilistic) grammar induction, taking the perspective of task reductions\nas productions in a context-free grammar over primitive actions. To account for\nthe difference between the distributions of possible and preferred plans we\nsubsequently modify this core EM technique, in short, by rescaling its input.\n"
  },
  {
    "id": "1006.0385",
    "title": "Brain-Like Stochastic Search: A Research Challenge and Funding\n  Opportunity",
    "abstract": "  Brain-Like Stochastic Search (BLiSS) refers to this task: given a family of\nutility functions U(u,A), where u is a vector of parameters or task\ndescriptors, maximize or minimize U with respect to u, using networks (Option\nNets) which input A and learn to generate good options u stochastically. This\npaper discusses why this is crucial to brain-like intelligence (an area funded\nby NSF) and to many applications, and discusses various possibilities for\nnetwork design and training. The appendix discusses recent research, relations\nto work on stochastic optimization in operations research, and relations to\nengineering-based approaches to understanding neocortex.\n"
  },
  {
    "id": "1006.0991",
    "title": "Variational Program Inference",
    "abstract": "  We introduce a framework for representing a variety of interesting problems\nas inference over the execution of probabilistic model programs. We represent a\n\"solution\" to such a problem as a guide program which runs alongside the model\nprogram and influences the model program's random choices, leading the model\nprogram to sample from a different distribution than from its priors. Ideally\nthe guide program influences the model program to sample from the posteriors\ngiven the evidence. We show how the KL- divergence between the true posterior\ndistribution and the distribution induced by the guided model program can be\nefficiently estimated (up to an additive constant) by sampling multiple\nexecutions of the guided model program. In addition, we show how to use the\nguide program as a proposal distribution in importance sampling to\nstatistically prove lower bounds on the probability of the evidence and on the\nprobability of a hypothesis and the evidence. We can use the quotient of these\ntwo bounds as an estimate of the conditional probability of the hypothesis\ngiven the evidence. We thus turn the inference problem into a heuristic search\nfor better guide programs.\n"
  },
  {
    "id": "1006.1080",
    "title": "The Dilated Triple",
    "abstract": "  The basic unit of meaning on the Semantic Web is the RDF statement, or\ntriple, which combines a distinct subject, predicate and object to make a\ndefinite assertion about the world. A set of triples constitutes a graph, to\nwhich they give a collective meaning. It is upon this simple foundation that\nthe rich, complex knowledge structures of the Semantic Web are built. Yet the\nvery expressiveness of RDF, by inviting comparison with real-world knowledge,\nhighlights a fundamental shortcoming, in that RDF is limited to statements of\nabsolute fact, independent of the context in which a statement is asserted.\nThis is in stark contrast with the thoroughly context-sensitive nature of human\nthought. The model presented here provides a particularly simple means of\ncontextualizing an RDF triple by associating it with related statements in the\nsame graph. This approach, in combination with a notion of graph similarity, is\nsufficient to select only those statements from an RDF graph which are\nsubjectively most relevant to the context of the requesting process.\n"
  },
  {
    "id": "1006.1190",
    "title": "Game Information System",
    "abstract": "  In this Information system age many organizations consider information system\nas their weapon to compete or gain competitive advantage or give the best\nservices for non profit organizations. Game Information System as combining\nInformation System and game is breakthrough to achieve organizations'\nperformance. The Game Information System will run the Information System with\ngame and how game can be implemented to run the Information System. Game is not\nonly for fun and entertainment, but will be a challenge to combine fun and\nentertainment with Information System. The Challenge to run the information\nsystem with entertainment, deliver the entertainment with information system\nall at once. Game information system can be implemented in many sectors as like\nthe information system itself but in difference's view. A view of game which\npeople can joy and happy and do their transaction as a fun things.\n"
  },
  {
    "id": "1006.1701",
    "title": "Virtual information system on working area",
    "abstract": "  In order to get strategic positioning for competition in business\norganization, the information system must be ahead in this information age\nwhere the information as one of the weapons to win the competition and in the\nright hand the information will become a right bullet. The information system\nwith the information technology support isn't enough if just only on internet\nor implemented with internet technology. The growth of information technology\nas tools for helping and making people easy to use must be accompanied by\nwanting to make fun and happy when they make contact with the information\ntechnology itself. Basically human like to play, since childhood human have\nbeen playing, free and happy and when human grow up they can't play as much as\nwhen human was in their childhood. We have to develop the information system\nwhich is not perform information system itself but can help human to explore\ntheir natural instinct for playing, making fun and happiness when they interact\nwith the information system. Virtual information system is the way to present\nplaying and having fun atmosphere on working area.\n"
  },
  {
    "id": "1006.1703",
    "title": "Indonesian Earthquake Decision Support System",
    "abstract": "  Earthquake DSS is an information technology environment which can be used by\ngovernment to sharpen, make faster and better the earthquake mitigation\ndecision. Earthquake DSS can be delivered as E-government which is not only for\ngovernment itself but in order to guarantee each citizen's rights for\neducation, training and information about earthquake and how to overcome the\nearthquake. Knowledge can be managed for future use and would become mining by\nsaving and maintain all the data and information about earthquake and\nearthquake mitigation in Indonesia. Using Web technology will enhance global\naccess and easy to use. Datawarehouse as unNormalized database for\nmultidimensional analysis will speed the query process and increase reports\nvariation. Link with other Disaster DSS in one national disaster DSS, link with\nother government information system and international will enhance the\nknowledge and sharpen the reports.\n"
  },
  {
    "id": "1006.2204",
    "title": "MDPs with Unawareness",
    "abstract": "  Markov decision processes (MDPs) are widely used for modeling decision-making\nproblems in robotics, automated control, and economics. Traditional MDPs assume\nthat the decision maker (DM) knows all states and actions. However, this may\nnot be true in many situations of interest. We define a new framework, MDPs\nwith unawareness (MDPUs) to deal with the possibilities that a DM may not be\naware of all possible actions. We provide a complete characterization of when a\nDM can learn to play near-optimally in an MDPU, and give an algorithm that\nlearns to play near-optimally when it is possible to do so, as efficiently as\npossible. In particular, we characterize when a near-optimal solution can be\nfound in polynomial time.\n"
  },
  {
    "id": "1006.2743",
    "title": "Global Optimization for Value Function Approximation",
    "abstract": "  Existing value function approximation methods have been successfully used in\nmany applications, but they often lack useful a priori error bounds. We propose\na new approximate bilinear programming formulation of value function\napproximation, which employs global optimization. The formulation provides\nstrong a priori guarantees on both robust and expected policy loss by\nminimizing specific norms of the Bellman residual. Solving a bilinear program\noptimally is NP-hard, but this is unavoidable because the Bellman-residual\nminimization itself is NP-hard. We describe and analyze both optimal and\napproximate algorithms for solving bilinear programs. The analysis shows that\nthis algorithm offers a convergent generalization of approximate policy\niteration. We also briefly analyze the behavior of bilinear programming\nalgorithms under incomplete samples. Finally, we demonstrate that the proposed\napproach can consistently minimize the Bellman residual on simple benchmark\nproblems.\n"
  },
  {
    "id": "1006.3021",
    "title": "A General Framework for Equivalences in Answer-Set Programming by\n  Countermodels in the Logic of Here-and-There",
    "abstract": "  Different notions of equivalence, such as the prominent notions of strong and\nuniform equivalence, have been studied in Answer-Set Programming, mainly for\nthe purpose of identifying programs that can serve as substitutes without\naltering the semantics, for instance in program optimization. Such semantic\ncomparisons are usually characterized by various selections of models in the\nlogic of Here-and-There (HT). For uniform equivalence however, correct\ncharacterizations in terms of HT-models can only be obtained for finite\ntheories, respectively programs. In this article, we show that a selection of\ncountermodels in HT captures uniform equivalence also for infinite theories.\nThis result is turned into coherent characterizations of the different notions\nof equivalence by countermodels, as well as by a mixture of HT-models and\ncountermodels (so-called equivalence interpretations). Moreover, we generalize\nthe so-called notion of relativized hyperequivalence for programs to\npropositional theories, and apply the same methodology in order to obtain a\nsemantic characterization which is amenable to infinite settings. This allows\nfor a lifting of the results to first-order theories under a very general\nsemantics given in terms of a quantified version of HT. We thus obtain a\ngeneral framework for the study of various notions of equivalence for theories\nunder answer-set semantics. Moreover, we prove an expedient property that\nallows for a simplified treatment of extended signatures, and provide further\nresults for non-ground logic programs. In particular, uniform equivalence\ncoincides under open and ordinary answer-set semantics, and for finite\nnon-ground programs under these semantics, also the usual characterization of\nuniform equivalence in terms of maximal and total HT-models of the grounding is\ncorrect, even for infinite domains, when corresponding ground programs are\ninfinite.\n"
  },
  {
    "id": "1006.4544",
    "title": "Human Disease Diagnosis Using a Fuzzy Expert System",
    "abstract": "  Human disease diagnosis is a complicated process and requires high level of\nexpertise. Any attempt of developing a web-based expert system dealing with\nhuman disease diagnosis has to overcome various difficulties. This paper\ndescribes a project work aiming to develop a web-based fuzzy expert system for\ndiagnosing human diseases. Now a days fuzzy systems are being used successfully\nin an increasing number of application areas; they use linguistic rules to\ndescribe systems. This research project focuses on the research and development\nof a web-based clinical tool designed to improve the quality of the exchange of\nhealth information between health care professionals and patients.\nPractitioners can also use this web-based tool to corroborate diagnosis. The\nproposed system is experimented on various scenarios in order to evaluate it's\nperformance. In all the cases, proposed system exhibits satisfactory results.\n"
  },
  {
    "id": "1006.4551",
    "title": "Vagueness of Linguistic variable",
    "abstract": "  In the area of computer science focusing on creating machines that can engage\non behaviors that humans consider intelligent. The ability to create\nintelligent machines has intrigued humans since ancient times and today with\nthe advent of the computer and 50 years of research into various programming\ntechniques, the dream of smart machines is becoming a reality. Researchers are\ncreating systems which can mimic human thought, understand speech, beat the\nbest human chessplayer, and countless other feats never before possible.\nAbility of the human to estimate the information is most brightly shown in\nusing of natural languages. Using words of a natural language for valuation\nqualitative attributes, for example, the person pawns uncertainty in form of\nvagueness in itself estimations. Vague sets, vague judgments, vague conclusions\ntakes place there and then, where and when the reasonable subject exists and\nalso is interested in something. The vague sets theory has arisen as the answer\nto an illegibility of language the reasonable subject speaks. Language of a\nreasonable subject is generated by vague events which are created by the reason\nand which are operated by the mind. The theory of vague sets represents an\nattempt to find such approximation of vague grouping which would be more\nconvenient, than the classical theory of sets in situations where the natural\nlanguage plays a significant role. Such theory has been offered by known\nAmerican mathematician Gau and Buehrer .In our paper we are describing how\nvagueness of linguistic variables can be solved by using the vague set\ntheory.This paper is mainly designed for one of directions of the eventology\n(the theory of the random vague events), which has arisen within the limits of\nthe probability theory and which pursue the unique purpose to describe\neventologically a movement of reason.\n"
  },
  {
    "id": "1006.4561",
    "title": "An Efficient Technique for Similarity Identification between Ontologies",
    "abstract": "  Ontologies usually suffer from the semantic heterogeneity when simultaneously\nused in information sharing, merging, integrating and querying processes.\nTherefore, the similarity identification between ontologies being used becomes\na mandatory task for all these processes to handle the problem of semantic\nheterogeneity. In this paper, we propose an efficient technique for similarity\nmeasurement between two ontologies. The proposed technique identifies all\ncandidate pairs of similar concepts without omitting any similar pair. The\nproposed technique can be used in different types of operations on ontologies\nsuch as merging, mapping and aligning. By analyzing its results a reasonable\nimprovement in terms of completeness, correctness and overall quality of the\nresults has been found.\n"
  },
  {
    "id": "1006.4563",
    "title": "The State of the Art: Ontology Web-Based Languages: XML Based",
    "abstract": "  Many formal languages have been proposed to express or represent Ontologies,\nincluding RDF, RDFS, DAML+OIL and OWL. Most of these languages are based on XML\nsyntax, but with various terminologies and expressiveness. Therefore, choosing\na language for building an Ontology is the main step. The main point of\nchoosing language to represent Ontology is based mainly on what the Ontology\nwill represent or be used for. That language should have a range of quality\nsupport features such as ease of use, expressive power, compatibility, sharing\nand versioning, internationalisation. This is because different kinds of\nknowledge-based applications need different language features. The main\nobjective of these languages is to add semantics to the existing information on\nthe web. The aims of this paper is to provide a good knowledge of existing\nlanguage and understanding of these languages and how could be used.\n"
  },
  {
    "id": "1006.4567",
    "title": "Understanding Semantic Web and Ontologies: Theory and Applications",
    "abstract": "  Semantic Web is actually an extension of the current one in that it\nrepresents information more meaningfully for humans and computers alike. It\nenables the description of contents and services in machine-readable form, and\nenables annotating, discovering, publishing, advertising and composing services\nto be automated. It was developed based on Ontology, which is considered as the\nbackbone of the Semantic Web. In other words, the current Web is transformed\nfrom being machine-readable to machine-understandable. In fact, Ontology is a\nkey technique with which to annotate semantics and provide a common,\ncomprehensible foundation for resources on the Semantic Web. Moreover, Ontology\ncan provide a common vocabulary, a grammar for publishing data, and can supply\na semantic description of data which can be used to preserve the Ontologies and\nkeep them ready for inference. This paper provides basic concepts of web\nservices and the Semantic Web, defines the structure and the main applications\nof ontology, and provides many relevant terms are explained in order to provide\na basic understanding of ontologies.\n"
  },
  {
    "id": "1006.5041",
    "title": "GroupLiNGAM: Linear non-Gaussian acyclic models for sets of variables",
    "abstract": "  Finding the structure of a graphical model has been received much attention\nin many fields. Recently, it is reported that the non-Gaussianity of data\nenables us to identify the structure of a directed acyclic graph without any\nprior knowledge on the structure. In this paper, we propose a novel\nnon-Gaussianity based algorithm for more general type of models; chain graphs.\nThe algorithm finds an ordering of the disjoint subsets of variables by\niteratively evaluating the independence between the variable subset and the\nresiduals when the remaining variables are regressed on those. However, its\ncomputational cost grows exponentially according to the number of variables.\nTherefore, we further discuss an efficient approximate approach for applying\nthe algorithm to large sized graphs. We illustrate the algorithm with\nartificial and real-world datasets.\n"
  },
  {
    "id": "1006.5511",
    "title": "Soft Approximations and uni-int Decision Making",
    "abstract": "  Notions of core, support and inversion of a soft set have been defined and\nstudied. Soft approximations are soft sets developed through core and support,\nand are used for granulating the soft space. Membership structure of a soft set\nhas been probed in and many interesting properties presented. The mathematical\napparatus developed so far in this paper yields a detailed analysis of two\nworks viz. [N. Cagman, S. Enginoglu, Soft set theory and uni-int decision\nmaking, European Jr. of Operational Research (article in press, available\nonline 12 May 2010)] and [N. Cagman, S. Enginoglu, Soft matrix theory and its\ndecision making, Computers and Mathematics with Applications 59 (2010) 3308 -\n3314.]. We prove (Theorem 8.1) that uni-int method of Cagman is equivalent to a\ncore-support expression which is computationally far less expansive than\nuni-int. This also highlights some shortcomings in Cagman's uni-int method and\nthus motivates us to improve the method. We first suggest an improvement in\nuni-int method and then present a new conjecture to solve the optimum choice\nproblem given by Cagman and Enginoglu. Our Example 8.6 presents a case where\nthe optimum choice is intuitively clear yet both uni-int methods (Cagman's and\nour improved one) give wrong answer but the new conjecture solves the problem\ncorrectly.\n"
  },
  {
    "id": "1006.5657",
    "title": "Reasoning Support for Risk Prediction and Prevention in Independent\n  Living",
    "abstract": "  In recent years there has been growing interest in solutions for the delivery\nof clinical care for the elderly, due to the large increase in aging\npopulation. Monitoring a patient in his home environment is necessary to ensure\ncontinuity of care in home settings, but, to be useful, this activity must not\nbe too invasive for patients and a burden for caregivers. We prototyped a\nsystem called SINDI (Secure and INDependent lIving), focused on i) collecting a\nlimited amount of data about the person and the environment through Wireless\nSensor Networks (WSN), and ii) inferring from these data enough information to\nsupport caregivers in understanding patients' well being and in predicting\npossible evolutions of their health. Our hierarchical logic-based model of\nhealth combines data from different sources, sensor data, tests results,\ncommon-sense knowledge and patient's clinical profile at the lower level, and\ncorrelation rules between health conditions across upper levels. The logical\nformalization and the reasoning process are based on Answer Set Programming.\nThe expressive power of this logic programming paradigm makes it possible to\nreason about health evolution even when the available information is incomplete\nand potentially incoherent, while declarativity simplifies rules specification\nby caregivers and allows automatic encoding of knowledge. This paper describes\nhow these issues have been targeted in the application scenario of the SINDI\nsystem.\n"
  },
  {
    "id": "1007.0412",
    "title": "Improving Iris Recognition Accuracy By Score Based Fusion Method",
    "abstract": "  Iris recognition technology, used to identify individuals by photographing\nthe iris of their eye, has become popular in security applications because of\nits ease of use, accuracy, and safety in controlling access to high-security\nareas. Fusion of multiple algorithms for biometric verification performance\nimprovement has received considerable attention. The proposed method combines\nthe zero-crossing 1 D wavelet Euler number, and genetic algorithm based for\nfeature extraction. The output from these three algorithms is normalized and\ntheir score are fused to decide whether the user is genuine or imposter. This\nnew strategies is discussed in this paper, in order to compute a multimodal\ncombined score.\n"
  },
  {
    "id": "1007.0603",
    "title": "Decomposition of the NVALUE constraint",
    "abstract": "  We study decompositions of the global NVALUE constraint. Our main\ncontribution is theoretical: we show that there are propagators for global\nconstraints like NVALUE which decomposition can simulate with the same time\ncomplexity but with a much greater space complexity. This suggests that the\nbenefit of a global propagator may often not be in saving time but in saving\nspace. Our other theoretical contribution is to show for the first time that\nrange consistency can be enforced on NVALUE with the same worst-case time\ncomplexity as bound consistency. Finally, the decompositions we study are\nreadily encoded as linear inequalities. We are therefore able to use them in\ninteger linear programs.\n"
  },
  {
    "id": "1007.0604",
    "title": "Symmetry within and between solutions",
    "abstract": "  Symmetry can be used to help solve many problems. For instance, Einstein's\nfamous 1905 paper (\"On the Electrodynamics of Moving Bodies\") uses symmetry to\nhelp derive the laws of special relativity. In artificial intelligence,\nsymmetry has played an important role in both problem representation and\nreasoning. I describe recent work on using symmetry to help solve constraint\nsatisfaction problems. Symmetries occur within individual solutions of problems\nas well as between different solutions of the same problem. Symmetry can also\nbe applied to the constraints in a problem to give new symmetric constraints.\nReasoning about symmetry can speed up problem solving, and has led to the\ndiscovery of new results in both graph and number theory.\n"
  },
  {
    "id": "1007.0614",
    "title": "Online Cake Cutting",
    "abstract": "  We propose an online form of the cake cutting problem. This models situations\nwhere players arrive and depart during the process of dividing a resource. We\nshow that well known fair division procedures like cut-and-choose and the\nDubins-Spanier moving knife procedure can be adapted to apply to such online\nproblems. We propose some desirable properties that online cake cutting\nprocedures might possess like online forms of proportionality and\nenvy-freeness, and identify which properties are in fact possessed by the\ndifferent online cake procedures.\n"
  },
  {
    "id": "1007.0637",
    "title": "Local search for stable marriage problems with ties and incomplete lists",
    "abstract": "  The stable marriage problem has a wide variety of practical applications,\nranging from matching resident doctors to hospitals, to matching students to\nschools, or more generally to any two-sided market. We consider a useful\nvariation of the stable marriage problem, where the men and women express their\npreferences using a preference list with ties over a subset of the members of\nthe other sex. Matchings are permitted only with people who appear in these\npreference lists. In this setting, we study the problem of finding a stable\nmatching that marries as many people as possible. Stability is an envy-free\nnotion: no man and woman who are not married to each other would both prefer\neach other to their partners or to being single. This problem is NP-hard. We\ntackle this problem using local search, exploiting properties of the problem to\nreduce the size of the neighborhood and to make local moves efficiently.\nExperimental results show that this approach is able to solve large problems,\nquickly returning stable matchings of large and often optimal size.\n"
  },
  {
    "id": "1007.0690",
    "title": "A unified view of Automata-based algorithms for Frequent Episode\n  Discovery",
    "abstract": "  Frequent Episode Discovery framework is a popular framework in Temporal Data\nMining with many applications. Over the years many different notions of\nfrequencies of episodes have been proposed along with different algorithms for\nepisode discovery. In this paper we present a unified view of all such\nfrequency counting algorithms. We present a generic algorithm such that all\ncurrent algorithms are special cases of it. This unified view allows one to\ngain insights into different frequencies and we present quantitative\nrelationships among different frequencies. Our unified view also helps in\nobtaining correctness proofs for various algorithms as we show here. We also\npoint out how this unified view helps us to consider generalization of the\nalgorithm so that they can discover episodes with general partial orders.\n"
  },
  {
    "id": "1007.0859",
    "title": "Local search for stable marriage problems",
    "abstract": "  The stable marriage (SM) problem has a wide variety of practical\napplications, ranging from matching resident doctors to hospitals, to matching\nstudents to schools, or more generally to any two-sided market. In the\nclassical formulation, n men and n women express their preferences (via a\nstrict total order) over the members of the other sex. Solving a SM problem\nmeans finding a stable marriage where stability is an envy-free notion: no man\nand woman who are not married to each other would both prefer each other to\ntheir partners or to being single. We consider both the classical stable\nmarriage problem and one of its useful variations (denoted SMTI) where the men\nand women express their preferences in the form of an incomplete preference\nlist with ties over a subset of the members of the other sex. Matchings are\npermitted only with people who appear in these lists, an we try to find a\nstable matching that marries as many people as possible. Whilst the SM problem\nis polynomial to solve, the SMTI problem is NP-hard. We propose to tackle both\nproblems via a local search approach, which exploits properties of the problems\nto reduce the size of the neighborhood and to make local moves efficiently. We\nevaluate empirically our algorithm for SM problems by measuring its runtime\nbehaviour and its ability to sample the lattice of all possible stable\nmarriages. We evaluate our algorithm for SMTI problems in terms of both its\nruntime behaviour and its ability to find a maximum cardinality stable\nmarriage.For SM problems, the number of steps of our algorithm grows only as\nO(nlog(n)), and that it samples very well the set of all stable marriages. It\nis thus a fair and efficient approach to generate stable marriages.Furthermore,\nour approach for SMTI problems is able to solve large problems, quickly\nreturning stable matchings of large and often optimal size despite the\nNP-hardness of this problem.\n"
  },
  {
    "id": "1007.1766",
    "title": "An svm multiclassifier approach to land cover mapping",
    "abstract": "  From the advent of the application of satellite imagery to land cover\nmapping, one of the growing areas of research interest has been in the area of\nimage classification. Image classifiers are algorithms used to extract land\ncover information from satellite imagery. Most of the initial research has\nfocussed on the development and application of algorithms to better existing\nand emerging classifiers. In this paper, a paradigm shift is proposed whereby a\ncommittee of classifiers is used to determine the final classification output.\nTwo of the key components of an ensemble system are that there should be\ndiversity among the classifiers and that there should be a mechanism through\nwhich the results are combined. In this paper, the members of the ensemble\nsystem include: Linear SVM, Gaussian SVM and Quadratic SVM. The final output\nwas determined through a simple majority vote of the individual classifiers.\nFrom the results obtained it was observed that the final derived map generated\nby an ensemble system can potentially improve on the results derived from the\nindividual classifiers making up the ensemble system. The ensemble system\nclassification accuracy was, in this case, better than the linear and quadratic\nSVM result. It was however less than that of the RBF SVM. Areas for further\nresearch could focus on improving the diversity of the ensemble system used in\nthis research.\n"
  },
  {
    "id": "1007.2534",
    "title": "A general method for deciding about logically constrained issues",
    "abstract": "  A general method is given for revising degrees of belief and arriving at\nconsistent decisions about a system of logically constrained issues. In\ncontrast to other works about belief revision, here the constraints are assumed\nto be fixed. The method has two variants, dual of each other, whose revised\ndegrees of belief are respectively above and below the original ones. The upper\n[resp. lower] revised degrees of belief are uniquely characterized as the\nlowest [resp. highest] ones that are invariant by a certain max-min [resp.\nmin-max] operation determined by the logical constraints. In both variants,\nmaking balance between the revised degree of belief of a proposition and that\nof its negation leads to decisions that are ensured to be consistent with the\nlogical constraints. These decisions are ensured to agree with the majority\ncriterion as applied to the original degrees of belief whenever this gives a\nconsistent result. They are also also ensured to satisfy a property of respect\nfor unanimity about any particular issue, as well as a property of monotonicity\nwith respect to the original degrees of belief. The application of the method\nto certain special domains comes down to well established or increasingly\naccepted methods, such as the single-link method of cluster analysis and the\nmethod of paths in preferential voting.\n"
  },
  {
    "id": "1007.3159",
    "title": "Logic-Based Decision Support for Strategic Environmental Assessment",
    "abstract": "  Strategic Environmental Assessment is a procedure aimed at introducing\nsystematic assessment of the environmental effects of plans and programs. This\nprocedure is based on the so-called coaxial matrices that define dependencies\nbetween plan activities (infrastructures, plants, resource extractions,\nbuildings, etc.) and positive and negative environmental impacts, and\ndependencies between these impacts and environmental receptors. Up to now, this\nprocedure is manually implemented by environmental experts for checking the\nenvironmental effects of a given plan or program, but it is never applied\nduring the plan/program construction. A decision support system, based on a\nclear logic semantics, would be an invaluable tool not only in assessing a\nsingle, already defined plan, but also during the planning process in order to\nproduce an optimized, environmentally assessed plan and to study possible\nalternative scenarios. We propose two logic-based approaches to the problem,\none based on Constraint Logic Programming and one on Probabilistic Logic\nProgramming that could be, in the future, conveniently merged to exploit the\nadvantages of both. We test the proposed approaches on a real energy plan and\nwe discuss their limitations and advantages.\n"
  },
  {
    "id": "1007.3515",
    "title": "Query-driven Procedures for Hybrid MKNF Knowledge Bases",
    "abstract": "  Hybrid MKNF knowledge bases are one of the most prominent tightly integrated\ncombinations of open-world ontology languages with closed-world (non-monotonic)\nrule paradigms. The definition of Hybrid MKNF is parametric on the description\nlogic (DL) underlying the ontology language, in the sense that non-monotonic\nrules can extend any decidable DL language. Two related semantics have been\ndefined for Hybrid MKNF: one that is based on the Stable Model Semantics for\nlogic programs and one on the Well-Founded Semantics (WFS). Under WFS, the\ndefinition of Hybrid MKNF relies on a bottom-up computation that has polynomial\ndata complexity whenever the DL language is tractable. Here we define a general\nquery-driven procedure for Hybrid MKNF that is sound with respect to the stable\nmodel-based semantics, and sound and complete with respect to its WFS variant.\nThis procedure is able to answer a slightly restricted form of conjunctive\nqueries, and is based on tabled rule evaluation extended with an external\noracle that captures reasoning within the ontology. Such an (abstract) oracle\nreceives as input a query along with knowledge already derived, and replies\nwith a (possibly empty) set of atoms, defined in the rules, whose truth would\nsuffice to prove the initial query. With appropriate assumptions on the\ncomplexity of the abstract oracle, the general procedure maintains the data\ncomplexity of the WFS for Hybrid MKNF knowledge bases.\n  To illustrate this approach, we provide a concrete oracle for EL+, a fragment\nof the light-weight DL EL++. Such an oracle has practical use, as EL++ is the\nlanguage underlying OWL 2 EL, which is part of the W3C recommendations for the\nSemantic Web, and is tractable for reasoning tasks such as subsumption. We show\nthat query-driven Hybrid MKNF preserves polynomial data complexity when using\nthe EL+ oracle and WFS.\n"
  },
  {
    "id": "1007.3663",
    "title": "A decidable subclass of finitary programs",
    "abstract": "  Answer set programming - the most popular problem solving paradigm based on\nlogic programs - has been recently extended to support uninterpreted function\nsymbols. All of these approaches have some limitation. In this paper we propose\na class of programs called FP2 that enjoys a different trade-off between\nexpressiveness and complexity. FP2 programs enjoy the following unique\ncombination of properties: (i) the ability of expressing predicates with\ninfinite extensions; (ii) full support for predicates with arbitrary arity;\n(iii) decidability of FP2 membership checking; (iv) decidability of skeptical\nand credulous stable model reasoning for call-safe queries. Odd cycles are\nsupported by composing FP2 programs with argument restricted programs.\n"
  },
  {
    "id": "1007.4868",
    "title": "Predicting Suicide Attacks: A Fuzzy Soft Set Approach",
    "abstract": "  This paper models a decision support system to predict the occurance of\nsuicide attack in a given collection of cities. The system comprises two parts.\nFirst part analyzes and identifies the factors which affect the prediction.\nAdmitting incomplete information and use of linguistic terms by experts, as two\ncharacteristic features of this peculiar prediction problem we exploit the\nTheory of Fuzzy Soft Sets. Hence the Part 2 of the model is an algorithm vz.\nFSP which takes the assessment of factors given in Part 1 as its input and\nproduces a possibility profile of cities likely to receive the accident. The\nalgorithm is of O(2^n) complexity. It has been illustrated by an example solved\nin detail. Simulation results for the algorithm have been presented which give\ninsight into the strengths and weaknesses of FSP. Three different decision\nmaking measures have been simulated and compared in our discussion.\n"
  },
  {
    "id": "1007.5024",
    "title": "A Program-Level Approach to Revising Logic Programs under the Answer Set\n  Semantics",
    "abstract": "  An approach to the revision of logic programs under the answer set semantics\nis presented. For programs P and Q, the goal is to determine the answer sets\nthat correspond to the revision of P by Q, denoted P * Q. A fundamental\nprinciple of classical (AGM) revision, and the one that guides the approach\nhere, is the success postulate. In AGM revision, this stipulates that A is in K\n* A. By analogy with the success postulate, for programs P and Q, this means\nthat the answer sets of Q will in some sense be contained in those of P * Q.\nThe essential idea is that for P * Q, a three-valued answer set for Q,\nconsisting of positive and negative literals, is first determined. The positive\nliterals constitute a regular answer set, while the negated literals make up a\nminimal set of naf literals required to produce the answer set from Q. These\nliterals are propagated to the program P, along with those rules of Q that are\nnot decided by these literals. The approach differs from work in update logic\nprograms in two main respects. First, we ensure that the revising logic program\nhas higher priority, and so we satisfy the success postulate; second, for the\npreference implicit in a revision P * Q, the program Q as a whole takes\nprecedence over P, unlike update logic programs, since answer sets of Q are\npropagated to P. We show that a core group of the AGM postulates are satisfied,\nas are the postulates that have been proposed for update logic programs.\n"
  },
  {
    "id": "1007.5104",
    "title": "An Empirical Study of Borda Manipulation",
    "abstract": "  We study the problem of coalitional manipulation in elections using the\nunweighted Borda rule. We provide empirical evidence of the manipulability of\nBorda elections in the form of two new greedy manipulation algorithms based on\nintuitions from the bin-packing and multiprocessor scheduling domains. Although\nwe have not been able to show that these algorithms beat existing methods in\nthe worst-case, our empirical evaluation shows that they significantly\noutperform the existing method and are able to find optimal manipulations in\nthe vast majority of the randomly generated elections that we tested. These\nempirical results provide further evidence that the Borda rule provides little\ndefense against coalitional manipulation.\n"
  },
  {
    "id": "1007.5130",
    "title": "Resource-Optimal Planning For An Autonomous Planetary Vehicle",
    "abstract": "  Autonomous planetary vehicles, also known as rovers, are small autonomous\nvehicles equipped with a variety of sensors used to perform exploration and\nexperiments on a planet's surface. Rovers work in a partially unknown\nenvironment, with narrow energy/time/movement constraints and, typically, small\ncomputational resources that limit the complexity of on-line planning and\nscheduling, thus they represent a great challenge in the field of autonomous\nvehicles. Indeed, formal models for such vehicles usually involve hybrid\nsystems with nonlinear dynamics, which are difficult to handle by most of the\ncurrent planning algorithms and tools. Therefore, when offline planning of the\nvehicle activities is required, for example for rovers that operate without a\ncontinuous Earth supervision, such planning is often performed on simplified\nmodels that are not completely realistic. In this paper we show how the\nUPMurphi model checking based planning tool can be used to generate\nresource-optimal plans to control the engine of an autonomous planetary\nvehicle, working directly on its hybrid model and taking into account several\nsafety constraints, thus achieving very accurate results.\n"
  },
  {
    "id": "1008.0273",
    "title": "Threat assessment of a possible Vehicle-Born Improvised Explosive Device\n  using DSmT",
    "abstract": "  This paper presents the solution about the threat of a VBIED (Vehicle-Born\nImprovised Explosive Device) obtained with the DSmT (Dezert-Smarandache\nTheory). This problem has been proposed recently to the authors by Simon\nMaskell and John Lavery as a typical illustrative example to try to compare the\ndifferent approaches for dealing with uncertainty for decision-making support.\nThe purpose of this paper is to show in details how a solid justified solution\ncan be obtained from DSmT approach and its fusion rules thanks to a proper\nmodeling of the belief functions involved in this problem.\n"
  },
  {
    "id": "1008.0659",
    "title": "Evaluating and Improving Modern Variable and Revision Ordering\n  Strategies in CSPs",
    "abstract": "  A key factor that can dramatically reduce the search space during constraint\nsolving is the criterion under which the variable to be instantiated next is\nselected. For this purpose numerous heuristics have been proposed. Some of the\nbest of such heuristics exploit information about failures gathered throughout\nsearch and recorded in the form of constraint weights, while others measure the\nimportance of variable assignments in reducing the search space. In this work\nwe experimentally evaluate the most recent and powerful variable ordering\nheuristics, and new variants of them, over a wide range of benchmarks. Results\ndemonstrate that heuristics based on failures are in general more efficient.\nBased on this, we then derive new revision ordering heuristics that exploit\nrecorded failures to efficiently order the propagation list when arc\nconsistency is maintained during search. Interestingly, in addition to reducing\nthe number of constraint checks and list operations, these heuristics are also\nable to cut down the size of the explored search tree.\n"
  },
  {
    "id": "1008.0660",
    "title": "Adaptive Branching for Constraint Satisfaction Problems",
    "abstract": "  The two standard branching schemes for CSPs are d-way and 2-way branching.\nAlthough it has been shown that in theory the latter can be exponentially more\neffective than the former, there is a lack of empirical evidence showing such\ndifferences. To investigate this, we initially make an experimental comparison\nof the two branching schemes over a wide range of benchmarks. Experimental\nresults verify the theoretical gap between d-way and 2-way branching as we move\nfrom a simple variable ordering heuristic like dom to more sophisticated ones\nlike dom/ddeg. However, perhaps surprisingly, experiments also show that when\nstate-of-the-art variable ordering heuristics like dom/wdeg are used then d-way\ncan be clearly more efficient than 2-way branching in many cases. Motivated by\nthis observation, we develop two generic heuristics that can be applied at\ncertain points during search to decide whether 2-way branching or a restricted\nversion of 2-way branching, which is close to d-way branching, will be\nfollowed. The application of these heuristics results in an adaptive branching\nscheme. Experiments with instantiations of the two generic heuristics confirm\nthat search with adaptive branching outperforms search with a fixed branching\nscheme on a wide range of problems.\n"
  },
  {
    "id": "1008.0823",
    "title": "A Homogeneous Reaction Rule Language for Complex Event Processing",
    "abstract": "  Event-driven automation of reactive functionalities for complex event\nprocessing is an urgent need in today's distributed service-oriented\narchitectures and Web-based event-driven environments. An important problem to\nbe addressed is how to correctly and efficiently capture and process the\nevent-based behavioral, reactive logic embodied in reaction rules, and\ncombining this with other conditional decision logic embodied, e.g., in\nderivation rules. This paper elaborates a homogeneous integration approach that\ncombines derivation rules, reaction rules and other rule types such as\nintegrity constraints into the general framework of logic programming, the\nindustrial-strength version of declarative programming. We describe syntax and\nsemantics of the language, implement a distributed web-based middleware using\nenterprise service technologies and illustrate its adequacy in terms of\nexpressiveness, efficiency and scalability through examples extracted from\nindustrial use cases. The developed reaction rule language provides expressive\nfeatures such as modular ID-based updates with support for external imports and\nself-updates of the intensional and extensional knowledge bases, transactions\nincluding integrity testing and roll-backs of update transition paths. It also\nsupports distributed complex event processing, event messaging and event\nquerying via efficient and scalable enterprise middleware technologies and\nevent/action reasoning based on an event/action algebra implemented by an\ninterval-based event calculus variant as a logic inference formalism.\n"
  },
  {
    "id": "1008.1328",
    "title": "Semantic Oriented Agent based Approach towards Engineering Data\n  Management, Web Information Retrieval and User System Communication Problems",
    "abstract": "  The four intensive problems to the software rose by the software industry\n.i.e., User System Communication / Human Machine Interface, Meta Data\nextraction, Information processing & management and Data representation are\ndiscussed in this research paper. To contribute in the field we have proposed\nand described an intelligent semantic oriented agent based search engine\nincluding the concepts of intelligent graphical user interface, natural\nlanguage based information processing, data management and data reconstruction\nfor the final user end information representation.\n"
  },
  {
    "id": "1008.1333",
    "title": "An Agent based Approach towards Metadata Extraction, Modelling and\n  Information Retrieval over the Web",
    "abstract": "  Web development is a challenging research area for its creativity and\ncomplexity. The existing raised key challenge in web technology technologic\ndevelopment is the presentation of data in machine read and process able format\nto take advantage in knowledge based information extraction and maintenance.\nCurrently it is not possible to search and extract optimized results using full\ntext queries because there is no such mechanism exists which can fully extract\nthe semantic from full text queries and then look for particular knowledge\nbased information.\n"
  },
  {
    "id": "1008.1484",
    "title": "A note on communicating between information systems based on including\n  degrees",
    "abstract": "  In order to study the communication between information systems, Gong and\nXiao [Z. Gong and Z. Xiao, Communicating between information systems based on\nincluding degrees, International Journal of General Systems 39 (2010) 189--206]\nproposed the concept of general relation mappings based on including degrees.\nSome properties and the extension for fuzzy information systems of the general\nrelation mappings have been investigated there. In this paper, we point out by\ncounterexamples that several assertions (Lemma 3.1, Lemma 3.2, Theorem 4.1, and\nTheorem 4.3) in the aforementioned work are not true in general.\n"
  },
  {
    "id": "1008.1723",
    "title": "Role of Ontology in Semantic Web Development",
    "abstract": "  World Wide Web (WWW) is the most popular global information sharing and\ncommunication system consisting of three standards .i.e., Uniform Resource\nIdentifier (URL), Hypertext Transfer Protocol (HTTP) and Hypertext Mark-up\nLanguage (HTML). Information is provided in text, image, audio and video\nformats over the web by using HTML which is considered to be unconventional in\ndefining and formalizing the meaning of the context...\n"
  },
  {
    "id": "1008.3314",
    "title": "Maximum entropy models and subjective interestingness: an application to\n  tiles in binary databases",
    "abstract": "  Recent research has highlighted the practical benefits of subjective\ninterestingness measures, which quantify the novelty or unexpectedness of a\npattern when contrasted with any prior information of the data miner\n(Silberschatz and Tuzhilin, 1995; Geng and Hamilton, 2006). A key challenge\nhere is the formalization of this prior information in a way that lends itself\nto the definition of an interestingness subjective measure that is both\nmeaningful and practical.\n  In this paper, we outline a general strategy of how this could be achieved,\nbefore working out the details for a use case that is important in its own\nright.\n  Our general strategy is based on considering prior information as constraints\non a probabilistic model representing the uncertainty about the data. More\nspecifically, we represent the prior information by the maximum entropy\n(MaxEnt) distribution subject to these constraints. We briefly outline various\nmeasures that could subsequently be used to contrast patterns with this MaxEnt\nmodel, thus quantifying their subjective interestingness.\n"
  },
  {
    "id": "1008.3879",
    "title": "A formalism for causal explanations with an Answer Set Programming\n  translation",
    "abstract": "  We examine the practicality for a user of using Answer Set Programming (ASP)\nfor representing logical formalisms. Our example is a formalism aiming at\ncapturing causal explanations from causal information. We show the naturalness\nand relative efficiency of this translation job. We are interested in the ease\nfor writing an ASP program. Limitations of the earlier systems made that in\npractice, the ``declarative aspect'' was more theoretical than practical. We\nshow how recent improvements in working ASP systems facilitate the translation.\n"
  },
  {
    "id": "1008.4257",
    "title": "Learning from Profession Knowledge: Application on Knitting",
    "abstract": "  Knowledge Management is a global process in companies. It includes all the\nprocesses that allow capitalization, sharing and evolution of the Knowledge\nCapital of the firm, generally recognized as a critical resource of the\norganization. Several approaches have been defined to capitalize knowledge but\nfew of them study how to learn from this knowledge. We present in this paper an\napproach that helps to enhance learning from profession knowledge in an\norganisation. We apply our approach on knitting industry.\n"
  },
  {
    "id": "1008.4326",
    "title": "Machine learning for constraint solver design -- A case study for the\n  alldifferent constraint",
    "abstract": "  Constraint solvers are complex pieces of software which require many design\ndecisions to be made by the implementer based on limited information. These\ndecisions affect the performance of the finished solver significantly. Once a\ndesign decision has been made, it cannot easily be reversed, although a\ndifferent decision may be more appropriate for a particular problem.\n  We investigate using machine learning to make these decisions automatically\ndepending on the problem to solve. We use the alldifferent constraint as a case\nstudy. Our system is capable of making non-trivial, multi-level decisions that\nimprove over always making a default choice and can be implemented as part of a\ngeneral-purpose constraint solver.\n"
  },
  {
    "id": "1008.4328",
    "title": "Distributed solving through model splitting",
    "abstract": "  Constraint problems can be trivially solved in parallel by exploring\ndifferent branches of the search tree concurrently. Previous approaches have\nfocused on implementing this functionality in the solver, more or less\ntransparently to the user. We propose a new approach, which modifies the\nconstraint model of the problem. An existing model is split into new models\nwith added constraints that partition the search space. Optionally, additional\nconstraints are imposed that rule out the search already done. The advantages\nof our approach are that it can be implemented easily, computations can be\nstopped and restarted, moved to different machines and indeed solved on\nmachines which are not able to communicate with each other at all.\n"
  },
  {
    "id": "1008.5163",
    "title": "Learning Multi-modal Similarity",
    "abstract": "  In many applications involving multi-media data, the definition of similarity\nbetween items is integral to several key tasks, e.g., nearest-neighbor\nretrieval, classification, and recommendation. Data in such regimes typically\nexhibits multiple modalities, such as acoustic and visual content of video.\nIntegrating such heterogeneous data to form a holistic similarity space is\ntherefore a key challenge to be overcome in many real-world applications.\n  We present a novel multiple kernel learning technique for integrating\nheterogeneous data into a single, unified similarity space. Our algorithm\nlearns an optimal ensemble of kernel transfor- mations which conform to\nmeasurements of human perceptual similarity, as expressed by relative\ncomparisons. To cope with the ubiquitous problems of subjectivity and\ninconsistency in multi- media similarity, we develop graph-based techniques to\nfilter similarity measurements, resulting in a simplified and robust training\nprocedure.\n"
  },
  {
    "id": "1008.5188",
    "title": "Totally Corrective Boosting for Regularized Risk Minimization",
    "abstract": "  Consideration of the primal and dual problems together leads to important new\ninsights into the characteristics of boosting algorithms. In this work, we\npropose a general framework that can be used to design new boosting algorithms.\nA wide variety of machine learning problems essentially minimize a regularized\nrisk functional. We show that the proposed boosting framework, termed CGBoost,\ncan accommodate various loss functions and different regularizers in a\ntotally-corrective optimization fashion. We show that, by solving the primal\nrather than the dual, a large body of totally-corrective boosting algorithms\ncan actually be efficiently solved and no sophisticated convex optimization\nsolvers are needed. We also demonstrate that some boosting algorithms like\nAdaBoost can be interpreted in our framework--even their optimization is not\ntotally corrective. We empirically show that various boosting algorithms based\non the proposed framework perform similarly on the UCIrvine machine learning\ndatasets [1] that we have used in the experiments.\n"
  },
  {
    "id": "1008.5189",
    "title": "Improving the Performance of maxRPC",
    "abstract": "  Max Restricted Path Consistency (maxRPC) is a local consistency for binary\nconstraints that can achieve considerably stronger pruning than arc\nconsistency. However, existing maxRRC algorithms suffer from overheads and\nredundancies as they can repeatedly perform many constraint checks without\ntriggering any value deletions. In this paper we propose techniques that can\nboost the performance of maxRPC algorithms. These include the combined use of\ntwo data structures to avoid many redundant constraint checks, and heuristics\nfor the efficient ordering and execution of certain operations. Based on these,\nwe propose two closely related algorithms. The first one which is a maxRPC\nalgorithm with optimal O(end^3) time complexity, displays good performance when\nused stand-alone, but is expensive to apply during search. The second one\napproximates maxRPC and has O(en^2d^4) time complexity, but a restricted\nversion with O(end^4) complexity can be very efficient when used during search.\nBoth algorithms have O(ed) space complexity. Experimental results demonstrate\nthat the resulting methods constantly outperform previous algorithms for\nmaxRPC, often by large margins, and constitute a more than viable alternative\nto arc consistency on many problems.\n"
  },
  {
    "id": "1009.0347",
    "title": "Solving the Resource Constrained Project Scheduling Problem with\n  Generalized Precedences by Lazy Clause Generation",
    "abstract": "  The technical report presents a generic exact solution approach for\nminimizing the project duration of the resource-constrained project scheduling\nproblem with generalized precedences (Rcpsp/max). The approach uses lazy clause\ngeneration, i.e., a hybrid of finite domain and Boolean satisfiability solving,\nin order to apply nogood learning and conflict-driven search on the solution\ngeneration. Our experiments show the benefit of lazy clause generation for\nfinding an optimal solutions and proving its optimality in comparison to other\nstate-of-the-art exact and non-exact methods. The method is highly robust: it\nmatched or bettered the best known results on all of the 2340 instances we\nexamined except 3, according to the currently available data on the PSPLib. Of\nthe 631 open instances in this set it closed 573 and improved the bounds of 51\nof the remaining 58 instances.\n"
  },
  {
    "id": "1009.0407",
    "title": "Experimental Evaluation of Branching Schemes for the CSP",
    "abstract": "  The search strategy of a CP solver is determined by the variable and value\nordering heuristics it employs and by the branching scheme it follows. Although\nthe effects of variable and value ordering heuristics on search effort have\nbeen widely studied, the effects of different branching schemes have received\nless attention. In this paper we study this effect through an experimental\nevaluation that includes standard branching schemes such as 2-way, d-way, and\ndichotomic domain splitting, as well as variations of set branching where\nbranching is performed on sets of values. We also propose and evaluate a\ngeneric approach to set branching where the partition of a domain into sets is\ncreated using the scores assigned to values by a value ordering heuristic, and\na clustering algorithm from machine learning. Experimental results demonstrate\nthat although exponential differences between branching schemes, as predicted\nin theory between 2-way and d-way branching, are not very common, still the\nchoice of branching scheme can make quite a difference on certain classes of\nproblems. Set branching methods are very competitive with 2-way branching and\noutperform it on some problem classes. A statistical analysis of the results\nreveals that our generic clustering-based set branching method is the best\namong the methods compared.\n"
  },
  {
    "id": "1009.0451",
    "title": "The Challenge of Believability in Video Games: Definitions, Agents\n  Models and Imitation Learning",
    "abstract": "  In this paper, we address the problem of creating believable agents (virtual\ncharacters) in video games. We consider only one meaning of believability,\n``giving the feeling of being controlled by a player'', and outline the problem\nof its evaluation. We present several models for agents in games which can\nproduce believable behaviours, both from industry and research. For high level\nof believability, learning and especially imitation learning seems to be the\nway to go. We make a quick overview of different approaches to make video\ngames' agents learn from players. To conclude we propose a two-step method to\ndevelop new models for believable agents. First we must find the criteria for\nbelievability for our application and define an evaluation method. Then the\nmodel and the learning algorithm can be designed.\n"
  },
  {
    "id": "1009.0501",
    "title": "Automatable Evaluation Method Oriented toward Behaviour Believability\n  for Video Games",
    "abstract": "  Classic evaluation methods of believable agents are time-consuming because\nthey involve many human to judge agents. They are well suited to validate work\non new believable behaviours models. However, during the implementation,\nnumerous experiments can help to improve agents' believability. We propose a\nmethod which aim at assessing how much an agent's behaviour looks like humans'\nbehaviours. By representing behaviours with vectors, we can store data computed\nfor humans and then evaluate as many agents as needed without further need of\nhumans. We present a test experiment which shows that even a simple evaluation\nfollowing our method can reveal differences between quite believable agents and\nhumans. This method seems promising although, as shown in our experiment,\nresults' analysis can be difficult.\n"
  },
  {
    "id": "1009.2003",
    "title": "AI 3D Cybug Gaming",
    "abstract": "  In this short paper I briefly discuss 3D war Game based on artificial\nintelligence concepts called AI WAR. Going in to the details, I present the\nimportance of CAICL language and how this language is used in AI WAR. Moreover\nI also present a designed and implemented 3D War Cybug for AI WAR using CAICL\nand discus the implemented strategy to defeat its enemies during the game life.\n"
  },
  {
    "id": "1009.2041",
    "title": "Multi-Agent Only-Knowing Revisited",
    "abstract": "  Levesque introduced the notion of only-knowing to precisely capture the\nbeliefs of a knowledge base. He also showed how only-knowing can be used to\nformalize non-monotonic behavior within a monotonic logic. Despite its appeal,\nall attempts to extend only-knowing to the many agent case have undesirable\nproperties. A belief model by Halpern and Lakemeyer, for instance, appeals to\nproof-theoretic constructs in the semantics and needs to axiomatize validity as\npart of the logic. It is also not clear how to generalize their ideas to a\nfirst-order case. In this paper, we propose a new account of multi-agent\nonly-knowing which, for the first time, has a natural possible-world semantics\nfor a quantified language with equality. We then provide, for the propositional\nfragment, a sound and complete axiomatization that faithfully lifts Levesque's\nproof theory to the many agent case. We also discuss comparisons to the earlier\napproach by Halpern and Lakemeyer.\n"
  },
  {
    "id": "1009.4586",
    "title": "Optimal Bangla Keyboard Layout using Association Rule of Data Mining",
    "abstract": "  In this paper we present an optimal Bangla Keyboard Layout, which distributes\nthe load equally on both hands so that maximizing the ease and minimizing the\neffort. Bangla alphabet has a large number of letters, for this it is difficult\nto type faster using Bangla keyboard. Our proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Here we use the\nassociation rule of data mining to distribute the Bangla characters in the\nkeyboard. First, we analyze the frequencies of data consisting of monograph,\ndigraph and trigraph, which are derived from data wire-house, and then used\nassociation rule of data mining to distribute the Bangla characters in the\nlayout. Finally, we propose a Bangla Keyboard Layout. Experimental results on\nseveral keyboard layout shows the effectiveness of the proposed approach with\nbetter performance.\n"
  },
  {
    "id": "1009.4982",
    "title": "Optimal Bangla Keyboard Layout using Data Mining Technique",
    "abstract": "  This paper presents an optimal Bangla Keyboard Layout, which distributes the\nload equally on both hands so that maximizing the ease and minimizing the\neffort. Bangla alphabet has a large number of letters, for this it is difficult\nto type faster using Bangla keyboard. Our proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Here we use the\nassociation rule of data mining to distribute the Bangla characters in the\nkeyboard. First, we analyze the frequencies of data consisting of monograph,\ndigraph and trigraph, which are derived from data wire-house, and then used\nassociation rule of data mining to distribute the Bangla characters in the\nlayout. Experimental results on several data show the effectiveness of the\nproposed approach with better performance.\n"
  },
  {
    "id": "1009.5048",
    "title": "The Most Advantageous Bangla Keyboard Layout Using Data Mining Technique",
    "abstract": "  Bangla alphabet has a large number of letters, for this it is complicated to\ntype faster using Bangla keyboard. The proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Association rule\nof data mining to distribute the Bangla characters in the keyboard is used\nhere. The frequencies of data consisting of monograph, digraph and trigraph are\nanalyzed, which are derived from data wire-house, and then used association\nrule of data mining to distribute the Bangla characters in the layout.\nExperimental results on several data show the effectiveness of the proposed\napproach with better performance. This paper presents an optimal Bangla\nKeyboard Layout, which distributes the load equally on both hands so that\nmaximizing the ease and minimizing the effort.\n"
  },
  {
    "id": "1009.5268",
    "title": "General Scaled Support Vector Machines",
    "abstract": "  Support Vector Machines (SVMs) are popular tools for data mining tasks such\nas classification, regression, and density estimation. However, original SVM\n(C-SVM) only considers local information of data points on or over the margin.\nTherefore, C-SVM loses robustness. To solve this problem, one approach is to\ntranslate (i.e., to move without rotation or change of shape) the hyperplane\naccording to the distribution of the entire data. But existing work can only be\napplied for 1-D case. In this paper, we propose a simple and efficient method\ncalled General Scaled SVM (GS-SVM) to extend the existing approach to\nmulti-dimensional case. Our method translates the hyperplane according to the\ndistribution of data projected on the normal vector of the hyperplane. Compared\nwith C-SVM, GS-SVM has better performance on several data sets.\n"
  },
  {
    "id": "1009.5290",
    "title": "Measuring Similarity of Graphs and their Nodes by Neighbor Matching",
    "abstract": "  The problem of measuring similarity of graphs and their nodes is important in\na range of practical problems. There is a number of proposed measures, some of\nthem being based on iterative calculation of similarity between two graphs and\nthe principle that two nodes are as similar as their neighbors are. In our\nwork, we propose one novel method of that sort, with a refined concept of\nsimilarity of two nodes that involves matching of their neighbors. We prove\nconvergence of the proposed method and show that it has some additional\ndesirable properties that, to our knowledge, the existing methods lack. We\nillustrate the method on two specific problems and empirically compare it to\nother methods.\n"
  },
  {
    "id": "1010.0298",
    "title": "Steepest Ascent Hill Climbing For A Mathematical Problem",
    "abstract": "  The paper proposes artificial intelligence technique called hill climbing to\nfind numerical solutions of Diophantine Equations. Such equations are important\nas they have many applications in fields like public key cryptography, integer\nfactorization, algebraic curves, projective curves and data dependency in super\ncomputers. Importantly, it has been proved that there is no general method to\nfind solutions of such equations. This paper is an attempt to find numerical\nsolutions of Diophantine equations using steepest ascent version of Hill\nClimbing. The method, which uses tree representation to depict possible\nsolutions of Diophantine equations, adopts a novel methodology to generate\nsuccessors. The heuristic function used help to make the process of finding\nsolution as a minimization process. The work illustrates the effectiveness of\nthe proposed methodology using a class of Diophantine equations given by a1. x1\np1 + a2. x2 p2 + ...... + an . xn pn = N where ai and N are integers. The\nexperimental results validate that the procedure proposed is successful in\nfinding solutions of Diophantine Equations with sufficiently large powers and\nlarge number of variables.\n"
  },
  {
    "id": "1010.2102",
    "title": "Hierarchical Multiclass Decompositions with Application to Authorship\n  Determination",
    "abstract": "  This paper is mainly concerned with the question of how to decompose\nmulticlass classification problems into binary subproblems. We extend known\nJensen-Shannon bounds on the Bayes risk of binary problems to hierarchical\nmulticlass problems and use these bounds to develop a heuristic procedure for\nconstructing hierarchical multiclass decomposition for multinomials. We test\nour method and compare it to the well known \"all-pairs\" decomposition. Our\ntests are performed using a new authorship determination benchmark test of\nmachine learning authors. The new method consistently outperforms the all-pairs\ndecomposition when the number of classes is small and breaks even on larger\nmulticlass problems. Using both methods, the classification accuracy we\nachieve, using an SVM over a feature set consisting of both high frequency\nsingle tokens and high frequency token-pairs, appears to be exceptionally high\ncompared to known results in authorship determination.\n"
  },
  {
    "id": "1010.3177",
    "title": "Introduction to the iDian",
    "abstract": "  The iDian (previously named as the Operation Agent System) is a framework\ndesigned to enable computer users to operate software in natural language.\nDistinct from current speech-recognition systems, our solution supports\nformat-free combinations of orders, and is open to both developers and\ncustomers. We used a multi-layer structure to build the entire framework,\napproached rule-based natural language processing, and implemented demos\nnarrowing down to Windows, text-editing and a few other applications. This\nessay will firstly give an overview of the entire system, and then scrutinize\nthe functions and structure of the system, and finally discuss the prospective\nde-velopment, esp. on-line interaction functions.\n"
  },
  {
    "id": "1010.4385",
    "title": "A Protocol for Self-Synchronized Duty-Cycling in Sensor Networks:\n  Generic Implementation in Wiselib",
    "abstract": "  In this work we present a protocol for self-synchronized duty-cycling in\nwireless sensor networks with energy harvesting capabilities. The protocol is\nimplemented in Wiselib, a library of generic algorithms for sensor networks.\nSimulations are conducted with the sensor network simulator Shawn. They are\nbased on the specifications of real hardware known as iSense sensor nodes. The\nexperimental results show that the proposed mechanism is able to adapt to\nchanging energy availabilities. Moreover, it is shown that the system is very\nrobust against packet loss.\n"
  },
  {
    "id": "1010.4561",
    "title": "New S-norm and T-norm Operators for Active Learning Method",
    "abstract": "  Active Learning Method (ALM) is a soft computing method used for modeling and\ncontrol based on fuzzy logic. All operators defined for fuzzy sets must serve\nas either fuzzy S-norm or fuzzy T-norm. Despite being a powerful modeling\nmethod, ALM does not possess operators which serve as S-norms and T-norms which\ndeprive it of a profound analytical expression/form. This paper introduces two\nnew operators based on morphology which satisfy the following conditions:\nFirst, they serve as fuzzy S-norm and T-norm. Second, they satisfy Demorgans\nlaw, so they complement each other perfectly. These operators are investigated\nvia three viewpoints: Mathematics, Geometry and fuzzy logic.\n"
  },
  {
    "id": "1010.4609",
    "title": "A Partial Taxonomy of Substitutability and Interchangeability",
    "abstract": "  Substitutability, interchangeability and related concepts in Constraint\nProgramming were introduced approximately twenty years ago and have given rise\nto considerable subsequent research. We survey this work, classify, and relate\nthe different concepts, and indicate directions for future work, in particular\nwith respect to making connections with research into symmetry breaking. This\npaper is a condensed version of a larger work in progress.\n"
  },
  {
    "id": "1010.4784",
    "title": "Learning under Concept Drift: an Overview",
    "abstract": "  Concept drift refers to a non stationary learning problem over time. The\ntraining and the application data often mismatch in real life problems. In this\nreport we present a context of concept drift problem 1. We focus on the issues\nrelevant to adaptive training set formation. We present the framework and\nterminology, and formulate a global picture of concept drift learners design.\nWe start with formalizing the framework for the concept drifting data in\nSection 1. In Section 2 we discuss the adaptivity mechanisms of the concept\ndrift learners. In Section 3 we overview the principle mechanisms of concept\ndrift learners. In this chapter we give a general picture of the available\nalgorithms and categorize them based on their properties. Section 5 discusses\nthe related research fields and Section 5 groups and presents major concept\ndrift applications. This report is intended to give a bird's view of concept\ndrift research field, provide a context of the research and position it within\nbroad spectrum of research fields and applications.\n"
  },
  {
    "id": "1010.4830",
    "title": "A Unifying Probabilistic Perspective for Spectral Dimensionality\n  Reduction: Insights and New Models",
    "abstract": "  We introduce a new perspective on spectral dimensionality reduction which\nviews these methods as Gaussian Markov random fields (GRFs). Our unifying\nperspective is based on the maximum entropy principle which is in turn inspired\nby maximum variance unfolding. The resulting model, which we call maximum\nentropy unfolding (MEU) is a nonlinear generalization of principal component\nanalysis. We relate the model to Laplacian eigenmaps and isomap. We show that\nparameter fitting in the locally linear embedding (LLE) is approximate maximum\nlikelihood MEU. We introduce a variant of LLE that performs maximum likelihood\nexactly: Acyclic LLE (ALLE). We show that MEU and ALLE are competitive with the\nleading spectral approaches on a robot navigation visualization and a human\nmotion capture data set. Finally the maximum likelihood perspective allows us\nto introduce a new approach to dimensionality reduction based on L1\nregularization of the Gaussian random field via the graphical lasso.\n"
  },
  {
    "id": "1010.5426",
    "title": "Translation-Invariant Representation for Cumulative Foot Pressure Images",
    "abstract": "  Human can be distinguished by different limb movements and unique ground\nreaction force. Cumulative foot pressure image is a 2-D cumulative ground\nreaction force during one gait cycle. Although it contains pressure spatial\ndistribution information and pressure temporal distribution information, it\nsuffers from several problems including different shoes and noise, when putting\nit into practice as a new biometric for pedestrian identification. In this\npaper, we propose a hierarchical translation-invariant representation for\ncumulative foot pressure images, inspired by the success of Convolutional deep\nbelief network for digital classification. Key contribution in our approach is\ndiscriminative hierarchical sparse coding scheme which helps to learn useful\ndiscriminative high-level visual features. Based on the feature representation\nof cumulative foot pressure images, we develop a pedestrian recognition system\nwhich is invariant to three different shoes and slight local shape change.\nExperiments are conducted on a proposed open dataset that contains more than\n2800 cumulative foot pressure images from 118 subjects. Evaluations suggest the\neffectiveness of the proposed method and the potential of cumulative foot\npressure images as a biometric.\n"
  },
  {
    "id": "1011.0098",
    "title": "Qualitative Reasoning about Relative Direction on Adjustable Levels of\n  Granularity",
    "abstract": "  An important issue in Qualitative Spatial Reasoning is the representation of\nrelative direction. In this paper we present simple geometric rules that enable\nreasoning about relative direction between oriented points. This framework, the\nOriented Point Algebra OPRA_m, has a scalable granularity m. We develop a\nsimple algorithm for computing the OPRA_m composition tables and prove its\ncorrectness. Using a composition table, algebraic closure for a set of OPRA\nstatements is sufficient to solve spatial navigation tasks. And it turns out\nthat scalable granularity is useful in these navigation tasks.\n"
  },
  {
    "id": "1011.0187",
    "title": "A Distributed AI Aided 3D Domino Game",
    "abstract": "  In the article a turn-based game played on four computers connected via\nnetwork is investigated. There are three computers with natural intelligence\nand one with artificial intelligence. Game table is seen by each player's own\nview point in all players' monitors. Domino pieces are three dimensional. For\ndistributed systems TCP/IP protocol is used. In order to get 3D image,\nMicrosoft XNA technology is applied. Domino 101 game is nondeterministic game\nthat is result of the game depends on the initial random distribution of the\npieces. Number of the distributions is equal to the multiplication of following\ncombinations: . Moreover, in this game that is played by four people, players\nare divided into 2 pairs. Accordingly, we cannot predict how the player uses\nthe dominoes that is according to the dominoes of his/her partner or according\nto his/her own dominoes. The fact that the natural intelligence can be a player\nin any level affects the outcome. These reasons make it difficult to develop an\nAI. In the article four levels of AI are developed. The AI in the first level\nis equivalent to the intelligence of a child who knows the rules of the game\nand recognizes the numbers. The AI in this level plays if it has any domino,\nsuitable to play or says pass. In most of the games which can be played on the\ninternet, the AI does the same. But the AI in the last level is a master\nplayer, and it can develop itself according to its competitors' levels.\n"
  },
  {
    "id": "1011.0190",
    "title": "Prunnig Algorithm of Generation a Minimal Set of Rule Reducts Based on\n  Rough Set Theory",
    "abstract": "  In this paper it is considered rule reduct generation problem, based on Rough\nSet Theory. Rule Reduct Generation (RG) and Modified Rule Generation (MRG)\nalgorithms are well-known. Alternative to these algorithms Pruning Algorithm of\nGeneration A Minimal Set of Rule Reducts, or briefly Pruning Rule Generation\n(PRG) algorithm is developed. PRG algorithm uses tree structured data type. PRG\nalgorithm is compared with RG and MRG algorithms\n"
  },
  {
    "id": "1011.0233",
    "title": "Reasoning about Cardinal Directions between Extended Objects: The\n  Hardness Result",
    "abstract": "  The cardinal direction calculus (CDC) proposed by Goyal and Egenhofer is a\nvery expressive qualitative calculus for directional information of extended\nobjects. Early work has shown that consistency checking of complete networks of\nbasic CDC constraints is tractable while reasoning with the CDC in general is\nNP-hard. This paper shows, however, if allowing some constraints unspecified,\nthen consistency checking of possibly incomplete networks of basic CDC\nconstraints is already intractable. This draws a sharp boundary between the\ntractable and intractable subclasses of the CDC. The result is achieved by a\nreduction from the well-known 3-SAT problem.\n"
  },
  {
    "id": "1011.0330",
    "title": "Imitation learning of motor primitives and language bootstrapping in\n  robots",
    "abstract": "  Imitation learning in robots, also called programing by demonstration, has\nmade important advances in recent years, allowing humans to teach context\ndependant motor skills/tasks to robots. We propose to extend the usual contexts\ninvestigated to also include acoustic linguistic expressions that might denote\na given motor skill, and thus we target joint learning of the motor skills and\ntheir potential acoustic linguistic name. In addition to this, a modification\nof a class of existing algorithms within the imitation learning framework is\nmade so that they can handle the unlabeled demonstration of several tasks/motor\nprimitives without having to inform the imitator of what task is being\ndemonstrated or what the number of tasks are, which is a necessity for language\nlearning, i.e; if one wants to teach naturally an open number of new motor\nskills together with their acoustic names. Finally, a mechanism for detecting\nwhether or not linguistic input is relevant to the task is also proposed, and\nour architecture also allows the robot to find the right framing for a given\nidentified motor primitive. With these additions it becomes possible to build\nan imitator that bridges the gap between imitation learning and language\nlearning by being able to learn linguistic expressions using methods from the\nimitation learning community. In this sense the imitator can learn a word by\nguessing whether a certain speech pattern present in the context means that a\nspecific task is to be executed. The imitator is however not assumed to know\nthat speech is relevant and has to figure this out on its own by looking at the\ndemonstrations: indeed, the architecture allows the robot to transparently also\nlearn tasks which should not be triggered by an acoustic word, but for example\nby the color or position of an object or a gesture made by someone in the\nenvironment. To demonstrate this ability to find the ...\n"
  },
  {
    "id": "1011.0628",
    "title": "Significance of Classification Techniques in Prediction of Learning\n  Disabilities",
    "abstract": "  The aim of this study is to show the importance of two classification\ntechniques, viz. decision tree and clustering, in prediction of learning\ndisabilities (LD) of school-age children. LDs affect about 10 percent of all\nchildren enrolled in schools. The problems of children with specific learning\ndisabilities have been a cause of concern to parents and teachers for some\ntime. Decision trees and clustering are powerful and popular tools used for\nclassification and prediction in Data mining. Different rules extracted from\nthe decision tree are used for prediction of learning disabilities. Clustering\nis the assignment of a set of observations into subsets, called clusters, which\nare useful in finding the different signs and symptoms (attributes) present in\nthe LD affected child. In this paper, J48 algorithm is used for constructing\nthe decision tree and K-means algorithm is used for creating the clusters. By\napplying these classification techniques, LD in any child can be identified.\n"
  },
  {
    "id": "1011.0950",
    "title": "Detecting Ontological Conflicts in Protocols between Semantic Web\n  Services",
    "abstract": "  The task of verifying the compatibility between interacting web services has\ntraditionally been limited to checking the compatibility of the interaction\nprotocol in terms of message sequences and the type of data being exchanged.\nSince web services are developed largely in an uncoordinated way, different\nservices often use independently developed ontologies for the same domain\ninstead of adhering to a single ontology as standard. In this work we\ninvestigate the approaches that can be taken by the server to verify the\npossibility to reach a state with semantically inconsistent results during the\nexecution of a protocol with a client, if the client ontology is published.\nOften database is used to store the actual data along with the ontologies\ninstead of storing the actual data as a part of the ontology description. It is\nimportant to observe that at the current state of the database the semantic\nconflict state may not be reached even if the verification done by the server\nindicates the possibility of reaching a conflict state. A relational algebra\nbased decision procedure is also developed to incorporate the current state of\nthe client and the server databases in the overall verification procedure.\n"
  },
  {
    "id": "1011.1478",
    "title": "Gradient Computation In Linear-Chain Conditional Random Fields Using The\n  Entropy Message Passing Algorithm",
    "abstract": "  The paper proposes a numerically stable recursive algorithm for the exact\ncomputation of the linear-chain conditional random field gradient. It operates\nas a forward algorithm over the log-domain expectation semiring and has the\npurpose of enhancing memory efficiency when applied to long observation\nsequences. Unlike the traditional algorithm based on the forward-backward\nrecursions, the memory complexity of our algorithm does not depend on the\nsequence length. The experiments on real data show that it can be useful for\nthe problems which deal with long sequences.\n"
  },
  {
    "id": "1011.1660",
    "title": "Reinforcement Learning Based on Active Learning Method",
    "abstract": "  In this paper, a new reinforcement learning approach is proposed which is\nbased on a powerful concept named Active Learning Method (ALM) in modeling. ALM\nexpresses any multi-input-single-output system as a fuzzy combination of some\nsingle-input-singleoutput systems. The proposed method is an actor-critic\nsystem similar to Generalized Approximate Reasoning based Intelligent Control\n(GARIC) structure to adapt the ALM by delayed reinforcement signals. Our system\nuses Temporal Difference (TD) learning to model the behavior of useful actions\nof a control system. The goodness of an action is modeled on Reward-\nPenalty-Plane. IDS planes will be updated according to this plane. It is shown\nthat the system can learn with a predefined fuzzy system or without it (through\nrandom actions).\n"
  },
  {
    "id": "1011.1662",
    "title": "A New Sufficient Condition for 1-Coverage to Imply Connectivity",
    "abstract": "  An effective approach for energy conservation in wireless sensor networks is\nscheduling sleep intervals for extraneous nodes while the remaining nodes stay\nactive to provide continuous service. For the sensor network to operate\nsuccessfully the active nodes must maintain both sensing coverage and network\nconnectivity, It proved before if the communication range of nodes is at least\ntwice the sensing range, complete coverage of a convex area implies\nconnectivity among the working set of nodes. In this paper we consider a\nrectangular region A = a *b, such that R a R b s s {\\pounds}, {\\pounds}, where\ns R is the sensing range of nodes. and put a constraint on minimum allowed\ndistance between nodes(s). according to this constraint we present a new lower\nbound for communication range relative to sensing range of sensors(s 2 + 3 *R)\nthat complete coverage of considered area implies connectivity among the\nworking set of nodes; also we present a new distribution method, that satisfy\nour constraint.\n"
  },
  {
    "id": "1011.2304",
    "title": "Target tracking in the recommender space: Toward a new recommender\n  system based on Kalman filtering",
    "abstract": "  In this paper, we propose a new approach for recommender systems based on\ntarget tracking by Kalman filtering. We assume that users and their seen\nresources are vectors in the multidimensional space of the categories of the\nresources. Knowing this space, we propose an algorithm based on a Kalman filter\nto track users and to predict the best prediction of their future position in\nthe recommendation space.\n"
  },
  {
    "id": "1011.4362",
    "title": "Should one compute the Temporal Difference fix point or minimize the\n  Bellman Residual? The unified oblique projection view",
    "abstract": "  We investigate projection methods, for evaluating a linear approximation of\nthe value function of a policy in a Markov Decision Process context. We\nconsider two popular approaches, the one-step Temporal Difference fix-point\ncomputation (TD(0)) and the Bellman Residual (BR) minimization. We describe\nexamples, where each method outperforms the other. We highlight a simple\nrelation between the objective function they minimize, and show that while BR\nenjoys a performance guarantee, TD(0) does not in general. We then propose a\nunified view in terms of oblique projections of the Bellman equation, which\nsubstantially simplifies and extends the characterization of (schoknecht,2002)\nand the recent analysis of (Yu & Bertsekas, 2008). Eventually, we describe some\nsimulations that suggest that if the TD(0) solution is usually slightly better\nthan the BR solution, its inherent numerical instability makes it very bad in\nsome cases, and thus worse on average.\n"
  },
  {
    "id": "1011.5349",
    "title": "Distributed Graph Coloring: An Approach Based on the Calling Behavior of\n  Japanese Tree Frogs",
    "abstract": "  Graph coloring, also known as vertex coloring, considers the problem of\nassigning colors to the nodes of a graph such that adjacent nodes do not share\nthe same color. The optimization version of the problem concerns the\nminimization of the number of used colors. In this paper we deal with the\nproblem of finding valid colorings of graphs in a distributed way, that is, by\nmeans of an algorithm that only uses local information for deciding the color\nof the nodes. Such algorithms prescind from any central control. Due to the\nfact that quite a few practical applications require to find colorings in a\ndistributed way, the interest in distributed algorithms for graph coloring has\nbeen growing during the last decade. As an example consider wireless ad-hoc and\nsensor networks, where tasks such as the assignment of frequencies or the\nassignment of TDMA slots are strongly related to graph coloring.\n  The algorithm proposed in this paper is inspired by the calling behavior of\nJapanese tree frogs. Male frogs use their calls to attract females.\nInterestingly, groups of males that are located nearby each other desynchronize\ntheir calls. This is because female frogs are only able to correctly localize\nthe male frogs when their calls are not too close in time. We experimentally\nshow that our algorithm is very competitive with the current state of the art,\nusing different sets of problem instances and comparing to one of the most\ncompetitive algorithms from the literature.\n"
  },
  {
    "id": "1011.5480",
    "title": "Bayesian Modeling of a Human MMORPG Player",
    "abstract": "  This paper describes an application of Bayesian programming to the control of\nan autonomous avatar in a multiplayer role-playing game (the example is based\non World of Warcraft). We model a particular task, which consists of choosing\nwhat to do and to select which target in a situation where allies and foes are\npresent. We explain the model in Bayesian programming and show how we could\nlearn the conditional probabilities from data gathered during human-played\nsessions.\n"
  },
  {
    "id": "1011.5951",
    "title": "Reinforcement Learning in Partially Observable Markov Decision Processes\n  using Hybrid Probabilistic Logic Programs",
    "abstract": "  We present a probabilistic logic programming framework to reinforcement\nlearning, by integrating reinforce-ment learning, in POMDP environments, with\nnormal hybrid probabilistic logic programs with probabilistic answer set\nseman-tics, that is capable of representing domain-specific knowledge. We\nformally prove the correctness of our approach. We show that the complexity of\nfinding a policy for a reinforcement learning problem in our approach is\nNP-complete. In addition, we show that any reinforcement learning problem can\nbe encoded as a classical logic program with answer set semantics. We also show\nthat a reinforcement learning problem can be encoded as a SAT problem. We\npresent a new high level action description language that allows the factored\nrepresentation of POMDP. Moreover, we modify the original model of POMDP so\nthat it be able to distinguish between knowledge producing actions and actions\nthat change the environment.\n"
  },
  {
    "id": "1011.6220",
    "title": "Multimodal Biometric Systems - Study to Improve Accuracy and Performance",
    "abstract": "  Biometrics is the science and technology of measuring and analyzing\nbiological data of human body, extracting a feature set from the acquired data,\nand comparing this set against to the template set in the database.\nExperimental studies show that Unimodal biometric systems had many\ndisadvantages regarding performance and accuracy. Multimodal biometric systems\nperform better than unimodal biometric systems and are popular even more\ncomplex also. We examine the accuracy and performance of multimodal biometric\nauthentication systems using state of the art Commercial Off- The-Shelf (COTS)\nproducts. Here we discuss fingerprint and face biometric systems, decision and\nfusion techniques used in these systems. We also discuss their advantage over\nunimodal biometric systems.\n"
  },
  {
    "id": "1012.0322",
    "title": "A Bayesian Methodology for Estimating Uncertainty of Decisions in\n  Safety-Critical Systems",
    "abstract": "  Uncertainty of decisions in safety-critical engineering applications can be\nestimated on the basis of the Bayesian Markov Chain Monte Carlo (MCMC)\ntechnique of averaging over decision models. The use of decision tree (DT)\nmodels assists experts to interpret causal relations and find factors of the\nuncertainty. Bayesian averaging also allows experts to estimate the uncertainty\naccurately when a priori information on the favored structure of DTs is\navailable. Then an expert can select a single DT model, typically the Maximum a\nPosteriori model, for interpretation purposes. Unfortunately, a priori\ninformation on favored structure of DTs is not always available. For this\nreason, we suggest a new prior on DTs for the Bayesian MCMC technique. We also\nsuggest a new procedure of selecting a single DT and describe an application\nscenario. In our experiments on the Short-Term Conflict Alert data our\ntechnique outperforms the existing Bayesian techniques in predictive accuracy\nof the selected single DTs.\n"
  },
  {
    "id": "1012.0830",
    "title": "Using ASP with recent extensions for causal explanations",
    "abstract": "  We examine the practicality for a user of using Answer Set Programming (ASP)\nfor representing logical formalisms. We choose as an example a formalism aiming\nat capturing causal explanations from causal information. We provide an\nimplementation, showing the naturalness and relative efficiency of this\ntranslation job. We are interested in the ease for writing an ASP program, in\naccordance with the claimed ``declarative'' aspect of ASP. Limitations of the\nearlier systems (poor data structure and difficulty in reusing pieces of\nprograms) made that in practice, the ``declarative aspect'' was more\ntheoretical than practical. We show how recent improvements in working ASP\nsystems facilitate a lot the translation, even if a few improvements could\nstill be useful.\n"
  },
  {
    "id": "1012.1255",
    "title": "URSA: A System for Uniform Reduction to SAT",
    "abstract": "  There are a huge number of problems, from various areas, being solved by\nreducing them to SAT. However, for many applications, translation into SAT is\nperformed by specialized, problem-specific tools. In this paper we describe a\nnew system for uniform solving of a wide class of problems by reducing them to\nSAT. The system uses a new specification language URSA that combines imperative\nand declarative programming paradigms. The reduction to SAT is defined\nprecisely by the semantics of the specification language. The domain of the\napproach is wide (e.g., many NP-complete problems can be simply specified and\nthen solved by the system) and there are problems easily solvable by the\nproposed system, while they can be hardly solved by using other programming\nlanguages or constraint programming systems. So, the system can be seen not\nonly as a tool for solving problems by reducing them to SAT, but also as a\ngeneral-purpose constraint solving system (for finite domains). In this paper,\nwe also describe an open-source implementation of the described approach. The\nperformed experiments suggest that the system is competitive to\nstate-of-the-art related modelling systems.\n"
  },
  {
    "id": "1012.1619",
    "title": "Are SNOMED CT Browsers Ready for Institutions? Introducing MySNOM",
    "abstract": "  SNOMED Clinical Terms (SNOMED CT) is one of the most widespread ontologies in\nthe life sciences, with more than 300,000 concepts and relationships, but is\ndistributed with no associated software tools. In this paper we present MySNOM,\na web-based SNOMED CT browser. MySNOM allows organizations to browse their own\ndistribution of SNOMED CT under a controlled environment, focuses on navigating\nusing the structure of SNOMED CT, and has diagramming capabilities.\n"
  },
  {
    "id": "1012.1635",
    "title": "A study on the relation between linguistics-oriented and domain-specific\n  semantics",
    "abstract": "  In this paper we dealt with the comparison and linking between lexical\nresources with domain knowledge provided by ontologies. It is one of the issues\nfor the combination of the Semantic Web Ontologies and Text Mining. We\ninvestigated the relations between the linguistics oriented and domain-specific\nsemantics, by associating the GO biological process concepts to the FrameNet\nsemantic frames. The result shows the gaps between the linguistics-oriented and\ndomain-specific semantics on the classification of events and the grouping of\ntarget words. The result provides valuable information for the improvement of\ndomain ontologies supporting for text mining systems. And also, it will result\nin benefits to language understanding technology.\n"
  },
  {
    "id": "1012.1643",
    "title": "Process Makna - A Semantic Wiki for Scientific Workflows",
    "abstract": "  Virtual e-Science infrastructures supporting Web-based scientific workflows\nare an example for knowledge-intensive collaborative and weakly-structured\nprocesses where the interaction with the human scientists during process\nexecution plays a central role. In this paper we propose the lightweight\ndynamic user-friendly interaction with humans during execution of scientific\nworkflows via the low-barrier approach of Semantic Wikis as an intuitive\ninterface for non-technical scientists. Our Process Makna Semantic Wiki system\nis a novel combination of an business process management system adapted for\nscientific workflows with a Corporate Semantic Web Wiki user interface\nsupporting knowledge intensive human interaction tasks during scientific\nworkflow execution.\n"
  },
  {
    "id": "1012.1646",
    "title": "Use of semantic technologies for the development of a dynamic\n  trajectories generator in a Semantic Chemistry eLearning platform",
    "abstract": "  ChemgaPedia is a multimedia, webbased eLearning service platform that\ncurrently contains about 18.000 pages organized in 1.700 chapters covering the\ncomplete bachelor studies in chemistry and related topics of chemistry,\npharmacy, and life sciences. The eLearning encyclopedia contains some 25.000\nmedia objects and the eLearning platform provides services such as virtual and\nremote labs for experiments. With up to 350.000 users per month the platform is\nthe most frequently used scientific educational service in the German spoken\nInternet. In this demo we show the benefit of mapping the static eLearning\ncontents of ChemgaPedia to a Linked Data representation for Semantic Chemistry\nwhich allows for generating dynamic eLearning paths tailored to the semantic\nprofiles of the users.\n"
  },
  {
    "id": "1012.1654",
    "title": "Using Semantic Wikis for Structured Argument in Medical Domain",
    "abstract": "  This research applies ideas from argumentation theory in the context of\nsemantic wikis, aiming to provide support for structured-large scale\nargumentation between human agents. The implemented prototype is exemplified by\nmodelling the MMR vaccine controversy.\n"
  },
  {
    "id": "1012.1658",
    "title": "Creating a new Ontology: a Modular Approach",
    "abstract": "  Creating a new Ontology: a Modular Approach\n"
  },
  {
    "id": "1012.1667",
    "title": "A semantic approach for the requirement-driven discovery of web services\n  in the Life Sciences",
    "abstract": "  Research in the Life Sciences depends on the integration of large,\ndistributed and heterogeneous data sources and web services. The discovery of\nwhich of these resources are the most appropriate to solve a given task is a\ncomplex research question, since there is a large amount of plausible\ncandidates and there is little, mostly unstructured, metadata to be able to\ndecide among them.We contribute a semi-automatic approach,based on semantic\ntechniques, to assist researchers in the discovery of the most appropriate web\nservices to full a set of given requirements.\n"
  },
  {
    "id": "1012.1743",
    "title": "Scientific Collaborations: principles of WikiBridge Design",
    "abstract": "  Semantic wikis, wikis enhanced with Semantic Web technologies, are\nappropriate systems for community-authored knowledge models. They are\nparticularly suitable for scientific collaboration. This paper details the\ndesign principles ofWikiBridge, a semantic wiki.\n"
  },
  {
    "id": "1012.1745",
    "title": "Populous: A tool for populating ontology templates",
    "abstract": "  We present Populous, a tool for gathering content with which to populate an\nontology. Domain experts need to add content, that is often repetitive in its\nform, but without having to tackle the underlying ontological representation.\nPopulous presents users with a table based form in which columns are\nconstrained to take values from particular ontologies; the user can select a\nconcept from an ontology via its meaningful label to give a value for a given\nentity attribute. Populated tables are mapped to patterns that can then be used\nto automatically generate the ontology's content. Populous's contribution is in\nthe knowledge gathering stage of ontology development. It separates knowledge\ngathering from the conceptualisation and also separates the user from the\nstandard ontology authoring environments. As a result, Populous can allow\nknowledge to be gathered in a straight-forward manner that can then be used to\ndo mass production of ontology content.\n"
  },
  {
    "id": "1012.1899",
    "title": "Querying Biomedical Ontologies in Natural Language using Answer Set",
    "abstract": "  In this work, we develop an intelligent user interface that allows users to\nenter biomedical queries in a natural language, and that presents the answers\n(possibly with explanations if requested) in a natural language. We develop a\nrule layer over biomedical ontologies and databases, and use automated\nreasoners to answer queries considering relevant parts of the rule layer.\n"
  },
  {
    "id": "1012.2148",
    "title": "Bisimulations for fuzzy transition systems",
    "abstract": "  There has been a long history of using fuzzy language equivalence to compare\nthe behavior of fuzzy systems, but the comparison at this level is too coarse.\nRecently, a finer behavioral measure, bisimulation, has been introduced to\nfuzzy finite automata. However, the results obtained are applicable only to\nfinite-state systems. In this paper, we consider bisimulation for general fuzzy\nsystems which may be infinite-state or infinite-event, by modeling them as\nfuzzy transition systems. To help understand and check bisimulation, we\ncharacterize it in three ways by enumerating whole transitions, comparing\nindividual transitions, and using a monotonic function. In addition, we address\ncomposition operations, subsystems, quotients, and homomorphisms of fuzzy\ntransition systems and discuss their properties connected with bisimulation.\nThe results presented here are useful for comparing the behavior of general\nfuzzy systems. In particular, this makes it possible to relate an infinite\nfuzzy system to a finite one, which is easier to analyze, with the same\nbehavior.\n"
  },
  {
    "id": "1012.2162",
    "title": "Nondeterministic fuzzy automata",
    "abstract": "  Fuzzy automata have long been accepted as a generalization of\nnondeterministic finite automata. A closer examination, however, shows that the\nfundamental property---nondeterminism---in nondeterministic finite automata has\nnot been well embodied in the generalization. In this paper, we introduce\nnondeterministic fuzzy automata with or without $\\el$-moves and fuzzy languages\nrecognized by them. Furthermore, we prove that (deterministic) fuzzy automata,\nnondeterministic fuzzy automata, and nondeterministic fuzzy automata with\n$\\el$-moves are all equivalent in the sense that they recognize the same class\nof fuzzy languages.\n"
  },
  {
    "id": "1012.2789",
    "title": "Experimental Comparison of Representation Methods and Distance Measures\n  for Time Series Data",
    "abstract": "  The previous decade has brought a remarkable increase of the interest in\napplications that deal with querying and mining of time series data. Many of\nthe research efforts in this context have focused on introducing new\nrepresentation methods for dimensionality reduction or novel similarity\nmeasures for the underlying data. In the vast majority of cases, each\nindividual work introducing a particular method has made specific claims and,\naside from the occasional theoretical justifications, provided quantitative\nexperimental observations. However, for the most part, the comparative aspects\nof these experiments were too narrowly focused on demonstrating the benefits of\nthe proposed methods over some of the previously introduced ones. In order to\nprovide a comprehensive validation, we conducted an extensive experimental\nstudy re-implementing eight different time series representations and nine\nsimilarity measures and their variants, and testing their effectiveness on\nthirty-eight time series data sets from a wide variety of application domains.\nIn this paper, we give an overview of these different techniques and present\nour comparative experimental findings regarding their effectiveness. In\naddition to providing a unified validation of some of the existing\nachievements, our experiments also indicate that, in some cases, certain claims\nin the literature may be unduly optimistic.\n"
  },
  {
    "id": "1012.3280",
    "title": "A new Recommender system based on target tracking: a Kalman Filter\n  approach",
    "abstract": "  In this paper, we propose a new approach for recommender systems based on\ntarget tracking by Kalman filtering. We assume that users and their seen\nresources are vectors in the multidimensional space of the categories of the\nresources. Knowing this space, we propose an algorithm based on a Kalman filter\nto track users and to predict the best prediction of their future position in\nthe recommendation space.\n"
  },
  {
    "id": "1012.3312",
    "title": "Dynamic Capitalization and Visualization Strategy in Collaborative\n  Knowledge Management System for EI Process",
    "abstract": "  Knowledge is attributed to human whose problem-solving behavior is subjective\nand complex. In today's knowledge economy, the need to manage knowledge\nproduced by a community of actors cannot be overemphasized. This is due to the\nfact that actors possess some level of tacit knowledge which is generally\ndifficult to articulate. Problem-solving requires searching and sharing of\nknowledge among a group of actors in a particular context. Knowledge expressed\nwithin the context of a problem resolution must be capitalized for future\nreuse. In this paper, an approach that permits dynamic capitalization of\nrelevant and reliable actors' knowledge in solving decision problem following\nEconomic Intelligence process is proposed. Knowledge annotation method and\ntemporal attributes are used for handling the complexity in the communication\namong actors and in contextualizing expressed knowledge. A prototype is built\nto demonstrate the functionalities of a collaborative Knowledge Management\nsystem based on this approach. It is tested with sample cases and the result\nshowed that dynamic capitalization leads to knowledge validation hence\nincreasing reliability of captured knowledge for reuse. The system can be\nadapted to various domains\n"
  },
  {
    "id": "1012.3336",
    "title": "Dynamic Knowledge Capitalization through Annotation among Economic\n  Intelligence Actors in a Collaborative Environment",
    "abstract": "  The shift from industrial economy to knowledge economy in today's world has\nrevolutionalized strategic planning in organizations as well as their problem\nsolving approaches. The point of focus today is knowledge and service\nproduction with more emphasis been laid on knowledge capital. Many\norganizations are investing on tools that facilitate knowledge sharing among\ntheir employees and they are as well promoting and encouraging collaboration\namong their staff in order to build the organization's knowledge capital with\nthe ultimate goal of creating a lasting competitive advantage for their\norganizations. One of the current leading approaches used for solving\norganization's decision problem is the Economic Intelligence (EI) approach\nwhich involves interactions among various actors called EI actors. These actors\ncollaborate to ensure the overall success of the decision problem solving\nprocess. In the course of the collaboration, the actors express knowledge which\ncould be capitalized for future reuse. In this paper, we propose in the first\nplace, an annotation model for knowledge elicitation among EI actors. Because\nof the need to build a knowledge capital, we also propose a dynamic knowledge\ncapitalisation approach for managing knowledge produced by the actors. Finally,\nthe need to manage the interactions and the interdependencies among\ncollaborating EI actors, led to our third proposition which constitute an\nawareness mechanism for group work management.\n"
  },
  {
    "id": "1012.3410",
    "title": "Descriptive-complexity based distance for fuzzy sets",
    "abstract": "  A new distance function dist(A,B) for fuzzy sets A and B is introduced. It is\nbased on the descriptive complexity, i.e., the number of bits (on average) that\nare needed to describe an element in the symmetric difference of the two sets.\nThe distance gives the amount of additional information needed to describe any\none of the two sets given the other. We prove its mathematical properties and\nperform pattern clustering on data based on this distance.\n"
  },
  {
    "id": "1012.4046",
    "title": "Artificial Intelligence in Reverse Supply Chain Management: The State of\n  the Art",
    "abstract": "  Product take-back legislation forces manufacturers to bear the costs of\ncollection and disposal of products that have reached the end of their useful\nlives. In order to reduce these costs, manufacturers can consider reuse,\nremanufacturing and/or recycling of components as an alternative to disposal.\nThe implementation of such alternatives usually requires an appropriate reverse\nsupply chain management. With the concepts of reverse supply chain are gaining\npopularity in practice, the use of artificial intelligence approaches in these\nareas is also becoming popular. As a result, the purpose of this paper is to\ngive an overview of the recent publications concerning the application of\nartificial intelligence techniques to reverse supply chain with emphasis on\ncertain types of product returns.\n"
  },
  {
    "id": "1012.4776",
    "title": "Automatic Estimation of the Exposure to Lateral Collision in Signalized\n  Intersections using Video Sensors",
    "abstract": "  Intersections constitute one of the most dangerous elements in road systems.\nTraffic signals remain the most common way to control traffic at high-volume\nintersections and offer many opportunities to apply intelligent transportation\nsystems to make traffic more efficient and safe. This paper describes an\nautomated method to estimate the temporal exposure of road users crossing the\nconflict zone to lateral collision with road users originating from a different\napproach. This component is part of a larger system relying on video sensors to\nprovide queue lengths and spatial occupancy that are used for real time traffic\ncontrol and monitoring. The method is evaluated on data collected during a real\nworld experiment.\n"
  },
  {
    "id": "1012.5585",
    "title": "Symmetry Breaking with Polynomial Delay",
    "abstract": "  A conservative class of constraint satisfaction problems CSPs is a class for\nwhich membership is preserved under arbitrary domain reductions. Many\nwell-known tractable classes of CSPs are conservative. It is well known that\nlexleader constraints may significantly reduce the number of solutions by\nexcluding symmetric solutions of CSPs. We show that adding certain lexleader\nconstraints to any instance of any conservative class of CSPs still allows us\nto find all solutions with a time which is polynomial between successive\nsolutions. The time is polynomial in the total size of the instance and the\nadditional lexleader constraints. It is well known that for complete symmetry\nbreaking one may need an exponential number of lexleader constraints. However,\nin practice, the number of additional lexleader constraints is typically\npolynomial number in the size of the instance. For polynomially many lexleader\nconstraints, we may in general not have complete symmetry breaking but\npolynomially many lexleader constraints may provide practically useful symmetry\nbreaking -- and they sometimes exclude super-exponentially many solutions. We\nprove that for any instance from a conservative class, the time between finding\nsuccessive solutions of the instance with polynomially many additional\nlexleader constraints is polynomial even in the size of the instance without\nlexleaderconstraints.\n"
  },
  {
    "id": "1012.5705",
    "title": "Looking for plausibility",
    "abstract": "  In the interpretation of experimental data, one is actually looking for\nplausible explanations. We look for a measure of plausibility, with which we\ncan compare different possible explanations, and which can be combined when\nthere are different sets of data. This is contrasted to the conventional\nmeasure for probabilities as well as to the proposed measure of possibilities.\nWe define what characteristics this measure of plausibility should have.\n  In getting to the conception of this measure, we explore the relation of\nplausibility to abductive reasoning, and to Bayesian probabilities. We also\ncompare with the Dempster-Schaefer theory of evidence, which also has its own\ndefinition for plausibility. Abduction can be associated with biconditionality\nin inference rules, and this provides a platform to relate to the\nCollins-Michalski theory of plausibility. Finally, using a formalism for wiring\nlogic onto Hopfield neural networks, we ask if this is relevant in obtaining\nthis measure.\n"
  },
  {
    "id": "1012.5815",
    "title": "SAPFOCS: a metaheuristic based approach to part family formation\n  problems in group technology",
    "abstract": "  This article deals with Part family formation problem which is believed to be\nmoderately complicated to be solved in polynomial time in the vicinity of Group\nTechnology (GT). In the past literature researchers investigated that the part\nfamily formation techniques are principally based on production flow analysis\n(PFA) which usually considers operational requirements, sequences and time.\nPart Coding Analysis (PCA) is merely considered in GT which is believed to be\nthe proficient method to identify the part families. PCA classifies parts by\nallotting them to different families based on their resemblances in: (1) design\ncharacteristics such as shape and size, and/or (2) manufacturing\ncharacteristics (machining requirements). A novel approach based on simulated\nannealing namely SAPFOCS is adopted in this study to develop effective part\nfamilies exploiting the PCA technique. Thereafter Taguchi's orthogonal design\nmethod is employed to solve the critical issues on the subject of parameters\nselection for the proposed metaheuristic algorithm. The adopted technique is\ntherefore tested on 5 different datasets of size 5 {\\times} 9 to 27 {\\times} 9\nand the obtained results are compared with C-Linkage clustering technique. The\nexperimental results reported that the proposed metaheuristic algorithm is\nextremely effective in terms of the quality of the solution obtained and has\noutperformed C-Linkage algorithm in most instances.\n"
  },
  {
    "id": "1012.5847",
    "title": "On Elementary Loops of Logic Programs",
    "abstract": "  Using the notion of an elementary loop, Gebser and Schaub refined the theorem\non loop formulas due to Lin and Zhao by considering loop formulas of elementary\nloops only. In this article, we reformulate their definition of an elementary\nloop, extend it to disjunctive programs, and study several properties of\nelementary loops, including how maximal elementary loops are related to minimal\nunfounded sets. The results provide useful insights into the stable model\nsemantics in terms of elementary loops. For a nondisjunctive program, using a\ngraph-theoretic characterization of an elementary loop, we show that the\nproblem of recognizing an elementary loop is tractable. On the other hand, we\nshow that the corresponding problem is {\\sf coNP}-complete for a disjunctive\nprogram. Based on the notion of an elementary loop, we present the class of\nHead-Elementary-loop-Free (HEF) programs, which strictly generalizes the class\nof Head-Cycle-Free (HCF) programs due to Ben-Eliyahu and Dechter. Like an HCF\nprogram, an HEF program can be turned into an equivalent nondisjunctive program\nin polynomial time by shifting head atoms into the body.\n"
  },
  {
    "id": "1012.5960",
    "title": "Extending Binary Qualitative Direction Calculi with a Granular Distance\n  Concept: Hidden Feature Attachment",
    "abstract": "  In this paper we introduce a method for extending binary qualitative\ndirection calculi with adjustable granularity like OPRAm or the star calculus\nwith a granular distance concept. This method is similar to the concept of\nextending points with an internal reference direction to get oriented points\nwhich are the basic entities in the OPRAm calculus. Even if the spatial objects\nare from a geometrical point of view infinitesimal small points locally\navailable reference measures are attached. In the case of OPRAm, a reference\ndirection is attached. The same principle works also with local reference\ndistances which are called elevations. The principle of attaching references\nfeatures to a point is called hidden feature attachment.\n"
  },
  {
    "id": "1012.6018",
    "title": "Learning a Representation of a Believable Virtual Character's\n  Environment with an Imitation Algorithm",
    "abstract": "  In video games, virtual characters' decision systems often use a simplified\nrepresentation of the world. To increase both their autonomy and believability\nwe want those characters to be able to learn this representation from human\nplayers. We propose to use a model called growing neural gas to learn by\nimitation the topology of the environment. The implementation of the model, the\nmodifications and the parameters we used are detailed. Then, the quality of the\nlearned representations and their evolution during the learning are studied\nusing different measures. Improvements for the growing neural gas to give more\ninformation to the character's model are given in the conclusion.\n"
  },
  {
    "id": "1101.2279",
    "title": "Planning with Partial Preference Models",
    "abstract": "  Current work in planning with preferences assume that the user's preference\nmodels are completely specified and aim to search for a single solution plan.\nIn many real-world planning scenarios, however, the user probably cannot\nprovide any information about her desired plans, or in some cases can only\nexpress partial preferences. In such situations, the planner has to present not\nonly one but a set of plans to the user, with the hope that some of them are\nsimilar to the plan she prefers. We first propose the usage of different\nmeasures to capture quality of plan sets that are suitable for such scenarios:\ndomain-independent distance measures defined based on plan elements (actions,\nstates, causal links) if no knowledge of the user's preferences is given, and\nthe Integrated Convex Preference measure in case the user's partial preference\nis provided. We then investigate various heuristic approaches to find set of\nplans according to these measures, and present empirical results demonstrating\nthe promise of our approach.\n"
  },
  {
    "id": "1101.2378",
    "title": "Extracting Features from Ratings: The Role of Factor Models",
    "abstract": "  Performing effective preference-based data retrieval requires detailed and\npreferentially meaningful structurized information about the current user as\nwell as the items under consideration. A common problem is that representations\nof items often only consist of mere technical attributes, which do not resemble\nhuman perception. This is particularly true for integral items such as movies\nor songs. It is often claimed that meaningful item features could be extracted\nfrom collaborative rating data, which is becoming available through social\nnetworking services. However, there is only anecdotal evidence supporting this\nclaim; but if it is true, the extracted information could very valuable for\npreference-based data retrieval. In this paper, we propose a methodology to\nsystematically check this common claim. We performed a preliminary\ninvestigation on a large collection of movie ratings and present initial\nevidence.\n"
  },
  {
    "id": "1101.3465",
    "title": "The \"psychological map of the brain\", as a personal information card\n  (file), - a project for the student of the 21st century",
    "abstract": "  We suggest a procedure that is relevant both to electronic performance and\nhuman psychology, so that the creative logic and the respect for human nature\nappear in a good agreement. The idea is to create an electronic card containing\nbasic information about a person's psychological behavior in order to make it\npossible to quickly decide about the suitability of one for another. This\n\"psychological electronics\" approach could be tested via student projects.\n"
  },
  {
    "id": "1101.4356",
    "title": "Meaning Negotiation as Inference",
    "abstract": "  Meaning negotiation (MN) is the general process with which agents reach an\nagreement about the meaning of a set of terms. Artificial Intelligence scholars\nhave dealt with the problem of MN by means of argumentations schemes, beliefs\nmerging and information fusion operators, and ontology alignment but the\nproposed approaches depend upon the number of participants. In this paper, we\ngive a general model of MN for an arbitrary number of agents, in which each\nparticipant discusses with the others her viewpoint by exhibiting it in an\nactual set of constraints on the meaning of the negotiated terms. We call this\npresentation of individual viewpoints an angle. The agents do not aim at\nforming a common viewpoint but, instead, at agreeing about an acceptable common\nangle. We analyze separately the process of MN by two agents (\\emph{bilateral}\nor \\emph{pairwise} MN) and by more than two agents (\\emph{multiparty} MN), and\nwe use game theoretic models to understand how the process develops in both\ncases: the models are Bargaining Game for bilateral MN and English Auction for\nmultiparty MN. We formalize the process of reaching such an agreement by giving\na deduction system that comprises of rules that are consistent and adequate for\nrepresenting MN.\n"
  },
  {
    "id": "1102.0079",
    "title": "Information-theoretic measures associated with rough set approximations",
    "abstract": "  Although some information-theoretic measures of uncertainty or granularity\nhave been proposed in rough set theory, these measures are only dependent on\nthe underlying partition and the cardinality of the universe, independent of\nthe lower and upper approximations. It seems somewhat unreasonable since the\nbasic idea of rough set theory aims at describing vague concepts by the lower\nand upper approximations. In this paper, we thus define new\ninformation-theoretic entropy and co-entropy functions associated to the\npartition and the approximations to measure the uncertainty and granularity of\nan approximation space. After introducing the novel notions of entropy and\nco-entropy, we then examine their properties. In particular, we discuss the\nrelationship of co-entropies between different universes. The theoretical\ndevelopment is accompanied by illustrative numerical examples.\n"
  },
  {
    "id": "1102.0714",
    "title": "An architecture for the evaluation of intelligent systems",
    "abstract": "  One of the main research areas in Artificial Intelligence is the coding of\nagents (programs) which are able to learn by themselves in any situation. This\nmeans that agents must be useful for purposes other than those they were\ncreated for, as, for example, playing chess. In this way we try to get closer\nto the pristine goal of Artificial Intelligence. One of the problems to decide\nwhether an agent is really intelligent or not is the measurement of its\nintelligence, since there is currently no way to measure it in a reliable way.\nThe purpose of this project is to create an interpreter that allows for the\nexecution of several environments, including those which are generated\nrandomly, so that an agent (a person or a program) can interact with them. Once\nthe interaction between the agent and the environment is over, the interpreter\nwill measure the intelligence of the agent according to the actions, states and\nrewards the agent has undergone inside the environment during the test. As a\nresult we will be able to measure agents' intelligence in any possible\nenvironment, and to make comparisons between several agents, in order to\ndetermine which of them is the most intelligent. In order to perform the tests,\nthe interpreter must be able to randomly generate environments that are really\nuseful to measure agents' intelligence, since not any randomly generated\nenvironment will serve that purpose.\n"
  },
  {
    "id": "1102.0831",
    "title": "Intelligent Semantic Web Search Engines: A Brief Survey",
    "abstract": "  The World Wide Web (WWW) allows the people to share the information (data)\nfrom the large database repositories globally. The amount of information grows\nbillions of databases. We need to search the information will specialize tools\nknown generically search engine. There are many of search engines available\ntoday, retrieving meaningful information is difficult. However to overcome this\nproblem in search engines to retrieve meaningful information intelligently,\nsemantic web technologies are playing a major role. In this paper we present\nsurvey on the search engine generations and the role of search engines in\nintelligent web and semantic search technologies.\n"
  },
  {
    "id": "1102.2670",
    "title": "Online Least Squares Estimation with Self-Normalized Processes: An\n  Application to Bandit Problems",
    "abstract": "  The analysis of online least squares estimation is at the heart of many\nstochastic sequential decision making problems. We employ tools from the\nself-normalized processes to provide a simple and self-contained proof of a\ntail bound of a vector-valued martingale. We use the bound to construct a new\ntighter confidence sets for the least squares estimate.\n  We apply the confidence sets to several online decision problems, such as the\nmulti-armed and the linearly parametrized bandit problems. The confidence sets\nare potentially applicable to other problems such as sleeping bandits,\ngeneralized linear bandits, and other linear control problems.\n  We improve the regret bound of the Upper Confidence Bound (UCB) algorithm of\nAuer et al. (2002) and show that its regret is with high-probability a problem\ndependent constant. In the case of linear bandits (Dani et al., 2008), we\nimprove the problem dependent bound in the dimension and number of time steps.\nFurthermore, as opposed to the previous result, we prove that our bound holds\nfor small sample sizes, and at the same time the worst case bound is improved\nby a logarithmic factor and the constant is improved.\n"
  },
  {
    "id": "1102.2984",
    "title": "Hybrid Model for Solving Multi-Objective Problems Using Evolutionary\n  Algorithm and Tabu Search",
    "abstract": "  This paper presents a new multi-objective hybrid model that makes cooperation\nbetween the strength of research of neighborhood methods presented by the tabu\nsearch (TS) and the important exploration capacity of evolutionary algorithm.\nThis model was implemented and tested in benchmark functions (ZDT1, ZDT2, and\nZDT3), using a network of computers.\n"
  },
  {
    "id": "1102.4924",
    "title": "New Worst-Case Upper Bound for #XSAT",
    "abstract": "  An algorithm running in O(1.1995n) is presented for counting models for exact\nsatisfiability formulae(#XSAT). This is faster than the previously best\nalgorithm which runs in O(1.2190n). In order to improve the efficiency of the\nalgorithm, a new principle, i.e. the common literals principle, is addressed to\nsimplify formulae. This allows us to eliminate more common literals. In\naddition, we firstly inject the resolution principles into solving #XSAT\nproblem, and therefore this further improves the efficiency of the algorithm.\n"
  },
  {
    "id": "1102.5385",
    "title": "Back and Forth Between Rules and SE-Models (Extended Version)",
    "abstract": "  Rules in logic programming encode information about mutual interdependencies\nbetween literals that is not captured by any of the commonly used semantics.\nThis information becomes essential as soon as a program needs to be modified or\nfurther manipulated.\n  We argue that, in these cases, a program should not be viewed solely as the\nset of its models. Instead, it should be viewed and manipulated as the set of\nsets of models of each rule inside it. With this in mind, we investigate and\nhighlight relations between the SE-model semantics and individual rules. We\nidentify a set of representatives of rule equivalence classes induced by\nSE-models, and so pinpoint the exact expressivity of this semantics with\nrespect to a single rule. We also characterise the class of sets of\nSE-interpretations representable by a single rule. Finally, we discuss the\nintroduction of two notions of equivalence, both stronger than strong\nequivalence [1] and weaker than strong update equivalence [2], which seem more\nsuitable whenever the dependency information found in rules is of interest.\n"
  },
  {
    "id": "1102.5635",
    "title": "Practical inventory routing: A problem definition and an optimization\n  method",
    "abstract": "  The global objective of this work is to provide practical optimization\nmethods to companies involved in inventory routing problems, taking into\naccount this new type of data. Also, companies are sometimes not able to deal\nwith changing plans every period and would like to adopt regular structures for\nserving customers.\n"
  },
  {
    "id": "1103.0127",
    "title": "Fuzzy Approach to Critical Bus Ranking under Normal and Line Outage\n  Contingencies",
    "abstract": "  Identification of critical or weak buses for a given operating condition is\nan important task in the load dispatch centre. It has become more vital in view\nof the threat of voltage instability leading to voltage collapse. This paper\npresents a fuzzy approach for ranking critical buses in a power system under\nnormal and network contingencies based on Line Flow index and voltage profiles\nat load buses. The Line Flow index determines the maximum load that is possible\nto be connected to a bus in order to maintain stability before the system\nreaches its bifurcation point. Line Flow index (LF index) along with voltage\nprofiles at the load buses are represented in Fuzzy Set notation. Further they\nare evaluated using fuzzy rules to compute Criticality Index. Based on this\nindex, critical buses are ranked. The bus with highest rank is the weakest bus\nas it can withstand a small amount of load before causing voltage collapse. The\nproposed method is tested on Five Bus Test System.\n"
  },
  {
    "id": "1103.0632",
    "title": "An Agent Based Architecture (Using Planning) for Dynamic and Semantic\n  Web Services Composition in an EBXML Context",
    "abstract": "  The process-based semantic composition of Web Services is gaining a\nconsiderable momentum as an approach for the effective integration of\ndistributed, heterogeneous, and autonomous applications. To compose Web\nServices semantically, we need an ontology. There are several ways of inserting\nsemantics in Web Services. One of them consists of using description languages\nlike OWL-S. In this paper, we introduce our work which consists in the\nproposition of a new model and the use of semantic matching technology for\nsemantic and dynamic composition of ebXML business processes.\n"
  },
  {
    "id": "1103.0697",
    "title": "A Wiki for Business Rules in Open Vocabulary, Executable English",
    "abstract": "  The problem of business-IT alignment is of widespread economic concern.\n  As one way of addressing the problem, this paper describes an online system\nthat functions as a kind of Wiki -- one that supports the collaborative writing\nand running of business and scientific applications, as rules in open\nvocabulary, executable English, using a browser.\n  Since the rules are in English, they are indexed by Google and other search\nengines. This is useful when looking for rules for a task that one has in mind.\n  The design of the system integrates the semantics of data, with a semantics\nof an inference method, and also with the meanings of English sentences. As\nsuch, the system has functionality that may be useful for the Rules, Logic,\nProof and Trust requirements of the Semantic Web.\n  The system accepts rules, and small numbers of facts, typed or copy-pasted\ndirectly into a browser. One can then run the rules, again using a browser. For\nlarger amounts of data, the system uses information in the rules to\nautomatically generate and run SQL over networked databases. From a few highly\ndeclarative rules, the system typically generates SQL that would be too\ncomplicated to write reliably by hand. However, the system can explain its\nresults in step-by-step hypertexted English, at the business or scientific\nlevel\n  As befits a Wiki, shared use of the system is free.\n"
  },
  {
    "id": "1103.1003",
    "title": "Teraflop-scale Incremental Machine Learning",
    "abstract": "  We propose a long-term memory design for artificial general intelligence\nbased on Solomonoff's incremental machine learning methods. We use R5RS Scheme\nand its standard library with a few omissions as the reference machine. We\nintroduce a Levin Search variant based on Stochastic Context Free Grammar\ntogether with four synergistic update algorithms that use the same grammar as a\nguiding probability distribution of programs. The update algorithms include\nadjusting production probabilities, re-using previous solutions, learning\nprogramming idioms and discovery of frequent subprograms. Experiments with two\ntraining sequences demonstrate that our approach to incremental learning is\neffective.\n"
  },
  {
    "id": "1103.1157",
    "title": "GRASP and path-relinking for Coalition Structure Generation",
    "abstract": "  In Artificial Intelligence with Coalition Structure Generation (CSG) one\nrefers to those cooperative complex problems that require to find an optimal\npartition, maximising a social welfare, of a set of entities involved in a\nsystem into exhaustive and disjoint coalitions. The solution of the CSG problem\nfinds applications in many fields such as Machine Learning (covering machines,\nclustering), Data Mining (decision tree, discretization), Graph Theory, Natural\nLanguage Processing (aggregation), Semantic Web (service composition), and\nBioinformatics. The problem of finding the optimal coalition structure is\nNP-complete. In this paper we present a greedy adaptive search procedure\n(GRASP) with path-relinking to efficiently search the space of coalition\nstructures. Experiments and comparisons to other algorithms prove the validity\nof the proposed method in solving this hard combinatorial problem.\n"
  },
  {
    "id": "1103.1205",
    "title": "A Directional Feature with Energy based Offline Signature Verification\n  Network",
    "abstract": "  Signature used as a biometric is implemented in various systems as well as\nevery signature signed by each person is distinct at the same time. So, it is\nvery important to have a computerized signature verification system. In offline\nsignature verification system dynamic features are not available obviously, but\none can use a signature as an image and apply image processing techniques to\nmake an effective offline signature verification system. Author proposes a\nintelligent network used directional feature and energy density both as inputs\nto the same network and classifies the signature. Neural network is used as a\nclassifier for this system. The results are compared with both the very basic\nenergy density method and a simple directional feature method of offline\nsignature verification system and this proposed new network is found very\neffective as compared to the above two methods, specially for less number of\ntraining samples, which can be implemented practically.\n"
  },
  {
    "id": "1103.1711",
    "title": "Planning Graph Heuristics for Belief Space Search",
    "abstract": "  Some recent works in conditional planning have proposed reachability\nheuristics to improve planner scalability, but many lack a formal description\nof the properties of their distance estimates. To place previous work in\ncontext and extend work on heuristics for conditional planning, we provide a\nformal basis for distance estimates between belief states. We give a definition\nfor the distance between belief states that relies on aggregating underlying\nstate distance measures. We give several techniques to aggregate state\ndistances and their associated properties. Many existing heuristics exhibit a\nsubset of the properties, but in order to provide a standardized comparison we\npresent several generalizations of planning graph heuristics that are used in a\nsingle planner. We compliment our belief state distance estimate framework by\nalso investigating efficient planning graph data structures that incorporate\nBDDs to compute the most effective heuristics.\n  We developed two planners to serve as test-beds for our investigation. The\nfirst, CAltAlt, is a conformant regression planner that uses A* search. The\nsecond, POND, is a conditional progression planner that uses AO* search. We\nshow the relative effectiveness of our heuristic techniques within these\nplanners. We also compare the performance of these planners with several state\nof the art approaches in conditional planning.\n"
  },
  {
    "id": "1103.2091",
    "title": "An Artificial Immune System Model for Multi-Agents Resource Sharing in\n  Distributed Environments",
    "abstract": "  Natural Immune system plays a vital role in the survival of the all living\nbeing. It provides a mechanism to defend itself from external predates making\nit consistent systems, capable of adapting itself for survival incase of\nchanges. The human immune system has motivated scientists and engineers for\nfinding powerful information processing algorithms that has solved complex\nengineering tasks. This paper explores one of the various possibilities for\nsolving problem in a Multiagent scenario wherein multiple robots are deployed\nto achieve a goal collectively. The final goal is dependent on the performance\nof individual robot and its survival without having to lose its energy beyond a\npredetermined threshold value by deploying an evolutionary computational\ntechnique otherwise called the artificial immune system that imitates the\nbiological immune system.\n"
  },
  {
    "id": "1103.2342",
    "title": "SPPAM - Statistical PreProcessing AlgorithM",
    "abstract": "  Most machine learning tools work with a single table where each row is an\ninstance and each column is an attribute. Each cell of the table contains an\nattribute value for an instance. This representation prevents one important\nform of learning, which is, classification based on groups of correlated\nrecords, such as multiple exams of a single patient, internet customer\npreferences, weather forecast or prediction of sea conditions for a given day.\nTo some extent, relational learning methods, such as inductive logic\nprogramming, can capture this correlation through the use of intensional\npredicates added to the background knowledge. In this work, we propose SPPAM,\nan algorithm that aggregates past observations in one single record. We show\nthat applying SPPAM to the original correlated data, before the learning task,\ncan produce classifiers that are better than the ones trained using all\nrecords.\n"
  },
  {
    "id": "1103.2376",
    "title": "Language, Emotions, and Cultures: Emotional Sapir-Whorf Hypothesis",
    "abstract": "  An emotional version of Sapir-Whorf hypothesis suggests that differences in\nlanguage emotionalities influence differences among cultures no less than\nconceptual differences. Conceptual contents of languages and cultures to\nsignificant extent are determined by words and their semantic differences;\nthese could be borrowed among languages and exchanged among cultures. Emotional\ndifferences, as suggested in the paper, are related to grammar and mostly\ncannot be borrowed. Conceptual and emotional mechanisms of languages are\nconsidered here along with their functions in the mind and cultural evolution.\nA fundamental contradiction in human mind is considered: language evolution\nrequires reduced emotionality, but \"too low\" emotionality makes language\n\"irrelevant to life,\" disconnected from sensory-motor experience. Neural\nmechanisms of these processes are suggested as well as their mathematical\nmodels: the knowledge instinct, the language instinct, the dual model\nconnecting language and cognition, dynamic logic, neural modeling fields.\nMathematical results are related to cognitive science, linguistics, and\npsychology. Experimental evidence and theoretical arguments are discussed.\nApproximate equations for evolution of human minds and cultures are obtained.\nTheir solutions identify three types of cultures: \"conceptual\"-pragmatic\ncultures, in which emotionality of language is reduced and differentiation\novertakes synthesis resulting in fast evolution at the price of uncertainty of\nvalues, self doubts, and internal crises; \"traditional-emotional\" cultures\nwhere differentiation lags behind synthesis, resulting in cultural stability at\nthe price of stagnation; and \"multi-cultural\" societies combining fast cultural\nevolution and stability. Unsolved problems and future theoretical and\nexperimental directions are discussed.\n"
  },
  {
    "id": "1103.3123",
    "title": "Reduced Ordered Binary Decision Diagram with Implied Literals: A New\n  knowledge Compilation Approach",
    "abstract": "  Knowledge compilation is an approach to tackle the computational\nintractability of general reasoning problems. According to this approach,\nknowledge bases are converted off-line into a target compilation language which\nis tractable for on-line querying. Reduced ordered binary decision diagram\n(ROBDD) is one of the most influential target languages. We generalize ROBDD by\nassociating some implied literals in each node and the new language is called\nreduced ordered binary decision diagram with implied literals (ROBDD-L). Then\nwe discuss a kind of subsets of ROBDD-L called ROBDD-i with precisely i implied\nliterals (0 \\leq i \\leq \\infty). In particular, ROBDD-0 is isomorphic to ROBDD;\nROBDD-\\infty requires that each node should be associated by the implied\nliterals as many as possible. We show that ROBDD-i has uniqueness over some\nspecific variables order, and ROBDD-\\infty is the most succinct subset in\nROBDD-L and can meet most of the querying requirements involved in the\nknowledge compilation map. Finally, we propose an ROBDD-i compilation algorithm\nfor any i and a ROBDD-\\infty compilation algorithm. Based on them, we implement\na ROBDD-L package called BDDjLu and then get some conclusions from preliminary\nexperimental results: ROBDD-\\infty is obviously smaller than ROBDD for all\nbenchmarks; ROBDD-\\infty is smaller than the d-DNNF the benchmarks whose\ncompilation results are relatively small; it seems that it is better to\ntransform ROBDDs-\\infty into FBDDs and ROBDDs rather than straight compile the\nbenchmarks.\n"
  },
  {
    "id": "1103.3223",
    "title": "Using Soft Computer Techniques on Smart Devices for Monitoring Chronic\n  Diseases: the CHRONIOUS case",
    "abstract": "  CHRONIOUS is an Open, Ubiquitous and Adaptive Chronic Disease Management\nPlatform for Chronic Obstructive Pulmonary Disease(COPD) Chronic Kidney Disease\n(CKD) and Renal Insufficiency. It consists of several modules: an ontology\nbased literature search engine, a rule based decision support system, remote\nsensors interacting with lifestyle interfaces (PDA, monitor touchscreen) and a\nmachine learning module. All these modules interact each other to allow the\nmonitoring of two types of chronic diseases and to help clinician in taking\ndecision for cure purpose. This paper illustrates how some machine learning\nalgorithms and a rule based decision support system can be used in smart\ndevices, to monitor chronic patient. We will analyse how a set of machine\nlearning algorithms can be used in smart devices to alert the clinician in case\nof a patient health condition worsening trend.\n"
  },
  {
    "id": "1103.3240",
    "title": "Decentralized Constraint Satisfaction",
    "abstract": "  We show that several important resource allocation problems in wireless\nnetworks fit within the common framework of Constraint Satisfaction Problems\n(CSPs). Inspired by the requirements of these applications, where variables are\nlocated at distinct network devices that may not be able to communicate but may\ninterfere, we define natural criteria that a CSP solver must possess in order\nto be practical. We term these algorithms decentralized CSP solvers. The best\nknown CSP solvers were designed for centralized problems and do not meet these\ncriteria. We introduce a stochastic decentralized CSP solver and prove that it\nwill find a solution in almost surely finite time, should one exist, also\nshowing it has many practically desirable properties. We benchmark the\nalgorithm's performance on a well-studied class of CSPs, random k-SAT,\nillustrating that the time the algorithm takes to find a satisfying assignment\nis competitive with stochastic centralized solvers on problems with order a\nthousand variables despite its decentralized nature. We demonstrate the\nsolver's practical utility for the problems that motivated its introduction by\nusing it to find a non-interfering channel allocation for a network formed from\ndata from downtown Manhattan.\n"
  },
  {
    "id": "1103.3417",
    "title": "Finding Shortest Path for Developed Cognitive Map Using Medial Axis",
    "abstract": "  this paper presents an enhancement of the medial axis algorithm to be used\nfor finding the optimal shortest path for developed cognitive map. The\ncognitive map has been developed, based on the architectural blueprint maps.\nThe idea for using the medial-axis is to find main path central pixels; each\ncenter pixel represents the center distance between two side boarder pixels.\nThe need for these pixels in the algorithm comes from the need of building a\nnetwork of nodes for the path, where each node represents a turning in the real\nworld (left, right, critical left, critical right...). The algorithm also\nignores from finding the center pixels paths that are too small for intelligent\nrobot navigation. The Idea of this algorithm is to find the possible shortest\npath between start and end points. The goal of this research is to extract a\nsimple, robust representation of the shape of the cognitive map together with\nthe optimal shortest path between start and end points. The intelligent robot\nwill use this algorithm in order to decrease the time that is needed for\nsweeping the targeted building.\n"
  },
  {
    "id": "1103.3420",
    "title": "Extraction of handwritten areas from colored image of bank checks by an\n  hybrid method",
    "abstract": "  One of the first step in the realization of an automatic system of check\nrecognition is the extraction of the handwritten area. We propose in this paper\nan hybrid method to extract these areas. This method is based on digit\nrecognition by Fourier descriptors and different steps of colored image\nprocessing . It requires the bank recognition of its code which is located in\nthe check marking band as well as the handwritten color recognition by the\nmethod of difference of histograms. The areas extraction is then carried out by\nthe use of some mathematical morphology tools.\n"
  },
  {
    "id": "1103.3687",
    "title": "Cost Based Satisficing Search Considered Harmful",
    "abstract": "  Recently, several researchers have found that cost-based satisficing search\nwith A* often runs into problems. Although some \"work arounds\" have been\nproposed to ameliorate the problem, there has not been any concerted effort to\npinpoint its origin. In this paper, we argue that the origins can be traced\nback to the wide variance in action costs that is observed in most planning\ndomains. We show that such cost variance misleads A* search, and that this is\nno trifling detail or accidental phenomenon, but a systemic weakness of the\nvery concept of \"cost-based evaluation functions + systematic search +\ncombinatorial graphs\". We show that satisficing search with sized-based\nevaluation functions is largely immune to this problem.\n"
  },
  {
    "id": "1103.3745",
    "title": "The AllDifferent Constraint with Precedences",
    "abstract": "  We propose AllDiffPrecedence, a new global constraint that combines together\nan AllDifferent constraint with precedence constraints that strictly order\ngiven pairs of variables. We identify a number of applications for this global\nconstraint including instruction scheduling and symmetry breaking. We give an\nefficient propagation algorithm that enforces bounds consistency on this global\nconstraint. We show how to implement this propagator using a decomposition that\nextends the bounds consistency enforcing decomposition proposed for the\nAllDifferent constraint. Finally, we prove that enforcing domain consistency on\nthis global constraint is NP-hard in general.\n"
  },
  {
    "id": "1103.3949",
    "title": "A Goal-Directed Implementation of Query Answering for Hybrid MKNF\n  Knowledge Bases",
    "abstract": "  Ontologies and rules are usually loosely coupled in knowledge representation\nformalisms. In fact, ontologies use open-world reasoning while the leading\nsemantics for rules use non-monotonic, closed-world reasoning. One exception is\nthe tightly-coupled framework of Minimal Knowledge and Negation as Failure\n(MKNF), which allows statements about individuals to be jointly derived via\nentailment from an ontology and inferences from rules. Nonetheless, the\npractical usefulness of MKNF has not always been clear, although recent work\nhas formalized a general resolution-based method for querying MKNF when rules\nare taken to have the well-founded semantics, and the ontology is modeled by a\ngeneral oracle. That work leaves open what algorithms should be used to relate\nthe entailments of the ontology and the inferences of rules. In this paper we\nprovide such algorithms, and describe the implementation of a query-driven\nsystem, CDF-Rules, for hybrid knowledge bases combining both (non-monotonic)\nrules under the well-founded semantics and a (monotonic) ontology, represented\nby a CDF Type-1 (ALQ) theory. To appear in Theory and Practice of Logic\nProgramming (TPLP)\n"
  },
  {
    "id": "1103.3954",
    "title": "BoolVar/PB v1.0, a java library for translating pseudo-Boolean\n  constraints into CNF formulae",
    "abstract": "  BoolVar/PB is an open source java library dedicated to the translation of\npseudo-Boolean constraints into CNF formulae. Input constraints can be\ncategorized with tags. Several encoding schemes are implemented in a way that\neach input constraint can be translated using one or several encoders,\naccording to the related tags. The library can be easily extended by adding new\nencoders and / or new output formats.\n"
  },
  {
    "id": "1103.5034",
    "title": "On Understanding and Machine Understanding",
    "abstract": "  In the present paper, we try to propose a self-similar network theory for the\nbasic understanding. By extending the natural languages to a kind of so called\nidealy sufficient language, we can proceed a few steps to the investigation of\nthe language searching and the language understanding of AI.\n  Image understanding, and the familiarity of the brain to the surrounding\nenvironment are also discussed. Group effects are discussed by addressing the\nessense of the power of influences, and constructing the influence network of a\nsociety. We also give a discussion of inspirations.\n"
  },
  {
    "id": "1104.0843",
    "title": "Phase Transitions in Knowledge Compilation: an Experimental Study",
    "abstract": "  Phase transitions in many complex combinational problems have been widely\nstudied in the past decade. In this paper, we investigate phase transitions in\nthe knowledge compilation empirically, where DFA, OBDD and d-DNNF are chosen as\nthe target languages to compile random k-SAT instances. We perform intensive\nexperiments to analyze the sizes of compilation results and draw the following\nconclusions: there exists an easy-hard-easy pattern in compilations; the peak\npoint of sizes in the pattern is only related to the ratio of the number of\nclauses to that of variables when k is fixed, regardless of target languages;\nmost sizes of compilation results increase exponentially with the number of\nvariables growing, but there also exists a phase transition that separates a\npolynomial-increment region from the exponential-increment region; Moreover, we\nexplain why the phase transition in compilations occurs by analyzing\nmicrostructures of DFAs, and conclude that a kind of solution\ninterchangeability with more than 2 variables has a sharp transition near the\npeak point of the easy-hard-easy pattern, and thus it has a great impact on\nsizes of DFAs.\n"
  },
  {
    "id": "1104.1677",
    "title": "Automatic Vehicle Checking Agent (VCA)",
    "abstract": "  A definition of intelligence is given in terms of performance that can be\nquantitatively measured. In this study, we have presented a conceptual model of\nIntelligent Agent System for Automatic Vehicle Checking Agent (VCA). To achieve\nthis goal, we have introduced several kinds of agents that exhibit intelligent\nfeatures. These are the Management agent, internal agent, External Agent,\nWatcher agent and Report agent. Metrics and measurements are suggested for\nevaluating the performance of Automatic Vehicle Checking Agent (VCA). Calibrate\ndata and test facilities are suggested to facilitate the development of\nintelligent systems.\n"
  },
  {
    "id": "1104.1678",
    "title": "A Proposed Decision Support System/Expert System for Guiding Fresh\n  Students in Selecting a Faculty in Gomal University, Pakistan",
    "abstract": "  This paper presents the design and development of a proposed rule based\nDecision Support System that will help students in selecting the best suitable\nfaculty/major decision while taking admission in Gomal University, Dera Ismail\nKhan, Pakistan. The basic idea of our approach is to design a model for testing\nand measuring the student capabilities like intelligence, understanding,\ncomprehension, mathematical concepts plus his/her past academic record plus\nhis/her intelligence level, and applying the module results to a rule-based\ndecision support system to determine the compatibility of those capabilities\nwith the available faculties/majors in Gomal University. The result is shown as\na list of suggested faculties/majors with the student capabilities and\nabilities.\n"
  },
  {
    "id": "1104.1924",
    "title": "Rational Deployment of CSP Heuristics",
    "abstract": "  Heuristics are crucial tools in decreasing search effort in varied fields of\nAI. In order to be effective, a heuristic must be efficient to compute, as well\nas provide useful information to the search algorithm. However, some well-known\nheuristics which do well in reducing backtracking are so heavy that the gain of\ndeploying them in a search algorithm might be outweighed by their overhead.\n  We propose a rational metareasoning approach to decide when to deploy\nheuristics, using CSP backtracking search as a case study. In particular, a\nvalue of information approach is taken to adaptive deployment of solution-count\nestimation heuristics for value ordering. Empirical results show that indeed\nthe proposed mechanism successfully balances the tradeoff between decreasing\nbacktracking and heuristic computational overhead, resulting in a significant\noverall search time reduction.\n"
  },
  {
    "id": "1104.3250",
    "title": "Adding noise to the input of a model trained with a regularized\n  objective",
    "abstract": "  Regularization is a well studied problem in the context of neural networks.\nIt is usually used to improve the generalization performance when the number of\ninput samples is relatively small or heavily contaminated with noise. The\nregularization of a parametric model can be achieved in different manners some\nof which are early stopping (Morgan and Bourlard, 1990), weight decay, output\nsmoothing that are used to avoid overfitting during the training of the\nconsidered model. From a Bayesian point of view, many regularization techniques\ncorrespond to imposing certain prior distributions on model parameters (Krogh\nand Hertz, 1991). Using Bishop's approximation (Bishop, 1995) of the objective\nfunction when a restricted type of noise is added to the input of a parametric\nfunction, we derive the higher order terms of the Taylor expansion and analyze\nthe coefficients of the regularization terms induced by the noisy input. In\nparticular we study the effect of penalizing the Hessian of the mapping\nfunction with respect to the input in terms of generalization performance. We\nalso show how we can control independently this coefficient by explicitly\npenalizing the Jacobian of the mapping function on corrupted inputs.\n"
  },
  {
    "id": "1104.3927",
    "title": "Translation-based Constraint Answer Set Solving",
    "abstract": "  We solve constraint satisfaction problems through translation to answer set\nprogramming (ASP). Our reformulations have the property that unit-propagation\nin the ASP solver achieves well defined local consistency properties like arc,\nbound and range consistency. Experiments demonstrate the computational value of\nthis approach.\n"
  },
  {
    "id": "1104.4053",
    "title": "On the evolution of the instance level of DL-lite knowledge bases",
    "abstract": "  Recent papers address the issue of updating the instance level of knowledge\nbases expressed in Description Logic following a model-based approach. One of\nthe outcomes of these papers is that the result of updating a knowledge base K\nis generally not expressible in the Description Logic used to express K. In\nthis paper we introduce a formula-based approach to this problem, by revisiting\nsome research work on formula-based updates developed in the '80s, in\nparticular the WIDTIO (When In Doubt, Throw It Out) approach. We show that our\noperator enjoys desirable properties, including that both insertions and\ndeletions according to such operator can be expressed in the DL used for the\noriginal KB. Also, we present polynomial time algorithms for the evolution of\nthe instance level knowledge bases expressed in the most expressive Description\nLogics of the DL-lite family.\n"
  },
  {
    "id": "1104.4153",
    "title": "Learning invariant features through local space contraction",
    "abstract": "  We present in this paper a novel approach for training deterministic\nauto-encoders. We show that by adding a well chosen penalty term to the\nclassical reconstruction cost function, we can achieve results that equal or\nsurpass those attained by other regularized auto-encoders as well as denoising\nauto-encoders on a range of datasets. This penalty term corresponds to the\nFrobenius norm of the Jacobian matrix of the encoder activations with respect\nto the input. We show that this penalty term results in a localized space\ncontraction which in turn yields robust features on the activation layer.\nFurthermore, we show how this penalty term is related to both regularized\nauto-encoders and denoising encoders and how it can be seen as a link between\ndeterministic and non-deterministic auto-encoders. We find empirically that\nthis penalty helps to carve a representation that better captures the local\ndirections of variation dictated by the data, corresponding to a\nlower-dimensional non-linear manifold, while being more invariant to the vast\nmajority of directions orthogonal to the manifold. Finally, we show that by\nusing the learned features to initialize a MLP, we achieve state of the art\nclassification error on a range of datasets, surpassing other methods of\npre-training.\n"
  },
  {
    "id": "1104.4290",
    "title": "Algorithms and Complexity Results for Persuasive Argumentation",
    "abstract": "  The study of arguments as abstract entities and their interaction as\nintroduced by Dung (Artificial Intelligence 177, 1995) has become one of the\nmost active research branches within Artificial Intelligence and Reasoning. A\nmain issue for abstract argumentation systems is the selection of acceptable\nsets of arguments. Value-based argumentation, as introduced by Bench-Capon (J.\nLogic Comput. 13, 2003), extends Dung's framework. It takes into account the\nrelative strength of arguments with respect to some ranking representing an\naudience: an argument is subjectively accepted if it is accepted with respect\nto some audience, it is objectively accepted if it is accepted with respect to\nall audiences. Deciding whether an argument is subjectively or objectively\naccepted, respectively, are computationally intractable problems. In fact, the\nproblems remain intractable under structural restrictions that render the main\ncomputational problems for non-value-based argumentation systems tractable. In\nthis paper we identify nontrivial classes of value-based argumentation systems\nfor which the acceptance problems are polynomial-time tractable. The classes\nare defined by means of structural restrictions in terms of the underlying\ngraphical structure of the value-based system. Furthermore we show that the\nacceptance problems are intractable for two classes of value-based systems that\nwhere conjectured to be tractable by Dunne (Artificial Intelligence 171, 2007).\n"
  },
  {
    "id": "1104.4910",
    "title": "Hybrid Tractable Classes of Binary Quantified Constraint Satisfaction\n  Problems",
    "abstract": "  In this paper, we investigate the hybrid tractability of binary Quantified\nConstraint Satisfaction Problems (QCSPs). First, a basic tractable class of\nbinary QCSPs is identified by using the broken-triangle property. In this\nclass, the variable ordering for the broken-triangle property must be same as\nthat in the prefix of the QCSP. Second, we break this restriction to allow that\nexistentially quantified variables can be shifted within or out of their\nblocks, and thus identify some novel tractable classes by introducing the\nbroken-angle property. Finally, we identify a more generalized tractable class,\ni.e., the min-of-max extendable class for QCSPs.\n"
  },
  {
    "id": "1104.5069",
    "title": "Synthesizing Robust Plans under Incomplete Domain Models",
    "abstract": "  Most current planners assume complete domain models and focus on generating\ncorrect plans. Unfortunately, domain modeling is a laborious and error-prone\ntask. While domain experts cannot guarantee completeness, often they are able\nto circumscribe the incompleteness of the model by providing annotations as to\nwhich parts of the domain model may be incomplete. In such cases, the goal\nshould be to generate plans that are robust with respect to any known\nincompleteness of the domain. In this paper, we first introduce annotations\nexpressing the knowledge of the domain incompleteness, and formalize the notion\nof plan robustness with respect to an incomplete domain model. We then propose\nan approach to compiling the problem of finding robust plans to the conformant\nprobabilistic planning problem. We present experimental results with\nProbabilistic-FF, a state-of-the-art planner, showing the promise of our\napproach.\n"
  },
  {
    "id": "1105.0288",
    "title": "Splitting and Updating Hybrid Knowledge Bases (Extended Version)",
    "abstract": "  Over the years, nonmonotonic rules have proven to be a very expressive and\nuseful knowledge representation paradigm. They have recently been used to\ncomplement the expressive power of Description Logics (DLs), leading to the\nstudy of integrative formal frameworks, generally referred to as hybrid\nknowledge bases, where both DL axioms and rules can be used to represent\nknowledge. The need to use these hybrid knowledge bases in dynamic domains has\ncalled for the development of update operators, which, given the substantially\ndifferent way Description Logics and rules are usually updated, has turned out\nto be an extremely difficult task.\n  In [SL10], a first step towards addressing this problem was taken, and an\nupdate operator for hybrid knowledge bases was proposed. Despite its\nsignificance -- not only for being the first update operator for hybrid\nknowledge bases in the literature, but also because it has some applications -\nthis operator was defined for a restricted class of problems where only the\nABox was allowed to change, which considerably diminished its applicability.\nMany applications that use hybrid knowledge bases in dynamic scenarios require\nboth DL axioms and rules to be updated.\n  In this paper, motivated by real world applications, we introduce an update\noperator for a large class of hybrid knowledge bases where both the DL\ncomponent as well as the rule component are allowed to dynamically change. We\nintroduce splitting sequences and splitting theorem for hybrid knowledge bases,\nuse them to define a modular update semantics, investigate its basic\nproperties, and illustrate its use on a realistic example about cargo imports.\n"
  },
  {
    "id": "1105.0650",
    "title": "Transition Systems for Model Generators - A Unifying Approach",
    "abstract": "  A fundamental task for propositional logic is to compute models of\npropositional formulas. Programs developed for this task are called\nsatisfiability solvers. We show that transition systems introduced by\nNieuwenhuis, Oliveras, and Tinelli to model and analyze satisfiability solvers\ncan be adapted for solvers developed for two other propositional formalisms:\nlogic programming under the answer-set semantics, and the logic PC(ID). We show\nthat in each case the task of computing models can be seen as \"satisfiability\nmodulo answer-set programming,\" where the goal is to find a model of a theory\nthat also is an answer set of a certain program. The unifying perspective we\ndevelop shows, in particular, that solvers CLASP and MINISATID are closely\nrelated despite being developed for different formalisms, one for answer-set\nprogramming and the latter for the logic PC(ID).\n"
  },
  {
    "id": "1105.0974",
    "title": "GANC: Greedy Agglomerative Normalized Cut",
    "abstract": "  This paper describes a graph clustering algorithm that aims to minimize the\nnormalized cut criterion and has a model order selection procedure. The\nperformance of the proposed algorithm is comparable to spectral approaches in\nterms of minimizing normalized cut. However, unlike spectral approaches, the\nproposed algorithm scales to graphs with millions of nodes and edges. The\nalgorithm consists of three components that are processed sequentially: a\ngreedy agglomerative hierarchical clustering procedure, model order selection,\nand a local refinement.\n  For a graph of n nodes and O(n) edges, the computational complexity of the\nalgorithm is O(n log^2 n), a major improvement over the O(n^3) complexity of\nspectral methods. Experiments are performed on real and synthetic networks to\ndemonstrate the scalability of the proposed approach, the effectiveness of the\nmodel order selection procedure, and the performance of the proposed algorithm\nin terms of minimizing the normalized cut metric.\n"
  },
  {
    "id": "1105.1247",
    "title": "Machine-Part cell formation through visual decipherable clustering of\n  Self Organizing Map",
    "abstract": "  Machine-part cell formation is used in cellular manufacturing in order to\nprocess a large variety, quality, lower work in process levels, reducing\nmanufacturing lead-time and customer response time while retaining flexibility\nfor new products. This paper presents a new and novel approach for obtaining\nmachine cells and part families. In the cellular manufacturing the fundamental\nproblem is the formation of part families and machine cells. The present paper\ndeals with the Self Organising Map (SOM) method an unsupervised learning\nalgorithm in Artificial Intelligence, and has been used as a visually\ndecipherable clustering tool of machine-part cell formation. The objective of\nthe paper is to cluster the binary machine-part matrix through visually\ndecipherable cluster of SOM color-coding and labelling via the SOM map nodes in\nsuch a way that the part families are processed in that machine cells. The\nUmatrix, component plane, principal component projection, scatter plot and\nhistogram of SOM have been reported in the present work for the successful\nvisualization of the machine-part cell formation. Computational result with the\nproposed algorithm on a set of group technology problems available in the\nliterature is also presented. The proposed SOM approach produced solutions with\na grouping efficacy that is at least as good as any results earlier reported in\nthe literature and improved the grouping efficacy for 70% of the problems and\nfound immensely useful to both industry practitioners and researchers.\n"
  },
  {
    "id": "1105.1436",
    "title": "Solving Rubik's Cube Using SAT Solvers",
    "abstract": "  Rubik's Cube is an easily-understood puzzle, which is originally called the\n\"magic cube\". It is a well-known planning problem, which has been studied for a\nlong time. Yet many simple properties remain unknown. This paper studies\nwhether modern SAT solvers are applicable to this puzzle. To our best\nknowledge, we are the first to translate Rubik's Cube to a SAT problem. To\nreduce the number of variables and clauses needed for the encoding, we replace\na naive approach of 6 Boolean variables to represent each color on each facelet\nwith a new approach of 3 or 2 Boolean variables. In order to be able to solve\nquickly Rubik's Cube, we replace the direct encoding of 18 turns with the layer\nencoding of 18-subtype turns based on 6-type turns. To speed up the solving\nfurther, we encode some properties of two-phase algorithm as an additional\nconstraint, and restrict some move sequences by adding some constraint clauses.\nUsing only efficient encoding cannot solve this puzzle. For this reason, we\nimprove the existing SAT solvers, and develop a new SAT solver based on\nPrecoSAT, though it is suited only for Rubik's Cube. The new SAT solver\nreplaces the lookahead solving strategy with an ALO (\\emph{at-least-one})\nsolving strategy, and decomposes the original problem into sub-problems. Each\nsub-problem is solved by PrecoSAT. The empirical results demonstrate both our\nSAT translation and new solving technique are efficient. Without the efficient\nSAT encoding and the new solving technique, Rubik's Cube will not be able to be\nsolved still by any SAT solver. Using the improved SAT solver, we can find\nalways a solution of length 20 in a reasonable time. Although our solver is\nslower than Kociemba's algorithm using lookup tables, but does not require a\nhuge lookup table.\n"
  },
  {
    "id": "1105.1929",
    "title": "The Hidden Web, XML and Semantic Web: A Scientific Data Management\n  Perspective",
    "abstract": "  The World Wide Web no longer consists just of HTML pages. Our work sheds\nlight on a number of trends on the Internet that go beyond simple Web pages.\nThe hidden Web provides a wealth of data in semi-structured form, accessible\nthrough Web forms and Web services. These services, as well as numerous other\napplications on the Web, commonly use XML, the eXtensible Markup Language. XML\nhas become the lingua franca of the Internet that allows customized markups to\nbe defined for specific domains. On top of XML, the Semantic Web grows as a\ncommon structured data source. In this work, we first explain each of these\ndevelopments in detail. Using real-world examples from scientific domains of\ngreat interest today, we then demonstrate how these new developments can assist\nthe managing, harvesting, and organization of data on the Web. On the way, we\nalso illustrate the current research avenues in these domains. We believe that\nthis effort would help bridge multiple database tracks, thereby attracting\nresearchers with a view to extend database technology.\n"
  },
  {
    "id": "1105.2902",
    "title": "A Multi-Purpose Scenario-based Simulator for Smart House Environments",
    "abstract": "  Developing smart house systems has been a great challenge for researchers and\nengineers in this area because of the high cost of implementation and\nevaluation process of these systems, while being very time consuming. Testing a\ndesigned smart house before actually building it is considered as an obstacle\ntowards an efficient smart house project. This is because of the variety of\nsensors, home appliances and devices available for a real smart environment. In\nthis paper, we present the design and implementation of a multi-purpose smart\nhouse simulation system for designing and simulating all aspects of a smart\nhouse environment. This simulator provides the ability to design the house plan\nand different virtual sensors and appliances in a two dimensional model of the\nvirtual house environment. This simulator can connect to any external smart\nhouse remote controlling system, providing evaluation capabilities to their\nsystem much easier than before. It also supports detailed adding of new\nemerging sensors and devices to help maintain its compatibility with future\nsimulation needs. Scenarios can also be defined for testing various possible\ncombinations of device states; so different criteria and variables can be\nsimply evaluated without the need of experimenting on a real environment.\n"
  },
  {
    "id": "1105.3486",
    "title": "Xapagy: a cognitive architecture for narrative reasoning",
    "abstract": "  We introduce the Xapagy cognitive architecture: a software system designed to\nperform narrative reasoning. The architecture has been designed from scratch to\nmodel and mimic the activities performed by humans when witnessing, reading,\nrecalling, narrating and talking about stories.\n"
  },
  {
    "id": "1105.3635",
    "title": "Probabilistic Inference from Arbitrary Uncertainty using Mixtures of\n  Factorized Generalized Gaussians",
    "abstract": "  This paper presents a general and efficient framework for probabilistic\ninference and learning from arbitrary uncertain information. It exploits the\ncalculation properties of finite mixture models, conjugate families and\nfactorization. Both the joint probability density of the variables and the\nlikelihood function of the (objective or subjective) observation are\napproximated by a special mixture model, in such a way that any desired\nconditional distribution can be directly obtained without numerical\nintegration. We have developed an extended version of the expectation\nmaximization (EM) algorithm to estimate the parameters of mixture models from\nuncertain training examples (indirect observations). As a consequence, any\npiece of exact or uncertain information about both input and output values is\nconsistently handled in the inference and learning stages. This ability,\nextremely useful in certain situations, is not found in most alternative\nmethods. The proposed framework is formally justified from standard\nprobabilistic principles and illustrative examples are provided in the fields\nof nonparametric pattern classification, nonlinear regression and pattern\ncompletion. Finally, experiments on a real application and comparative results\nover standard databases provide empirical evidence of the utility of the method\nin a wide range of applications.\n"
  },
  {
    "id": "1105.3821",
    "title": "Ontological Crises in Artificial Agents' Value Systems",
    "abstract": "  Decision-theoretic agents predict and evaluate the results of their actions\nusing a model, or ontology, of their environment. An agent's goal, or utility\nfunction, may also be specified in terms of the states of, or entities within,\nits ontology. If the agent may upgrade or replace its ontology, it faces a\ncrisis: the agent's original goal may not be well-defined with respect to its\nnew ontology. This crisis must be resolved before the agent can make plans\ntowards achieving its goals.\n  We discuss in this paper which sorts of agents will undergo ontological\ncrises and why we may want to create such agents. We present some concrete\nexamples, and argue that a well-defined procedure for resolving ontological\ncrises is needed. We point to some possible approaches to solving this problem,\nand evaluate these methods on our examples.\n"
  },
  {
    "id": "1105.3833",
    "title": "Typical models: minimizing false beliefs",
    "abstract": "  A knowledge system S describing a part of real world does in general not\ncontain complete information. Reasoning with incomplete information is prone to\nerrors since any belief derived from S may be false in the present state of the\nworld. A false belief may suggest wrong decisions and lead to harmful actions.\nSo an important goal is to make false beliefs as unlikely as possible. This\nwork introduces the notions of \"typical atoms\" and \"typical models\", and shows\nthat reasoning with typical models minimizes the expected number of false\nbeliefs over all ways of using incomplete information. Various properties of\ntypical models are studied, in particular, correctness and stability of beliefs\nsuggested by typical models, and their connection to oblivious reasoning.\n"
  },
  {
    "id": "1105.5440",
    "title": "The Ariadne's Clew Algorithm",
    "abstract": "  We present a new approach to path planning, called the \"Ariadne's clew\nalgorithm\". It is designed to find paths in high-dimensional continuous spaces\nand applies to robots with many degrees of freedom in static, as well as\ndynamic environments - ones where obstacles may move. The Ariadne's clew\nalgorithm comprises two sub-algorithms, called Search and Explore, applied in\nan interleaved manner. Explore builds a representation of the accessible space\nwhile Search looks for the target. Both are posed as optimization problems. We\ndescribe a real implementation of the algorithm to plan paths for a six degrees\nof freedom arm in a dynamic environment where another six degrees of freedom\narm is used as a moving obstacle. Experimental results show that a path is\nfound in about one second without any pre-processing.\n"
  },
  {
    "id": "1105.5441",
    "title": "Computational Aspects of Reordering Plans",
    "abstract": "  This article studies the problem of modifying the action ordering of a plan\nin order to optimise the plan according to various criteria. One of these\ncriteria is to make a plan less constrained and the other is to minimize its\nparallel execution time. Three candidate definitions are proposed for the first\nof these criteria, constituting a sequence of increasing optimality guarantees.\nTwo of these are based on deordering plans, which means that ordering relations\nmay only be removed, not added, while the third one uses reordering, where\narbitrary modifications to the ordering are allowed. It is shown that only the\nweakest one of the three criteria is tractable to achieve, the other two being\nNP-hard and even difficult to approximate. Similarly, optimising the parallel\nexecution time of a plan is studied both for deordering and reordering of\nplans. In the general case, both of these computations are NP-hard. However, it\nis shown that optimal deorderings can be computed in polynomial time for a\nclass of planning languages based on the notions of producers, consumers and\nthreats, which includes most of the commonly used planning languages. Computing\noptimal reorderings can potentially lead to even faster parallel executions,\nbut this problem remains NP-hard and difficult to approximate even under quite\nsevere restrictions.\n"
  },
  {
    "id": "1105.5442",
    "title": "The Divide-and-Conquer Subgoal-Ordering Algorithm for Speeding up Logic\n  Inference",
    "abstract": "  It is common to view programs as a combination of logic and control: the\nlogic part defines what the program must do, the control part -- how to do it.\nThe Logic Programming paradigm was developed with the intention of separating\nthe logic from the control. Recently, extensive research has been conducted on\nautomatic generation of control for logic programs. Only a few of these works\nconsidered the issue of automatic generation of control for improving the\nefficiency of logic programs. In this paper we present a novel algorithm for\nautomatic finding of lowest-cost subgoal orderings. The algorithm works using\nthe divide-and-conquer strategy. The given set of subgoals is partitioned into\nsmaller sets, based on co-occurrence of free variables. The subsets are ordered\nrecursively and merged, yielding a provably optimal order. We experimentally\ndemonstrate the utility of the algorithm by testing it in several domains, and\ndiscuss the possibilities of its cooperation with other existing methods.\n"
  },
  {
    "id": "1105.5443",
    "title": "The Gn,m Phase Transition is Not Hard for the Hamiltonian Cycle Problem",
    "abstract": "  Using an improved backtrack algorithm with sophisticated pruning techniques,\nwe revise previous observations correlating a high frequency of hard to solve\nHamiltonian Cycle instances with the Gn,m phase transition between\nHamiltonicity and non-Hamiltonicity. Instead all tested graphs of 100 to 1500\nvertices are easily solved. When we artificially restrict the degree sequence\nwith a bounded maximum degree, although there is some increase in difficulty,\nthe frequency of hard graphs is still low. When we consider more regular graphs\nbased on a generalization of knight's tours, we observe frequent instances of\nreally hard graphs, but on these the average degree is bounded by a constant.\nWe design a set of graphs with a feature our algorithm is unable to detect and\nso are very hard for our algorithm, but in these we can vary the average degree\nfrom O(1) to O(n). We have so far found no class of graphs correlated with the\nGn,m phase transition which asymptotically produces a high frequency of hard\ninstances.\n"
  },
  {
    "id": "1105.5444",
    "title": "Semantic Similarity in a Taxonomy: An Information-Based Measure and its\n  Application to Problems of Ambiguity in Natural Language",
    "abstract": "  This article presents a measure of semantic similarity in an IS-A taxonomy\nbased on the notion of shared information content. Experimental evaluation\nagainst a benchmark set of human similarity judgments demonstrates that the\nmeasure performs better than the traditional edge-counting approach. The\narticle presents algorithms that take advantage of taxonomic similarity in\nresolving syntactic and semantic ambiguity, along with experimental results\ndemonstrating their effectiveness.\n"
  },
  {
    "id": "1105.5446",
    "title": "A Temporal Description Logic for Reasoning about Actions and Plans",
    "abstract": "  A class of interval-based temporal languages for uniformly representing and\nreasoning about actions and plans is presented. Actions are represented by\ndescribing what is true while the action itself is occurring, and plans are\nconstructed by temporally relating actions and world states. The temporal\nlanguages are members of the family of Description Logics, which are\ncharacterized by high expressivity combined with good computational properties.\nThe subsumption problem for a class of temporal Description Logics is\ninvestigated and sound and complete decision procedures are given. The basic\nlanguage TL-F is considered first: it is the composition of a temporal logic TL\n-- able to express interval temporal networks -- together with the non-temporal\nlogic F -- a Feature Description Logic. It is proven that subsumption in this\nlanguage is an NP-complete problem. Then it is shown how to reason with the\nmore expressive languages TLU-FU and TL-ALCF. The former adds disjunction both\nat the temporal and non-temporal sides of the language, the latter extends the\nnon-temporal side with set-valued features (i.e., roles) and a propositionally\ncomplete language.\n"
  },
  {
    "id": "1105.5447",
    "title": "Adaptive Parallel Iterative Deepening Search",
    "abstract": "  Many of the artificial intelligence techniques developed to date rely on\nheuristic search through large spaces. Unfortunately, the size of these spaces\nand the corresponding computational effort reduce the applicability of\notherwise novel and effective algorithms. A number of parallel and distributed\napproaches to search have considerably improved the performance of the search\nprocess. Our goal is to develop an architecture that automatically selects\nparallel search strategies for optimal performance on a variety of search\nproblems. In this paper we describe one such architecture realized in the\nEureka system, which combines the benefits of many different approaches to\nparallel heuristic search. Through empirical and theoretical analyses we\nobserve that features of the problem space directly affect the choice of\noptimal parallel search strategy. We then employ machine learning techniques to\nselect the optimal parallel search strategy for a given problem space. When a\nnew search task is input to the system, Eureka uses features describing the\nsearch space and the chosen architecture to automatically select the\nappropriate search strategy. Eureka has been tested on a MIMD parallel\nprocessor, a distributed network of workstations, and a single workstation\nusing multithreading. Results generated from fifteen puzzle problems, robot arm\nmotion problems, artificial search spaces, and planning problems indicate that\nEureka outperforms any of the tested strategies used exclusively for all\nproblem instances and is able to greatly reduce the search time for these\napplications.\n"
  },
  {
    "id": "1105.5448",
    "title": "Order of Magnitude Comparisons of Distance",
    "abstract": "  Order of magnitude reasoning - reasoning by rough comparisons of the sizes of\nquantities - is often called 'back of the envelope calculation', with the\nimplication that the calculations are quick though approximate. This paper\nexhibits an interesting class of constraint sets in which order of magnitude\nreasoning is demonstrably fast. Specifically, we present a polynomial-time\nalgorithm that can solve a set of constraints of the form 'Points a and b are\nmuch closer together than points c and d.' We prove that this algorithm can be\napplied if `much closer together' is interpreted either as referring to an\ninfinite difference in scale or as referring to a finite difference in scale,\nas long as the difference in scale is greater than the number of variables in\nthe constraint set. We also prove that the first-order theory over such\nconstraints is decidable.\n"
  },
  {
    "id": "1105.5449",
    "title": "AntNet: Distributed Stigmergetic Control for Communications Networks",
    "abstract": "  This paper introduces AntNet, a novel approach to the adaptive learning of\nrouting tables in communications networks. AntNet is a distributed, mobile\nagents based Monte Carlo system that was inspired by recent work on the ant\ncolony metaphor for solving optimization problems. AntNet's agents concurrently\nexplore the network and exchange collected information. The communication among\nthe agents is indirect and asynchronous, mediated by the network itself. This\nform of communication is typical of social insects and is called stigmergy. We\ncompare our algorithm with six state-of-the-art routing algorithms coming from\nthe telecommunications and machine learning fields. The algorithms' performance\nis evaluated over a set of realistic testbeds. We run many experiments over\nreal and artificial IP datagram networks with increasing number of nodes and\nunder several paradigmatic spatial and temporal traffic distributions. Results\nare very encouraging. AntNet showed superior performance under all the\nexperimental conditions with respect to its competitors. We analyze the main\ncharacteristics of the algorithm and try to explain the reasons for its\nsuperiority.\n"
  },
  {
    "id": "1105.5450",
    "title": "A Counter Example to Theorems of Cox and Fine",
    "abstract": "  Cox's well-known theorem justifying the use of probability is shown not to\nhold in finite domains. The counterexample also suggests that Cox's assumptions\nare insufficient to prove the result even in infinite domains. The same\ncounterexample is used to disprove a result of Fine on comparative conditional\nprobability.\n"
  },
  {
    "id": "1105.5451",
    "title": "The Automatic Inference of State Invariants in TIM",
    "abstract": "  As planning is applied to larger and richer domains the effort involved in\nconstructing domain descriptions increases and becomes a significant burden on\nthe human application designer. If general planners are to be applied\nsuccessfully to large and complex domains it is necessary to provide the domain\ndesigner with some assistance in building correctly encoded domains. One way of\ndoing this is to provide domain-independent techniques for extracting, from a\ndomain description, knowledge that is implicit in that description and that can\nassist domain designers in debugging domain descriptions. This knowledge can\nalso be exploited to improve the performance of planners: several researchers\nhave explored the potential of state invariants in speeding up the performance\nof domain-independent planners. In this paper we describe a process by which\nstate invariants can be extracted from the automatically inferred type\nstructure of a domain. These techniques are being developed for exploitation by\nSTAN, a Graphplan based planner that employs state analysis techniques to\nenhance its performance.\n"
  },
  {
    "id": "1105.5452",
    "title": "Unifying Class-Based Representation Formalisms",
    "abstract": "  The notion of class is ubiquitous in computer science and is central in many\nformalisms for the representation of structured knowledge used both in\nknowledge representation and in databases. In this paper we study the basic\nissues underlying such representation formalisms and single out both their\ncommon characteristics and their distinguishing features. Such investigation\nleads us to propose a unifying framework in which we are able to capture the\nfundamental aspects of several representation languages used in different\ncontexts. The proposed formalism is expressed in the style of description\nlogics, which have been introduced in knowledge representation as a means to\nprovide a semantically well-founded basis for the structural aspects of\nknowledge representation systems. The description logic considered in this\npaper is a subset of first order logic with nice computational characteristics.\nIt is quite expressive and features a novel combination of constructs that has\nnot been studied before. The distinguishing constructs are number restrictions,\nwhich generalize existence and functional dependencies, inverse roles, which\nallow one to refer to the inverse of a relationship, and possibly cyclic\nassertions, which are necessary for capturing real world domains. We are able\nto show that it is precisely such combination of constructs that makes our\nlogic powerful enough to model the essential set of features for defining class\nstructures that are common to frame systems, object-oriented database\nlanguages, and semantic data models. As a consequence of the established\ncorrespondences, several significant extensions of each of the above formalisms\nbecome available. The high expressiveness of the logic we propose and the need\nfor capturing the reasoning in different contexts forces us to distinguish\nbetween unrestricted and finite model reasoning. A notable feature of our\nproposal is that reasoning in both cases is decidable. We argue that, by virtue\nof the high expressive power and of the associated reasoning capabilities on\nboth unrestricted and finite models, our logic provides a common core for\nclass-based representation formalisms.\n"
  },
  {
    "id": "1105.5453",
    "title": "Complexity of Prioritized Default Logics",
    "abstract": "  In default reasoning, usually not all possible ways of resolving conflicts\nbetween default rules are acceptable. Criteria expressing acceptable ways of\nresolving the conflicts may be hardwired in the inference mechanism, for\nexample specificity in inheritance reasoning can be handled this way, or they\nmay be given abstractly as an ordering on the default rules. In this article we\ninvestigate formalizations of the latter approach in Reiter's default logic.\nOur goal is to analyze and compare the computational properties of three such\nformalizations in terms of their computational complexity: the prioritized\ndefault logics of Baader and Hollunder, and Brewka, and a prioritized default\nlogic that is based on lexicographic comparison. The analysis locates the\npropositional variants of these logics on the second and third levels of the\npolynomial hierarchy, and identifies the boundary between tractable and\nintractable inference for restricted classes of prioritized default theories.\n"
  },
  {
    "id": "1105.5454",
    "title": "Squeaky Wheel Optimization",
    "abstract": "  We describe a general approach to optimization which we term `Squeaky Wheel'\nOptimization (SWO). In SWO, a greedy algorithm is used to construct a solution\nwhich is then analyzed to find the trouble spots, i.e., those elements, that,\nif improved, are likely to improve the objective function score. The results of\nthe analysis are used to generate new priorities that determine the order in\nwhich the greedy algorithm constructs the next solution. This\nConstruct/Analyze/Prioritize cycle continues until some limit is reached, or an\nacceptable solution is found. SWO can be viewed as operating on two search\nspaces: solutions and prioritizations. Successive solutions are only indirectly\nrelated, via the re-prioritization that results from analyzing the prior\nsolution. Similarly, successive prioritizations are generated by constructing\nand analyzing solutions. This `coupled search' has some interesting properties,\nwhich we discuss. We report encouraging experimental results on two domains,\nscheduling problems that arise in fiber-optic cable manufacturing, and graph\ncoloring problems. The fact that these domains are very different supports our\nclaim that SWO is a general technique for optimization.\n"
  },
  {
    "id": "1105.5455",
    "title": "Variational Cumulant Expansions for Intractable Distributions",
    "abstract": "  Intractable distributions present a common difficulty in inference within the\nprobabilistic knowledge representation framework and variational methods have\nrecently been popular in providing an approximate solution. In this article, we\ndescribe a perturbational approach in the form of a cumulant expansion which,\nto lowest order, recovers the standard Kullback-Leibler variational bound.\nHigher-order terms describe corrections on the variational approach without\nincurring much further computational cost. The relationship to other\nperturbational approaches such as TAP is also elucidated. We demonstrate the\nmethod on a particular class of undirected graphical models, Boltzmann\nmachines, for which our simulation results confirm improved accuracy and\nenhanced stability during learning.\n"
  },
  {
    "id": "1105.5457",
    "title": "Efficient Implementation of the Plan Graph in STAN",
    "abstract": "  STAN is a Graphplan-based planner, so-called because it uses a variety of\nSTate ANalysis techniques to enhance its performance. STAN competed in the\nAIPS-98 planning competition where it compared well with the other competitors\nin terms of speed, finding solutions fastest to many of the problems posed.\nAlthough the domain analysis techniques STAN exploits are an important factor\nin its overall performance, we believe that the speed at which STAN solved the\ncompetition problems is largely due to the implementation of its plan graph.\nThe implementation is based on two insights: that many of the graph\nconstruction operations can be implemented as bit-level logical operations on\nbit vectors, and that the graph should not be explicitly constructed beyond the\nfix point. This paper describes the implementation of STAN's plan graph and\nprovides experimental results which demonstrate the circumstances under which\nadvantages can be obtained from using this implementation.\n"
  },
  {
    "id": "1105.5458",
    "title": "Cooperation between Top-Down and Bottom-Up Theorem Provers",
    "abstract": "  Top-down and bottom-up theorem proving approaches each have specific\nadvantages and disadvantages. Bottom-up provers profit from strong redundancy\ncontrol but suffer from the lack of goal-orientation, whereas top-down provers\nare goal-oriented but often have weak calculi when their proof lengths are\nconsidered. In order to integrate both approaches, we try to achieve\ncooperation between a top-down and a bottom-up prover in two different ways:\nThe first technique aims at supporting a bottom-up with a top-down prover. A\ntop-down prover generates subgoal clauses, they are then processed by a\nbottom-up prover. The second technique deals with the use of bottom-up\ngenerated lemmas in a top-down prover. We apply our concept to the areas of\nmodel elimination and superposition. We discuss the ability of our techniques\nto shorten proofs as well as to reorder the search space in an appropriate\nmanner. Furthermore, in order to identify subgoal clauses and lemmas which are\nactually relevant for the proof task, we develop methods for a relevancy-based\nfiltering. Experiments with the provers SETHEO and SPASS performed in the\nproblem library TPTP reveal the high potential of our cooperation approaches.\n"
  },
  {
    "id": "1105.5459",
    "title": "Solving Highly Constrained Search Problems with Quantum Computers",
    "abstract": "  A previously developed quantum search algorithm for solving 1-SAT problems in\na single step is generalized to apply to a range of highly constrained k-SAT\nproblems. We identify a bound on the number of clauses in satisfiability\nproblems for which the generalized algorithm can find a solution in a constant\nnumber of steps as the number of variables increases. This performance\ncontrasts with the linear growth in the number of steps required by the best\nclassical algorithms, and the exponential number required by classical and\nquantum methods that ignore the problem structure. In some cases, the algorithm\ncan also guarantee that insoluble problems in fact have no solutions, unlike\npreviously proposed quantum search algorithms.\n"
  },
  {
    "id": "1105.5460",
    "title": "Decision-Theoretic Planning: Structural Assumptions and Computational\n  Leverage",
    "abstract": "  Planning under uncertainty is a central problem in the study of automated\nsequential decision making, and has been addressed by researchers in many\ndifferent fields, including AI planning, decision analysis, operations\nresearch, control theory and economics. While the assumptions and perspectives\nadopted in these areas often differ in substantial ways, many planning problems\nof interest to researchers in these fields can be modeled as Markov decision\nprocesses (MDPs) and analyzed using the techniques of decision theory. This\npaper presents an overview and synthesis of MDP-related methods, showing how\nthey provide a unifying framework for modeling many classes of planning\nproblems studied in AI. It also describes structural properties of MDPs that,\nwhen exhibited by particular classes of problems, can be exploited in the\nconstruction of optimal or approximately optimal policies or plans. Planning\nproblems commonly possess structure in the reward and value functions used to\ndescribe performance criteria, in the functions used to describe state\ntransitions and observations, and in the relationships among features used to\ndescribe states, actions, rewards, and observations. Specialized\nrepresentations, and algorithms employing these representations, can achieve\ncomputational leverage by exploiting these various forms of structure. Certain\nAI techniques -- in particular those based on the use of structured,\nintensional representations -- can be viewed in this way. This paper surveys\nseveral types of representations for both classical and decision-theoretic\nplanning problems, and planning algorithms that exploit these representations\nin a number of different ways to ease the computational burden of constructing\npolicies or plans. It focuses primarily on abstraction, aggregation and\ndecomposition techniques based on AI-style representations.\n"
  },
  {
    "id": "1105.5461",
    "title": "Probabilistic Deduction with Conditional Constraints over Basic Events",
    "abstract": "  We study the problem of probabilistic deduction with conditional constraints\nover basic events. We show that globally complete probabilistic deduction with\nconditional constraints over basic events is NP-hard. We then concentrate on\nthe special case of probabilistic deduction in conditional constraint trees. We\nelaborate very efficient techniques for globally complete probabilistic\ndeduction. In detail, for conditional constraint trees with point\nprobabilities, we present a local approach to globally complete probabilistic\ndeduction, which runs in linear time in the size of the conditional constraint\ntrees. For conditional constraint trees with interval probabilities, we show\nthat globally complete probabilistic deduction can be done in a global approach\nby solving nonlinear programs. We show how these nonlinear programs can be\ntransformed into equivalent linear programs, which are solvable in polynomial\ntime in the size of the conditional constraint trees.\n"
  },
  {
    "id": "1105.5462",
    "title": "Variational Probabilistic Inference and the QMR-DT Network",
    "abstract": "  We describe a variational approximation method for efficient inference in\nlarge-scale probabilistic models. Variational methods are deterministic\nprocedures that provide approximations to marginal and conditional\nprobabilities of interest. They provide alternatives to approximate inference\nmethods based on stochastic sampling or search. We describe a variational\napproach to the problem of diagnostic inference in the `Quick Medical\nReference' (QMR) network. The QMR network is a large-scale probabilistic\ngraphical model built on statistical and expert knowledge. Exact probabilistic\ninference is infeasible in this model for all but a small set of cases. We\nevaluate our variational inference algorithm on a large set of diagnostic test\ncases, comparing the algorithm to a state-of-the-art stochastic sampling\nmethod.\n"
  },
  {
    "id": "1105.5463",
    "title": "Extensible Knowledge Representation: the Case of Description Reasoners",
    "abstract": "  This paper offers an approach to extensible knowledge representation and\nreasoning for a family of formalisms known as Description Logics. The approach\nis based on the notion of adding new concept constructors, and includes a\nheuristic methodology for specifying the desired extensions, as well as a\nmodularized software architecture that supports implementing extensions. The\narchitecture detailed here falls in the normalize-compared paradigm, and\nsupports both intentional reasoning (subsumption) involving concepts, and\nextensional reasoning involving individuals after incremental updates to the\nknowledge base. The resulting approach can be used to extend the reasoner with\nspecialized notions that are motivated by specific problems or application\nareas, such as reasoning about dates, plans, etc. In addition, it provides an\nopportunity to implement constructors that are not currently yet sufficiently\nwell understood theoretically, but are needed in practice. Also, for\nconstructors that are provably hard to reason with (e.g., ones whose presence\nwould lead to undecidability), it allows the implementation of incomplete\nreasoners where the incompleteness is tailored to be acceptable for the\napplication at hand.\n"
  },
  {
    "id": "1105.5465",
    "title": "Constructing Conditional Plans by a Theorem-Prover",
    "abstract": "  The research on conditional planning rejects the assumptions that there is no\nuncertainty or incompleteness of knowledge with respect to the state and\nchanges of the system the plans operate on. Without these assumptions the\nsequences of operations that achieve the goals depend on the initial state and\nthe outcomes of nondeterministic changes in the system. This setting raises the\nquestions of how to represent the plans and how to perform plan search. The\nanswers are quite different from those in the simpler classical framework. In\nthis paper, we approach conditional planning from a new viewpoint that is\nmotivated by the use of satisfiability algorithms in classical planning.\nTranslating conditional planning to formulae in the propositional logic is not\nfeasible because of inherent computational limitations. Instead, we translate\nconditional planning to quantified Boolean formulae. We discuss three\nformalizations of conditional planning as quantified Boolean formulae, and\npresent experimental results obtained with a theorem-prover.\n"
  },
  {
    "id": "1105.5466",
    "title": "Issues in Stacked Generalization",
    "abstract": "  Stacked generalization is a general method of using a high-level model to\ncombine lower-level models to achieve greater predictive accuracy. In this\npaper we address two crucial issues which have been considered to be a `black\nart' in classification tasks ever since the introduction of stacked\ngeneralization in 1992 by Wolpert: the type of generalizer that is suitable to\nderive the higher-level model, and the kind of attributes that should be used\nas its input. We find that best results are obtained when the higher-level\nmodel combines the confidence (and not just the predictions) of the lower-level\nones. We demonstrate the effectiveness of stacked generalization for combining\nthree different types of learning algorithms for classification tasks. We also\ncompare the performance of stacked generalization with majority vote and\npublished results of arcing and bagging.\n"
  },
  {
    "id": "1105.5516",
    "title": "Ontology Alignment at the Instance and Schema Level",
    "abstract": "  We present PARIS, an approach for the automatic alignment of ontologies.\nPARIS aligns not only instances, but also relations and classes. Alignments at\nthe instance-level cross-fertilize with alignments at the schema-level.\nThereby, our system provides a truly holistic solution to the problem of\nontology alignment. The heart of the approach is probabilistic. This allows\nPARIS to run without any parameter tuning. We demonstrate the efficiency of the\nalgorithm and its precision through extensive experiments. In particular, we\nobtain a precision of around 90% in experiments with two of the world's largest\nontologies.\n"
  },
  {
    "id": "1105.5667",
    "title": "Complexity of and Algorithms for Borda Manipulation",
    "abstract": "  We prove that it is NP-hard for a coalition of two manipulators to compute\nhow to manipulate the Borda voting rule. This resolves one of the last open\nproblems in the computational complexity of manipulating common voting rules.\nBecause of this NP-hardness, we treat computing a manipulation as an\napproximation problem where we try to minimize the number of manipulators.\nBased on ideas from bin packing and multiprocessor scheduling, we propose two\nnew approximation methods to compute manipulations of the Borda rule.\nExperiments show that these methods significantly outperform the previous best\nknown %existing approximation method. We are able to find optimal manipulations\nin almost all the randomly generated elections tested. Our results suggest\nthat, whilst computing a manipulation of the Borda rule by a coalition is\nNP-hard, computational complexity may provide only a weak barrier against\nmanipulation in practice.\n"
  },
  {
    "id": "1105.6124",
    "title": "Reasoning on Interval and Point-based Disjunctive Metric Constraints in\n  Temporal Contexts",
    "abstract": "  We introduce a temporal model for reasoning on disjunctive metric constraints\non intervals and time points in temporal contexts. This temporal model is\ncomposed of a labeled temporal algebra and its reasoning algorithms. The\nlabeled temporal algebra defines labeled disjunctive metric point-based\nconstraints, where each disjunct in each input disjunctive constraint is\nunivocally associated to a label. Reasoning algorithms manage labeled\nconstraints, associated label lists, and sets of mutually inconsistent\ndisjuncts. These algorithms guarantee consistency and obtain a minimal network.\nAdditionally, constraints can be organized in a hierarchy of alternative\ntemporal contexts. Therefore, we can reason on context-dependent disjunctive\nmetric constraints on intervals and points. Moreover, the model is able to\nrepresent non-binary constraints, such that logical dependencies on disjuncts\nin constraints can be handled. The computational cost of reasoning algorithms\nis exponential in accordance with the underlying problem complexity, although\nsome improvements are proposed.\n"
  },
  {
    "id": "1105.6148",
    "title": "Overcoming Misleads In Logic Programs by Redefining Negation",
    "abstract": "  Negation as failure and incomplete information in logic programs have been\nstudied by many researchers In order to explains HOW a negated conclusion was\nreached, we introduce and proof a different way for negating facts to\novercoming misleads in logic programs. Negating facts can be achieved by asking\nthe user for constants that do not appear elsewhere in the knowledge base.\n"
  },
  {
    "id": "1106.0171",
    "title": "Proposal of Pattern Recognition as a necessary and sufficient Principle\n  to Cognitive Science",
    "abstract": "  Despite the prevalence of the Computational Theory of Mind and the\nConnectionist Model, the establishing of the key principles of the Cognitive\nScience are still controversy and inconclusive. This paper proposes the concept\nof Pattern Recognition as Necessary and Sufficient Principle for a general\ncognitive science modeling, in a very ambitious scientific proposal. A formal\nphysical definition of the pattern recognition concept is also proposed to\nsolve many key conceptual gaps on the field.\n"
  },
  {
    "id": "1106.0218",
    "title": "The Good Old Davis-Putnam Procedure Helps Counting Models",
    "abstract": "  As was shown recently, many important AI problems require counting the number\nof models of propositional formulas. The problem of counting models of such\nformulas is, according to present knowledge, computationally intractable in a\nworst case. Based on the Davis-Putnam procedure, we present an algorithm, CDP,\nthat computes the exact number of models of a propositional CNF or DNF formula\nF. Let m and n be the number of clauses and variables of F, respectively, and\nlet p denote the probability that a literal l of F occurs in a clause C of F,\nthen the average running time of CDP is shown to be O(nm^d), where\nd=-1/log(1-p). The practical performance of CDP has been estimated in a series\nof experiments on a wide variety of CNF formulas.\n"
  },
  {
    "id": "1106.0219",
    "title": "Identifying Mislabeled Training Data",
    "abstract": "  This paper presents a new approach to identifying and eliminating mislabeled\ntraining instances for supervised learning. The goal of this approach is to\nimprove classification accuracies produced by learning algorithms by improving\nthe quality of the training data. Our approach uses a set of learning\nalgorithms to create classifiers that serve as noise filters for the training\ndata. We evaluate single algorithm, majority vote and consensus filters on five\ndatasets that are prone to labeling errors. Our experiments illustrate that\nfiltering significantly improves classification accuracy for noise levels up to\n30 percent. An analytical and empirical evaluation of the precision of our\napproach shows that consensus filters are conservative at throwing away good\ndata at the expense of retaining bad data and that majority filters are better\nat detecting bad data at the expense of throwing away good data. This suggests\nthat for situations in which there is a paucity of data, consensus filters are\npreferable, whereas majority vote filters are preferable for situations with an\nabundance of data.\n"
  },
  {
    "id": "1106.0220",
    "title": "Committee-Based Sample Selection for Probabilistic Classifiers",
    "abstract": "  In many real-world learning tasks, it is expensive to acquire a sufficient\nnumber of labeled examples for training. This paper investigates methods for\nreducing annotation cost by `sample selection'. In this approach, during\ntraining the learning program examines many unlabeled examples and selects for\nlabeling only those that are most informative at each stage. This avoids\nredundantly labeling examples that contribute little new information. Our work\nfollows on previous research on Query By Committee, extending the\ncommittee-based paradigm to the context of probabilistic classification. We\ndescribe a family of empirical methods for committee-based sample selection in\nprobabilistic classification models, which evaluate the informativeness of an\nexample by measuring the degree of disagreement between several model variants.\nThese variants (the committee) are drawn randomly from a probability\ndistribution conditioned by the training set labeled so far. The method was\napplied to the real-world natural language processing task of stochastic\npart-of-speech tagging. We find that all variants of the method achieve a\nsignificant reduction in annotation cost, although their computational\nefficiency differs. In particular, the simplest variant, a two member committee\nwith no parameters to tune, gives excellent results. We also show that sample\nselection yields a significant reduction in the size of the model used by the\ntagger.\n"
  },
  {
    "id": "1106.0224",
    "title": "Reasoning about Minimal Belief and Negation as Failure",
    "abstract": "  We investigate the problem of reasoning in the propositional fragment of\nMBNF, the logic of minimal belief and negation as failure introduced by\nLifschitz, which can be considered as a unifying framework for several\nnonmonotonic formalisms, including default logic, autoepistemic logic,\ncircumscription, epistemic queries, and logic programming. We characterize the\ncomplexity and provide algorithms for reasoning in propositional MBNF. In\nparticular, we show that entailment in propositional MBNF lies at the third\nlevel of the polynomial hierarchy, hence it is harder than reasoning in all the\nabove mentioned propositional formalisms for nonmonotonic reasoning. We also\nprove the exact correspondence between negation as failure in MBNF and negative\nintrospection in Moore's autoepistemic logic.\n"
  },
  {
    "id": "1106.0225",
    "title": "Randomized Algorithms for the Loop Cutset Problem",
    "abstract": "  We show how to find a minimum weight loop cutset in a Bayesian network with\nhigh probability. Finding such a loop cutset is the first step in the method of\nconditioning for inference. Our randomized algorithm for finding a loop cutset\noutputs a minimum loop cutset after O(c 6^k kn) steps with probability at least\n1 - (1 - 1/(6^k))^c6^k, where c > 1 is a constant specified by the user, k is\nthe minimal size of a minimum weight loop cutset, and n is the number of\nvertices. We also show empirically that a variant of this algorithm often finds\na loop cutset that is closer to the minimum weight loop cutset than the ones\nfound by the best deterministic algorithms known.\n"
  },
  {
    "id": "1106.0229",
    "title": "OBDD-based Universal Planning for Synchronized Agents in\n  Non-Deterministic Domains",
    "abstract": "  Recently model checking representation and search techniques were shown to be\nefficiently applicable to planning, in particular to non-deterministic\nplanning. Such planning approaches use Ordered Binary Decision Diagrams (OBDDs)\nto encode a planning domain as a non-deterministic finite automaton and then\napply fast algorithms from model checking to search for a solution. OBDDs can\neffectively scale and can provide universal plans for complex planning domains.\nWe are particularly interested in addressing the complexities arising in\nnon-deterministic, multi-agent domains. In this article, we present UMOP, a new\nuniversal OBDD-based planning framework for non-deterministic, multi-agent\ndomains. We introduce a new planning domain description language, NADL, to\nspecify non-deterministic, multi-agent domains. The language contributes the\nexplicit definition of controllable agents and uncontrollable environment\nagents. We describe the syntax and semantics of NADL and show how to build an\nefficient OBDD-based representation of an NADL description. The UMOP planning\nsystem uses NADL and different OBDD-based universal planning algorithms. It\nincludes the previously developed strong and strong cyclic planning algorithms.\nIn addition, we introduce our new optimistic planning algorithm that relaxes\noptimality guarantees and generates plausible universal plans in some domains\nwhere no strong nor strong cyclic solution exists. We present empirical results\napplying UMOP to domains ranging from deterministic and single-agent with no\nenvironment actions to non-deterministic and multi-agent with complex\nenvironment actions. UMOP is shown to be a rich and efficient planning system.\n"
  },
  {
    "id": "1106.0230",
    "title": "Planning Graph as a (Dynamic) CSP: Exploiting EBL, DDB and other CSP\n  Search Techniques in Graphplan",
    "abstract": "  This paper reviews the connections between Graphplan's planning-graph and the\ndynamic constraint satisfaction problem and motivates the need for adapting CSP\nsearch techniques to the Graphplan algorithm. It then describes how explanation\nbased learning, dependency directed backtracking, dynamic variable ordering,\nforward checking, sticky values and random-restart search strategies can be\nadapted to Graphplan. Empirical results are provided to demonstrate that these\naugmentations improve Graphplan's performance significantly (up to 1000x\nspeedups) on several benchmark problems. Special attention is paid to the\nexplanation-based learning and dependency directed backtracking techniques as\nthey are empirically found to be most useful in improving the performance of\nGraphplan.\n"
  },
  {
    "id": "1106.0233",
    "title": "Space Efficiency of Propositional Knowledge Representation Formalisms",
    "abstract": "  We investigate the space efficiency of a Propositional Knowledge\nRepresentation (PKR) formalism. Intuitively, the space efficiency of a\nformalism F in representing a certain piece of knowledge A, is the size of the\nshortest formula of F that represents A. In this paper we assume that knowledge\nis either a set of propositional interpretations (models) or a set of\npropositional formulae (theorems). We provide a formal way of talking about the\nrelative ability of PKR formalisms to compactly represent a set of models or a\nset of theorems. We introduce two new compactness measures, the corresponding\nclasses, and show that the relative space efficiency of a PKR formalism in\nrepresenting models/theorems is directly related to such classes. In\nparticular, we consider formalisms for nonmonotonic reasoning, such as\ncircumscription and default logic, as well as belief revision operators and the\nstable model semantics for logic programs with negation. One interesting result\nis that formalisms with the same time complexity do not necessarily belong to\nthe same space efficiency class.\n"
  },
  {
    "id": "1106.0234",
    "title": "Value-Function Approximations for Partially Observable Markov Decision\n  Processes",
    "abstract": "  Partially observable Markov decision processes (POMDPs) provide an elegant\nmathematical framework for modeling complex decision and planning problems in\nstochastic domains in which states of the system are observable only\nindirectly, via a set of imperfect or noisy observations. The modeling\nadvantage of POMDPs, however, comes at a price -- exact methods for solving\nthem are computationally very expensive and thus applicable in practice only to\nvery simple problems. We focus on efficient approximation (heuristic) methods\nthat attempt to alleviate the computational problem and trade off accuracy for\nspeed. We have two objectives here. First, we survey various approximation\nmethods, analyze their properties and relations and provide some new insights\ninto their differences. Second, we present a number of new approximation\nmethods and novel refinements of existing techniques. The theoretical results\nare supported by experiments on a problem from the agent navigation domain.\n"
  },
  {
    "id": "1106.0237",
    "title": "On Deducing Conditional Independence from d-Separation in Causal Graphs\n  with Feedback (Research Note)",
    "abstract": "  Pearl and Dechter (1996) claimed that the d-separation criterion for\nconditional independence in acyclic causal networks also applies to networks of\ndiscrete variables that have feedback cycles, provided that the variables of\nthe system are uniquely determined by the random disturbances. I show by\nexample that this is not true in general. Some condition stronger than\nuniqueness is needed, such as the existence of a causal dynamics guaranteed to\nlead to the unique solution.\n"
  },
  {
    "id": "1106.0238",
    "title": "What's in an Attribute? Consequences for the Least Common Subsumer",
    "abstract": "  Functional relationships between objects, called `attributes', are of\nconsiderable importance in knowledge representation languages, including\nDescription Logics (DLs). A study of the literature indicates that papers have\nmade, often implicitly, different assumptions about the nature of attributes:\nwhether they are always required to have a value, or whether they can be\npartial functions. The work presented here is the first explicit study of this\ndifference for subclasses of the CLASSIC DL, involving the same-as concept\nconstructor. It is shown that although determining subsumption between concept\ndescriptions has the same complexity (though requiring different algorithms),\nthe story is different in the case of determining the least common subsumer\n(lcs). For attributes interpreted as partial functions, the lcs exists and can\nbe computed relatively easily; even in this case our results correct and extend\nthree previous papers about the lcs of DLs. In the case where attributes must\nhave a value, the lcs may not exist, and even if it exists it may be of\nexponential size. Interestingly, it is possible to decide in polynomial time if\nthe lcs exists.\n"
  },
  {
    "id": "1106.0239",
    "title": "The Complexity of Reasoning with Cardinality Restrictions and Nominals\n  in Expressive Description Logics",
    "abstract": "  We study the complexity of the combination of the Description Logics ALCQ and\nALCQI with a terminological formalism based on cardinality restrictions on\nconcepts. These combinations can naturally be embedded into C^2, the two\nvariable fragment of predicate logic with counting quantifiers, which yields\ndecidability in NExpTime. We show that this approach leads to an optimal\nsolution for ALCQI, as ALCQI with cardinality restrictions has the same\ncomplexity as C^2 (NExpTime-complete). In contrast, we show that for ALCQ, the\nproblem can be solved in ExpTime. This result is obtained by a reduction of\nreasoning with cardinality restrictions to reasoning with the (in general\nweaker) terminological formalism of general axioms for ALCQ extended with\nnominals. Using the same reduction, we show that, for the extension of ALCQI\nwith nominals, reasoning with general axioms is a NExpTime-complete problem.\nFinally, we sharpen this result and show that pure concept satisfiability for\nALCQI with nominals is NExpTime-complete. Without nominals, this problem is\nknown to be PSpace-complete.\n"
  },
  {
    "id": "1106.0240",
    "title": "Backbone Fragility and the Local Search Cost Peak",
    "abstract": "  The local search algorithm WSat is one of the most successful algorithms for\nsolving the satisfiability (SAT) problem. It is notably effective at solving\nhard Random 3-SAT instances near the so-called `satisfiability threshold', but\nstill shows a peak in search cost near the threshold and large variations in\ncost over different instances. We make a number of significant contributions to\nthe analysis of WSat on high-cost random instances, using the\nrecently-introduced concept of the backbone of a SAT instance. The backbone is\nthe set of literals which are entailed by an instance. We find that the number\nof solutions predicts the cost well for small-backbone instances but is much\nless relevant for the large-backbone instances which appear near the threshold\nand dominate in the overconstrained region. We show a very strong correlation\nbetween search cost and the Hamming distance to the nearest solution early in\nWSat's search. This pattern leads us to introduce a measure of the backbone\nfragility of an instance, which indicates how persistent the backbone is as\nclauses are removed. We propose that high-cost random instances for local\nsearch are those with very large backbones which are also backbone-fragile. We\nsuggest that the decay in cost beyond the satisfiability threshold is due to\nincreasing backbone robustness (the opposite of backbone fragility). Our\nhypothesis makes three correct predictions. First, that the backbone robustness\nof an instance is negatively correlated with the local search cost when other\nfactors are controlled for. Second, that backbone-minimal instances (which are\n3-SAT instances altered so as to be more backbone-fragile) are unusually hard\nfor WSat. Third, that the clauses most often unsatisfied during search are\nthose whose deletion has the most effect on the backbone. In understanding the\npathologies of local search methods, we hope to contribute to the development\nof new and better techniques.\n"
  },
  {
    "id": "1106.0241",
    "title": "An Application of Reinforcement Learning to Dialogue Strategy Selection\n  in a Spoken Dialogue System for Email",
    "abstract": "  This paper describes a novel method by which a spoken dialogue system can\nlearn to choose an optimal dialogue strategy from its experience interacting\nwith human users. The method is based on a combination of reinforcement\nlearning and performance modeling of spoken dialogue systems. The reinforcement\nlearning component applies Q-learning (Watkins, 1989), while the performance\nmodeling component applies the PARADISE evaluation framework (Walker et al.,\n1997) to learn the performance function (reward) used in reinforcement\nlearning. We illustrate the method with a spoken dialogue system named ELVIS\n(EmaiL Voice Interactive System), that supports access to email over the phone.\nWe conduct a set of experiments for training an optimal dialogue strategy on a\ncorpus of 219 dialogues in which human users interact with ELVIS over the\nphone. We then test that strategy on a corpus of 18 dialogues. We show that\nELVIS can learn to optimize its strategy selection for agent initiative, for\nreading messages, and for summarizing email folders.\n"
  },
  {
    "id": "1106.0242",
    "title": "Nonapproximability Results for Partially Observable Markov Decision\n  Processes",
    "abstract": "  We show that for several variations of partially observable Markov decision\nprocesses, polynomial-time algorithms for finding control policies are unlikely\nto or simply don't have guarantees of finding policies within a constant factor\nor a constant summand of optimal. Here \"unlikely\" means \"unless some complexity\nclasses collapse,\" where the collapses considered are P=NP, P=PSPACE, or P=EXP.\nUntil or unless these collapses are shown to hold, any control-policy designer\nmust choose between such performance guarantees and efficient computation.\n"
  },
  {
    "id": "1106.0243",
    "title": "On Reasonable and Forced Goal Orderings and their Use in an\n  Agenda-Driven Planning Algorithm",
    "abstract": "  The paper addresses the problem of computing goal orderings, which is one of\nthe longstanding issues in AI planning. It makes two new contributions. First,\nit formally defines and discusses two different goal orderings, which are\ncalled the reasonable and the forced ordering. Both orderings are defined for\nsimple STRIPS operators as well as for more complex ADL operators supporting\nnegation and conditional effects. The complexity of these orderings is\ninvestigated and their practical relevance is discussed. Secondly, two\ndifferent methods to compute reasonable goal orderings are developed. One of\nthem is based on planning graphs, while the other investigates the set of\nactions directly. Finally, it is shown how the ordering relations, which have\nbeen derived for a given set of goals G, can be used to compute a so-called\ngoal agenda that divides G into an ordered set of subgoals. Any planner can\nthen, in principle, use the goal agenda to plan for increasing sets of\nsubgoals. This can lead to an exponential complexity reduction, as the solution\nto a complex planning problem is found by solving easier subproblems. Since\nonly a polynomial overhead is caused by the goal agenda computation, a\npotential exists to dramatically speed up planning algorithms as we demonstrate\nin the empirical evaluation, where we use this method in the IPP planner.\n"
  },
  {
    "id": "1106.0244",
    "title": "Asimovian Adaptive Agents",
    "abstract": "  The goal of this research is to develop agents that are adaptive and\npredictable and timely. At first blush, these three requirements seem\ncontradictory. For example, adaptation risks introducing undesirable side\neffects, thereby making agents' behavior less predictable. Furthermore,\nalthough formal verification can assist in ensuring behavioral predictability,\nit is known to be time-consuming. Our solution to the challenge of satisfying\nall three requirements is the following. Agents have finite-state automaton\nplans, which are adapted online via evolutionary learning (perturbation)\noperators. To ensure that critical behavioral constraints are always satisfied,\nagents' plans are first formally verified. They are then reverified after every\nadaptation. If reverification concludes that constraints are violated, the\nplans are repaired. The main objective of this paper is to improve the\nefficiency of reverification after learning, so that agents have a sufficiently\nrapid response time. We present two solutions: positive results that certain\nlearning operators are a priori guaranteed to preserve useful classes of\nbehavioral assurance constraints (which implies that no reverification is\nneeded for these operators), and efficient incremental reverification\nalgorithms for those learning operators that have negative a priori results.\n"
  },
  {
    "id": "1106.0245",
    "title": "A Model of Inductive Bias Learning",
    "abstract": "  A major problem in machine learning is that of inductive bias: how to choose\na learner's hypothesis space so that it is large enough to contain a solution\nto the problem being learnt, yet small enough to ensure reliable generalization\nfrom reasonably-sized training sets. Typically such bias is supplied by hand\nthrough the skill and insights of experts. In this paper a model for\nautomatically learning bias is investigated. The central assumption of the\nmodel is that the learner is embedded within an environment of related learning\ntasks. Within such an environment the learner can sample from multiple tasks,\nand hence it can search for a hypothesis space that contains good solutions to\nmany of the problems in the environment. Under certain restrictions on the set\nof all hypothesis spaces available to the learner, we show that a hypothesis\nspace that performs well on a sufficiently large number of training tasks will\nalso perform well when learning novel tasks in the same environment. Explicit\nbounds are also derived demonstrating that learning multiple tasks within an\nenvironment of related tasks can potentially give much better generalization\nthan learning a single task.\n"
  },
  {
    "id": "1106.0246",
    "title": "Mean Field Methods for a Special Class of Belief Networks",
    "abstract": "  The chief aim of this paper is to propose mean-field approximations for a\nbroad class of Belief networks, of which sigmoid and noisy-or networks can be\nseen as special cases. The approximations are based on a powerful mean-field\ntheory suggested by Plefka. We show that Saul, Jaakkola and Jordan' s approach\nis the first order approximation in Plefka's approach, via a variational\nderivation. The application of Plefka's theory to belief networks is not\ncomputationally tractable. To tackle this problem we propose new approximations\nbased on Taylor series. Small scale experiments show that the proposed schemes\nare attractive.\n"
  },
  {
    "id": "1106.0247",
    "title": "On the Compilability and Expressive Power of Propositional Planning\n  Formalisms",
    "abstract": "  The recent approaches of extending the GRAPHPLAN algorithm to handle more\nexpressive planning formalisms raise the question of what the formal meaning of\n\"expressive power\" is. We formalize the intuition that expressive power is a\nmeasure of how concisely planning domains and plans can be expressed in a\nparticular formalism by introducing the notion of \"compilation schemes\" between\nplanning formalisms. Using this notion, we analyze the expressiveness of a\nlarge family of propositional planning formalisms, ranging from basic STRIPS to\na formalism with conditional effects, partial state specifications, and\npropositional formulae in the preconditions. One of the results is that\nconditional effects cannot be compiled away if plan size should grow only\nlinearly but can be compiled away if we allow for polynomial growth of the\nresulting plans. This result confirms that the recently proposed extensions to\nthe GRAPHPLAN algorithm concerning conditional effects are optimal with respect\nto the \"compilability\" framework. Another result is that general propositional\nformulae cannot be compiled into conditional effects if the plan size should be\npreserved linearly. This implies that allowing general propositional formulae\nin preconditions and effect conditions adds another level of difficulty in\ngenerating a plan.\n"
  },
  {
    "id": "1106.0249",
    "title": "Partial-Order Planning with Concurrent Interacting Actions",
    "abstract": "  In order to generate plans for agents with multiple actuators, agent teams,\nor distributed controllers, we must be able to represent and plan using\nconcurrent actions with interacting effects. This has historically been\nconsidered a challenging task requiring a temporal planner with the ability to\nreason explicitly about time. We show that with simple modifications, the\nSTRIPS action representation language can be used to represent interacting\nactions. Moreover, algorithms for partial-order planning require only small\nmodifications in order to be applied in such multiagent domains. We demonstrate\nthis fact by developing a sound and complete partial-order planner for planning\nwith concurrent interacting actions, POMP, that extends existing partial-order\nplanners in a straightforward way. These results open the way to the use of\npartial-order planners for the centralized control of cooperative multiagent\nsystems.\n"
  },
  {
    "id": "1106.0250",
    "title": "Planning by Rewriting",
    "abstract": "  Domain-independent planning is a hard combinatorial problem. Taking into\naccount plan quality makes the task even more difficult. This article\nintroduces Planning by Rewriting (PbR), a new paradigm for efficient\nhigh-quality domain-independent planning. PbR exploits declarative\nplan-rewriting rules and efficient local search techniques to transform an\neasy-to-generate, but possibly suboptimal, initial plan into a high-quality\nplan. In addition to addressing the issues of planning efficiency and plan\nquality, this framework offers a new anytime planning algorithm. We have\nimplemented this planner and applied it to several existing domains. The\nexperimental results show that the PbR approach provides significant savings in\nplanning effort while generating high-quality plans.\n"
  },
  {
    "id": "1106.0251",
    "title": "Speeding Up the Convergence of Value Iteration in Partially Observable\n  Markov Decision Processes",
    "abstract": "  Partially observable Markov decision processes (POMDPs) have recently become\npopular among many AI researchers because they serve as a natural model for\nplanning under uncertainty. Value iteration is a well-known algorithm for\nfinding optimal policies for POMDPs. It typically takes a large number of\niterations to converge. This paper proposes a method for accelerating the\nconvergence of value iteration. The method has been evaluated on an array of\nbenchmark problems and was found to be very effective: It enabled value\niteration to converge after only a few iterations on all the test problems.\n"
  },
  {
    "id": "1106.0252",
    "title": "Conformant Planning via Symbolic Model Checking",
    "abstract": "  We tackle the problem of planning in nondeterministic domains, by presenting\na new approach to conformant planning. Conformant planning is the problem of\nfinding a sequence of actions that is guaranteed to achieve the goal despite\nthe nondeterminism of the domain. Our approach is based on the representation\nof the planning domain as a finite state automaton. We use Symbolic Model\nChecking techniques, in particular Binary Decision Diagrams, to compactly\nrepresent and efficiently search the automaton. In this paper we make the\nfollowing contributions. First, we present a general planning algorithm for\nconformant planning, which applies to fully nondeterministic domains, with\nuncertainty in the initial condition and in action effects. The algorithm is\nbased on a breadth-first, backward search, and returns conformant plans of\nminimal length, if a solution to the planning problem exists, otherwise it\nterminates concluding that the problem admits no conformant solution. Second,\nwe provide a symbolic representation of the search space based on Binary\nDecision Diagrams (BDDs), which is the basis for search techniques derived from\nsymbolic model checking. The symbolic representation makes it possible to\nanalyze potentially large sets of states and transitions in a single\ncomputation step, thus providing for an efficient implementation. Third, we\npresent CMBP (Conformant Model Based Planner), an efficient implementation of\nthe data structures and algorithm described above, directly based on BDD\nmanipulations, which allows for a compact representation of the search layers\nand an efficient implementation of the search steps. Finally, we present an\nexperimental comparison of our approach with the state-of-the-art conformant\nplanners CGP, QBFPLAN and GPT. Our analysis includes all the planning problems\nfrom the distribution packages of these systems, plus other problems defined to\nstress a number of specific factors. Our approach appears to be the most\neffective: CMBP is strictly more expressive than QBFPLAN and CGP and, in all\nthe problems where a comparison is possible, CMBP outperforms its competitors,\nsometimes by orders of magnitude.\n"
  },
  {
    "id": "1106.0253",
    "title": "AIS-BN: An Adaptive Importance Sampling Algorithm for Evidential\n  Reasoning in Large Bayesian Networks",
    "abstract": "  Stochastic sampling algorithms, while an attractive alternative to exact\nalgorithms in very large Bayesian network models, have been observed to perform\npoorly in evidential reasoning with extremely unlikely evidence. To address\nthis problem, we propose an adaptive importance sampling algorithm, AIS-BN,\nthat shows promising convergence rates even under extreme conditions and seems\nto outperform the existing sampling algorithms consistently. Three sources of\nthis performance improvement are (1) two heuristics for initialization of the\nimportance function that are based on the theoretical properties of importance\nsampling in finite-dimensional integrals and the structural advantages of\nBayesian networks, (2) a smooth learning method for the importance function,\nand (3) a dynamic weighting function for combining samples from different\nstages of the algorithm. We tested the performance of the AIS-BN algorithm\nalong with two state of the art general purpose sampling algorithms, likelihood\nweighting (Fung and Chang, 1989; Shachter and Peot, 1989) and self-importance\nsampling (Shachter and Peot, 1989). We used in our tests three large real\nBayesian network models available to the scientific community: the CPCS network\n(Pradhan et al., 1994), the PathFinder network (Heckerman, Horvitz, and\nNathwani, 1990), and the ANDES network (Conati, Gertner, VanLehn, and Druzdzel,\n1997), with evidence as unlikely as 10^-41. While the AIS-BN algorithm always\nperformed better than the other two algorithms, in the majority of the test\ncases it achieved orders of magnitude improvement in precision of the results.\nImprovement in speed given a desired precision is even more dramatic, although\nwe are unable to report numerical results here, as the other algorithms almost\nnever achieved the precision reached even by the first few iterations of the\nAIS-BN algorithm.\n"
  },
  {
    "id": "1106.0254",
    "title": "Conflict-Directed Backjumping Revisited",
    "abstract": "  In recent years, many improvements to backtracking algorithms for solving\nconstraint satisfaction problems have been proposed. The techniques for\nimproving backtracking algorithms can be conveniently classified as look-ahead\nschemes and look-back schemes. Unfortunately, look-ahead and look-back schemes\nare not entirely orthogonal as it has been observed empirically that the\nenhancement of look-ahead techniques is sometimes counterproductive to the\neffects of look-back techniques. In this paper, we focus on the relationship\nbetween the two most important look-ahead techniques---using a variable\nordering heuristic and maintaining a level of local consistency during the\nbacktracking search---and the look-back technique of conflict-directed\nbackjumping (CBJ). We show that there exists a \"perfect\" dynamic variable\nordering such that CBJ becomes redundant. We also show theoretically that as\nthe level of local consistency that is maintained in the backtracking search is\nincreased, the less that backjumping will be an improvement. Our theoretical\nresults partially explain why a backtracking algorithm doing more in the\nlook-ahead phase cannot benefit more from the backjumping look-back scheme.\nFinally, we show empirically that adding CBJ to a backtracking algorithm that\nmaintains generalized arc consistency (GAC), an algorithm that we refer to as\nGAC-CBJ, can still provide orders of magnitude speedups. Our empirical results\ncontrast with Bessiere and Regin's conclusion (1996) that CBJ is useless to an\nalgorithm that maintains arc consistency.\n"
  },
  {
    "id": "1106.0256",
    "title": "Grounding the Lexical Semantics of Verbs in Visual Perception using\n  Force Dynamics and Event Logic",
    "abstract": "  This paper presents an implemented system for recognizing the occurrence of\nevents described by simple spatial-motion verbs in short image sequences. The\nsemantics of these verbs is specified with event-logic expressions that\ndescribe changes in the state of force-dynamic relations between the\nparticipants of the event. An efficient finite representation is introduced for\nthe infinite sets of intervals that occur when describing liquid and\nsemi-liquid events. Additionally, an efficient procedure using this\nrepresentation is presented for inferring occurrences of compound events,\ndescribed with event-logic expressions, from occurrences of primitive events.\nUsing force dynamics and event logic to specify the lexical semantics of events\nallows the system to be more robust than prior systems based on motion profile.\n"
  },
  {
    "id": "1106.0257",
    "title": "Popular Ensemble Methods: An Empirical Study",
    "abstract": "  An ensemble consists of a set of individually trained classifiers (such as\nneural networks or decision trees) whose predictions are combined when\nclassifying novel instances. Previous research has shown that an ensemble is\noften more accurate than any of the single classifiers in the ensemble. Bagging\n(Breiman, 1996c) and Boosting (Freund and Shapire, 1996; Shapire, 1990) are two\nrelatively new but popular methods for producing ensembles. In this paper we\nevaluate these methods on 23 data sets using both neural networks and decision\ntrees as our classification algorithm. Our results clearly indicate a number of\nconclusions. First, while Bagging is almost always more accurate than a single\nclassifier, it is sometimes much less accurate than Boosting. On the other\nhand, Boosting can create ensembles that are less accurate than a single\nclassifier -- especially when using neural networks. Analysis indicates that\nthe performance of the Boosting methods is dependent on the characteristics of\nthe data set being examined. In fact, further results show that Boosting\nensembles may overfit noisy data sets, thus decreasing its performance.\nFinally, consistent with previous studies, our work suggests that most of the\ngain in an ensemble's performance comes in the first few classifiers combined;\nhowever, relatively large gains can be seen up to 25 classifiers when Boosting\ndecision trees.\n"
  },
  {
    "id": "1106.0284",
    "title": "An Evolutionary Algorithm with Advanced Goal and Priority Specification\n  for Multi-objective Optimization",
    "abstract": "  This paper presents an evolutionary algorithm with a new goal-sequence\ndomination scheme for better decision support in multi-objective optimization.\nThe approach allows the inclusion of advanced hard/soft priority and constraint\ninformation on each objective component, and is capable of incorporating\nmultiple specifications with overlapping or non-overlapping objective functions\nvia logical 'OR' and 'AND' connectives to drive the search towards multiple\nregions of trade-off. In addition, we propose a dynamic sharing scheme that is\nsimple and adaptively estimated according to the on-line population\ndistribution without needing any a priori parameter setting. Each feature in\nthe proposed algorithm is examined to show its respective contribution, and the\nperformance of the algorithm is compared with other evolutionary optimization\nmethods. It is shown that the proposed algorithm has performed well in the\ndiversity of evolutionary search and uniform distribution of non-dominated\nindividuals along the final trade-offs, without significant computational\neffort. The algorithm is also applied to the design optimization of a practical\nservo control system for hard disk drives with a single voice-coil-motor\nactuator. Results of the evolutionary designed servo control system show a\nsuperior closed-loop performance compared to classical PID or RPT approaches.\n"
  },
  {
    "id": "1106.0285",
    "title": "The GRT Planning System: Backward Heuristic Construction in Forward\n  State-Space Planning",
    "abstract": "  This paper presents GRT, a domain-independent heuristic planning system for\nSTRIPS worlds. GRT solves problems in two phases. In the pre-processing phase,\nit estimates the distance between each fact and the goals of the problem, in a\nbackward direction. Then, in the search phase, these estimates are used in\norder to further estimate the distance between each intermediate state and the\ngoals, guiding so the search process in a forward direction and on a best-first\nbasis. The paper presents the benefits from the adoption of opposite directions\nbetween the preprocessing and the search phases, discusses some difficulties\nthat arise in the pre-processing phase and introduces techniques to cope with\nthem. Moreover, it presents several methods of improving the efficiency of the\nheuristic, by enriching the representation and by reducing the size of the\nproblem. Finally, a method of overcoming local optimal states, based on domain\naxioms, is proposed. According to it, difficult problems are decomposed into\neasier sub-problems that have to be solved sequentially. The performance\nresults from various domains, including those of the recent planning\ncompetitions, show that GRT is among the fastest planners.\n"
  },
  {
    "id": "1106.0664",
    "title": "The Complexity of Reasoning about Spatial Congruence",
    "abstract": "  In the recent literature of Artificial Intelligence, an intensive research\neffort has been spent, for various algebras of qualitative relations used in\nthe representation of temporal and spatial knowledge, on the problem of\nclassifying the computational complexity of reasoning problems for subsets of\nalgebras. The main purpose of these researches is to describe a restricted set\nof maximal tractable subalgebras, ideally in an exhaustive fashion with respect\nto the hosting algebras. In this paper we introduce a novel algebra for\nreasoning about Spatial Congruence, show that the satisfiability problem in the\nspatial algebra MC-4 is NP-complete, and present a complete classification of\ntractability in the algebra, based on the individuation of three maximal\ntractable subclasses, one containing the basic relations. The three algebras\nare formed by 14, 10 and 9 relations out of 16 which form the full algebra.\n"
  },
  {
    "id": "1106.0665",
    "title": "Infinite-Horizon Policy-Gradient Estimation",
    "abstract": "  Gradient-based approaches to direct policy search in reinforcement learning\nhave received much recent attention as a means to solve problems of partial\nobservability and to avoid some of the problems associated with policy\ndegradation in value-function methods. In this paper we introduce GPOMDP, a\nsimulation-based algorithm for generating a {\\em biased} estimate of the\ngradient of the {\\em average reward} in Partially Observable Markov Decision\nProcesses (POMDPs) controlled by parameterized stochastic policies. A similar\nalgorithm was proposed by Kimura, Yamamura, and Kobayashi (1995). The\nalgorithm's chief advantages are that it requires storage of only twice the\nnumber of policy parameters, uses one free parameter $\\beta\\in [0,1)$ (which\nhas a natural interpretation in terms of bias-variance trade-off), and requires\nno knowledge of the underlying state. We prove convergence of GPOMDP, and show\nhow the correct choice of the parameter $\\beta$ is related to the {\\em mixing\ntime} of the controlled POMDP. We briefly describe extensions of GPOMDP to\ncontrolled Markov chains, continuous state, observation and control spaces,\nmultiple-agents, higher-order derivatives, and a version for training\nstochastic policies with internal states. In a companion paper (Baxter,\nBartlett, & Weaver, 2001) we show how the gradient estimates generated by\nGPOMDP can be used in both a traditional stochastic gradient algorithm and a\nconjugate-gradient procedure to find local optima of the average reward\n"
  },
  {
    "id": "1106.0667",
    "title": "Reasoning within Fuzzy Description Logics",
    "abstract": "  Description Logics (DLs) are suitable, well-known, logics for managing\nstructured knowledge. They allow reasoning about individuals and well defined\nconcepts, i.e., set of individuals with common properties. The experience in\nusing DLs in applications has shown that in many cases we would like to extend\ntheir capabilities. In particular, their use in the context of Multimedia\nInformation Retrieval (MIR) leads to the convincement that such DLs should\nallow the treatment of the inherent imprecision in multimedia object content\nrepresentation and retrieval. In this paper we will present a fuzzy extension\nof ALC, combining Zadeh's fuzzy logic with a classical DL. In particular,\nconcepts becomes fuzzy and, thus, reasoning about imprecise concepts is\nsupported. We will define its syntax, its semantics, describe its properties\nand present a constraint propagation calculus for reasoning in it.\n"
  },
  {
    "id": "1106.0668",
    "title": "An Analysis of Reduced Error Pruning",
    "abstract": "  Top-down induction of decision trees has been observed to suffer from the\ninadequate functioning of the pruning phase. In particular, it is known that\nthe size of the resulting tree grows linearly with the sample size, even though\nthe accuracy of the tree does not improve. Reduced Error Pruning is an\nalgorithm that has been used as a representative technique in attempts to\nexplain the problems of decision tree learning. In this paper we present\nanalyses of Reduced Error Pruning in three different settings. First we study\nthe basic algorithmic properties of the method, properties that hold\nindependent of the input decision tree and pruning examples. Then we examine a\nsituation that intuitively should lead to the subtree under consideration to be\nreplaced by a leaf node, one in which the class label and attribute values of\nthe pruning examples are independent of each other. This analysis is conducted\nunder two different assumptions. The general analysis shows that the pruning\nprobability of a node fitting pure noise is bounded by a function that\ndecreases exponentially as the size of the tree grows. In a specific analysis\nwe assume that the examples are distributed uniformly to the tree. This\nassumption lets us approximate the number of subtrees that are pruned because\nthey do not receive any pruning examples. This paper clarifies the different\nvariants of the Reduced Error Pruning algorithm, brings new insight to its\nalgorithmic properties, analyses the algorithm with less imposed assumptions\nthan before, and includes the previously overlooked empty subtrees to the\nanalysis.\n"
  },
  {
    "id": "1106.0669",
    "title": "GIB: Imperfect Information in a Computationally Challenging Game",
    "abstract": "  This paper investigates the problems arising in the construction of a program\nto play the game of contract bridge. These problems include both the difficulty\nof solving the game's perfect information variant, and techniques needed to\naddress the fact that bridge is not, in fact, a perfect information game. GIB,\nthe program being described, involves five separate technical advances:\npartition search, the practical application of Monte Carlo techniques to\nrealistic problems, a focus on achievable sets to solve problems inherent in\nthe Monte Carlo approach, an extension of alpha-beta pruning from total orders\nto arbitrary distributive lattices, and the use of squeaky wheel optimization\nto find approximately optimal solutions to cardplay problems. GIB is currently\nbelieved to be of approximately expert caliber, and is currently the strongest\ncomputer bridge program in the world.\n"
  },
  {
    "id": "1106.0671",
    "title": "Domain Filtering Consistencies",
    "abstract": "  Enforcing local consistencies is one of the main features of constraint\nreasoning. Which level of local consistency should be used when searching for\nsolutions in a constraint network is a basic question. Arc consistency and\npartial forms of arc consistency have been widely studied, and have been known\nfor sometime through the forward checking or the MAC search algorithms. Until\nrecently, stronger forms of local consistency remained limited to those that\nchange the structure of the constraint graph, and thus, could not be used in\npractice, especially on large networks. This paper focuses on the local\nconsistencies that are stronger than arc consistency, without changing the\nstructure of the network, i.e., only removing inconsistent values from the\ndomains. In the last five years, several such local consistencies have been\nproposed by us or by others. We make an overview of all of them, and highlight\nsome relations between them. We compare them both theoretically and\nexperimentally, considering their pruning efficiency and the time required to\nenforce them.\n"
  },
  {
    "id": "1106.0672",
    "title": "Policy Recognition in the Abstract Hidden Markov Model",
    "abstract": "  In this paper, we present a method for recognising an agent's behaviour in\ndynamic, noisy, uncertain domains, and across multiple levels of abstraction.\nWe term this problem on-line plan recognition under uncertainty and view it\ngenerally as probabilistic inference on the stochastic process representing the\nexecution of the agent's plan. Our contributions in this paper are twofold. In\nterms of probabilistic inference, we introduce the Abstract Hidden Markov Model\n(AHMM), a novel type of stochastic processes, provide its dynamic Bayesian\nnetwork (DBN) structure and analyse the properties of this network. We then\ndescribe an application of the Rao-Blackwellised Particle Filter to the AHMM\nwhich allows us to construct an efficient, hybrid inference method for this\nmodel. In terms of plan recognition, we propose a novel plan recognition\nframework based on the AHMM as the plan execution model. The Rao-Blackwellised\nhybrid inference for AHMM can take advantage of the independence properties\ninherent in a model of plan execution, leading to an algorithm for online\nprobabilistic plan recognition that scales well with the number of levels in\nthe plan hierarchy. This illustrates that while stochastic models for plan\nexecution can be complex, they exhibit special structures which, if exploited,\ncan lead to efficient plan recognition algorithms. We demonstrate the\nusefulness of the AHMM framework via a behaviour recognition system in a\ncomplex spatial environment using distributed video surveillance data.\n"
  },
  {
    "id": "1106.0675",
    "title": "The FF Planning System: Fast Plan Generation Through Heuristic Search",
    "abstract": "  We describe and evaluate the algorithmic techniques that are used in the FF\nplanning system. Like the HSP system, FF relies on forward state space search,\nusing a heuristic that estimates goal distances by ignoring delete lists.\nUnlike HSP's heuristic, our method does not assume facts to be independent. We\nintroduce a novel search strategy that combines hill-climbing with systematic\nsearch, and we show how other powerful heuristic information can be extracted\nand used to prune the search space. FF was the most successful automatic\nplanner at the recent AIPS-2000 planning competition. We review the results of\nthe competition, give data for other benchmark domains, and investigate the\nreasons for the runtime performance of FF compared to HSP.\n"
  },
  {
    "id": "1106.0678",
    "title": "ATTac-2000: An Adaptive Autonomous Bidding Agent",
    "abstract": "  The First Trading Agent Competition (TAC) was held from June 22nd to July\n8th, 2000. TAC was designed to create a benchmark problem in the complex domain\nof e-marketplaces and to motivate researchers to apply unique approaches to a\ncommon task. This article describes ATTac-2000, the first-place finisher in\nTAC. ATTac-2000 uses a principled bidding strategy that includes several\nelements of adaptivity. In addition to the success at the competition, isolated\nempirical results are presented indicating the robustness and effectiveness of\nATTac-2000's adaptive strategy.\n"
  },
  {
    "id": "1106.0679",
    "title": "Efficient Methods for Qualitative Spatial Reasoning",
    "abstract": "  The theoretical properties of qualitative spatial reasoning in the RCC8\nframework have been analyzed extensively. However, no empirical investigation\nhas been made yet. Our experiments show that the adaption of the algorithms\nused for qualitative temporal reasoning can solve large RCC8 instances, even if\nthey are in the phase transition region -- provided that one uses the maximal\ntractable subsets of RCC8 that have been identified by us. In particular, we\ndemonstrate that the orthogonal combination of heuristic methods is successful\nin solving almost all apparently hard instances in the phase transition region\nup to a certain size in reasonable time.\n"
  },
  {
    "id": "1106.1510",
    "title": "Towards OWL-based Knowledge Representation in Petrology",
    "abstract": "  This paper presents our work on development of OWL-driven systems for formal\nrepresentation and reasoning about terminological knowledge and facts in\npetrology. The long-term aim of our project is to provide solid foundations for\na large-scale integration of various kinds of knowledge, including basic terms,\nrock classification algorithms, findings and reports. We describe three steps\nwe have taken towards that goal here. First, we develop a semi-automated\nprocedure for transforming a database of igneous rock samples to texts in a\ncontrolled natural language (CNL), and then a collection of OWL ontologies.\nSecond, we create an OWL ontology of important petrology terms currently\ndescribed in natural language thesauri. We describe a prototype of a tool for\ncollecting definitions from domain experts. Third, we present an approach to\nformalization of current industrial standards for classification of rock\nsamples, which requires linear equations in OWL 2. In conclusion, we discuss a\nrange of opportunities arising from the use of semantic technologies in\npetrology and outline the future work in this area.\n"
  },
  {
    "id": "1106.1796",
    "title": "Accelerating Reinforcement Learning by Composing Solutions of\n  Automatically Identified Subtasks",
    "abstract": "  This paper discusses a system that accelerates reinforcement learning by\nusing transfer from related tasks. Without such transfer, even if two tasks are\nvery similar at some abstract level, an extensive re-learning effort is\nrequired. The system achieves much of its power by transferring parts of\npreviously learned solutions rather than a single complete solution. The system\nexploits strong features in the multi-dimensional function produced by\nreinforcement learning in solving a particular task. These features are stable\nand easy to recognize early in the learning process. They generate a\npartitioning of the state space and thus the function. The partition is\nrepresented as a graph. This is used to index and compose functions stored in a\ncase base to form a close approximation to the solution of the new task.\nExperiments demonstrate that function composition often produces more than an\norder of magnitude increase in learning rate compared to a basic reinforcement\nlearning algorithm.\n"
  },
  {
    "id": "1106.1797",
    "title": "Parameter Learning of Logic Programs for Symbolic-Statistical Modeling",
    "abstract": "  We propose a logical/mathematical framework for statistical parameter\nlearning of parameterized logic programs, i.e. definite clause programs\ncontaining probabilistic facts with a parameterized distribution. It extends\nthe traditional least Herbrand model semantics in logic programming to\ndistribution semantics, possible world semantics with a probability\ndistribution which is unconditionally applicable to arbitrary logic programs\nincluding ones for HMMs, PCFGs and Bayesian networks. We also propose a new EM\nalgorithm, the graphical EM algorithm, that runs for a class of parameterized\nlogic programs representing sequential decision processes where each decision\nis exclusive and independent. It runs on a new data structure called support\ngraphs describing the logical relationship between observations and their\nexplanations, and learns parameters by computing inside and outside probability\ngeneralized for logic programs. The complexity analysis shows that when\ncombined with OLDT search for all explanations for observations, the graphical\nEM algorithm, despite its generality, has the same time complexity as existing\nEM algorithms, i.e. the Baum-Welch algorithm for HMMs, the Inside-Outside\nalgorithm for PCFGs, and the one for singly connected Bayesian networks that\nhave been developed independently in each research field. Learning experiments\nwith PCFGs using two corpora of moderate size indicate that the graphical EM\nalgorithm can significantly outperform the Inside-Outside algorithm.\n"
  },
  {
    "id": "1106.1799",
    "title": "Finding a Path is Harder than Finding a Tree",
    "abstract": "  I consider the problem of learning an optimal path graphical model from data\nand show the problem to be NP-hard for the maximum likelihood and minimum\ndescription length approaches and a Bayesian approach. This hardness result\nholds despite the fact that the problem is a restriction of the polynomially\nsolvable problem of finding the optimal tree graphical model.\n"
  },
  {
    "id": "1106.1800",
    "title": "Extensions of Simple Conceptual Graphs: the Complexity of Rules and\n  Constraints",
    "abstract": "  Simple conceptual graphs are considered as the kernel of most knowledge\nrepresentation formalisms built upon Sowa's model. Reasoning in this model can\nbe expressed by a graph homomorphism called projection, whose semantics is\nusually given in terms of positive, conjunctive, existential FOL. We present\nhere a family of extensions of this model, based on rules and constraints,\nkeeping graph homomorphism as the basic operation. We focus on the formal\ndefinitions of the different models obtained, including their operational\nsemantics and relationships with FOL, and we analyze the decidability and\ncomplexity of the associated problems (consistency and deduction). As soon as\nrules are involved in reasonings, these problems are not decidable, but we\nexhibit a condition under which they fall in the polynomial hierarchy. These\nresults extend and complete the ones already published by the authors. Moreover\nwe systematically study the complexity of some particular cases obtained by\nrestricting the form of constraints and/or rules.\n"
  },
  {
    "id": "1106.1802",
    "title": "Fusions of Description Logics and Abstract Description Systems",
    "abstract": "  Fusions are a simple way of combining logics. For normal modal logics,\nfusions have been investigated in detail. In particular, it is known that,\nunder certain conditions, decidability transfers from the component logics to\ntheir fusion. Though description logics are closely related to modal logics,\nthey are not necessarily normal. In addition, ABox reasoning in description\nlogics is not covered by the results from modal logics. In this paper, we\nextend the decidability transfer results from normal modal logics to a large\nclass of description logics. To cover different description logics in a uniform\nway, we introduce abstract description systems, which can be seen as a common\ngeneralization of description and modal logics, and show the transfer results\nin this general setting.\n"
  },
  {
    "id": "1106.1803",
    "title": "Improving the Efficiency of Inductive Logic Programming Through the Use\n  of Query Packs",
    "abstract": "  Inductive logic programming, or relational learning, is a powerful paradigm\nfor machine learning or data mining. However, in order for ILP to become\npractically useful, the efficiency of ILP systems must improve substantially.\nTo this end, the notion of a query pack is introduced: it structures sets of\nsimilar queries. Furthermore, a mechanism is described for executing such query\npacks. A complexity analysis shows that considerable efficiency improvements\ncan be achieved through the use of this query pack execution mechanism. This\nclaim is supported by empirical results obtained by incorporating support for\nquery pack execution in two existing learning systems.\n"
  },
  {
    "id": "1106.1804",
    "title": "A Critical Assessment of Benchmark Comparison in Planning",
    "abstract": "  Recent trends in planning research have led to empirical comparison becoming\ncommonplace. The field has started to settle into a methodology for such\ncomparisons, which for obvious practical reasons requires running a subset of\nplanners on a subset of problems. In this paper, we characterize the\nmethodology and examine eight implicit assumptions about the problems, planners\nand metrics used in many of these comparisons. The problem assumptions are:\nPR1) the performance of a general purpose planner should not be\npenalized/biased if executed on a sampling of problems and domains, PR2) minor\nsyntactic differences in representation do not affect performance, and PR3)\nproblems should be solvable by STRIPS capable planners unless they require ADL.\nThe planner assumptions are: PL1) the latest version of a planner is the best\none to use, PL2) default parameter settings approximate good performance, and\nPL3) time cut-offs do not unduly bias outcome. The metrics assumptions are: M1)\nperformance degrades similarly for each planner when run on degraded runtime\nenvironments (e.g., machine platform) and M2) the number of plan steps\ndistinguishes performance. We find that most of these assumptions are not\nsupported empirically; in particular, that planners are affected differently by\nthese assumptions. We conclude with a call to the community to devote research\nresources to improving the state of the practice and especially to enhancing\nthe available benchmark problems.\n"
  },
  {
    "id": "1106.1813",
    "title": "SMOTE: Synthetic Minority Over-sampling Technique",
    "abstract": "  An approach to the construction of classifiers from imbalanced datasets is\ndescribed. A dataset is imbalanced if the classification categories are not\napproximately equally represented. Often real-world data sets are predominately\ncomposed of \"normal\" examples with only a small percentage of \"abnormal\" or\n\"interesting\" examples. It is also the case that the cost of misclassifying an\nabnormal (interesting) example as a normal example is often much higher than\nthe cost of the reverse error. Under-sampling of the majority (normal) class\nhas been proposed as a good means of increasing the sensitivity of a classifier\nto the minority class. This paper shows that a combination of our method of\nover-sampling the minority (abnormal) class and under-sampling the majority\n(normal) class can achieve better classifier performance (in ROC space) than\nonly under-sampling the majority class. This paper also shows that a\ncombination of our method of over-sampling the minority class and\nunder-sampling the majority class can achieve better classifier performance (in\nROC space) than varying the loss ratios in Ripper or class priors in Naive\nBayes. Our method of over-sampling the minority class involves creating\nsynthetic minority class examples. Experiments are performed using C4.5, Ripper\nand a Naive Bayes classifier. The method is evaluated using the area under the\nReceiver Operating Characteristic curve (AUC) and the ROC convex hull strategy.\n"
  },
  {
    "id": "1106.1814",
    "title": "When do Numbers Really Matter?",
    "abstract": "  Common wisdom has it that small distinctions in the probabilities\n(parameters) quantifying a belief network do not matter much for the results of\nprobabilistic queries. Yet, one can develop realistic scenarios under which\nsmall variations in network parameters can lead to significant changes in\ncomputed queries. A pending theoretical question is then to analytically\ncharacterize parameter changes that do or do not matter. In this paper, we\nstudy the sensitivity of probabilistic queries to changes in network parameters\nand prove some tight bounds on the impact that such parameters can have on\nqueries. Our analytic results pinpoint some interesting situations under which\nparameter changes do or do not matter. These results are important for\nknowledge engineers as they help them identify influential network parameters.\nThey also help explain some of the previous experimental results and\nobservations with regards to network robustness against parameter changes.\n"
  },
  {
    "id": "1106.1816",
    "title": "Monitoring Teams by Overhearing: A Multi-Agent Plan-Recognition Approach",
    "abstract": "  Recent years are seeing an increasing need for on-line monitoring of teams of\ncooperating agents, e.g., for visualization, or performance tracking. However,\nin monitoring deployed teams, we often cannot rely on the agents to always\ncommunicate their state to the monitoring system. This paper presents a\nnon-intrusive approach to monitoring by 'overhearing', where the monitored\nteam's state is inferred (via plan-recognition) from team-members' routine\ncommunications, exchanged as part of their coordinated task execution, and\nobserved (overheard) by the monitoring system. Key challenges in this approach\ninclude the demanding run-time requirements of monitoring, the scarceness of\nobservations (increasing monitoring uncertainty), and the need to scale-up\nmonitoring to address potentially large teams. To address these, we present a\nset of complementary novel techniques, exploiting knowledge of the social\nstructures and procedures in the monitored team: (i) an efficient probabilistic\nplan-recognition algorithm, well-suited for processing communications as\nobservations; (ii) an approach to exploiting knowledge of the team's social\nbehavior to predict future observations during execution (reducing monitoring\nuncertainty); and (iii) monitoring algorithms that trade expressivity for\nscalability, representing only certain useful monitoring hypotheses, but\nallowing for any number of agents and their different activities to be\nrepresented in a single coherent entity. We present an empirical evaluation of\nthese techniques, in combination and apart, in monitoring a deployed team of\nagents, running on machines physically distributed across the country, and\nengaged in complex, dynamic task execution. We also compare the performance of\nthese techniques to human expert and novice monitors, and show that the\ntechniques presented are capable of monitoring at human-expert levels, despite\nthe difficulty of the task.\n"
  },
  {
    "id": "1106.1817",
    "title": "Automatically Training a Problematic Dialogue Predictor for a Spoken\n  Dialogue System",
    "abstract": "  Spoken dialogue systems promise efficient and natural access to a large\nvariety of information sources and services from any phone. However, current\nspoken dialogue systems are deficient in their strategies for preventing,\nidentifying and repairing problems that arise in the conversation. This paper\nreports results on automatically training a Problematic Dialogue Predictor to\npredict problematic human-computer dialogues using a corpus of 4692 dialogues\ncollected with the 'How May I Help You' (SM) spoken dialogue system. The\nProblematic Dialogue Predictor can be immediately applied to the system's\ndecision of whether to transfer the call to a human customer care agent, or be\nused as a cue to the system's dialogue manager to modify its behavior to repair\nproblems, and even perhaps, to prevent them. We show that a Problematic\nDialogue Predictor using automatically-obtainable features from the first two\nexchanges in the dialogue can predict problematic dialogues 13.2% more\naccurately than the baseline.\n"
  },
  {
    "id": "1106.1818",
    "title": "Inducing Interpretable Voting Classifiers without Trading Accuracy for\n  Simplicity: Theoretical Results, Approximation Algorithms",
    "abstract": "  Recent advances in the study of voting classification algorithms have brought\nempirical and theoretical results clearly showing the discrimination power of\nensemble classifiers. It has been previously argued that the search of this\nclassification power in the design of the algorithms has marginalized the need\nto obtain interpretable classifiers. Therefore, the question of whether one\nmight have to dispense with interpretability in order to keep classification\nstrength is being raised in a growing number of machine learning or data mining\npapers. The purpose of this paper is to study both theoretically and\nempirically the problem. First, we provide numerous results giving insight into\nthe hardness of the simplicity-accuracy tradeoff for voting classifiers. Then\nwe provide an efficient \"top-down and prune\" induction heuristic, WIDC, mainly\nderived from recent results on the weak learning and boosting frameworks. It is\nto our knowledge the first attempt to build a voting classifier as a base\nformula using the weak learning framework (the one which was previously highly\nsuccessful for decision tree induction), and not the strong learning framework\n(as usual for such classifiers with boosting-like approaches). While it uses a\nwell-known induction scheme previously successful in other classes of concept\nrepresentations, thus making it easy to implement and compare, WIDC also relies\non recent or new results we give about particular cases of boosting known as\npartition boosting and ranking loss boosting. Experimental results on\nthirty-one domains, most of which readily available, tend to display the\nability of WIDC to produce small, accurate, and interpretable decision\ncommittees.\n"
  },
  {
    "id": "1106.1819",
    "title": "A Knowledge Compilation Map",
    "abstract": "  We propose a perspective on knowledge compilation which calls for analyzing\ndifferent compilation approaches according to two key dimensions: the\nsuccinctness of the target compilation language, and the class of queries and\ntransformations that the language supports in polytime. We then provide a\nknowledge compilation map, which analyzes a large number of existing target\ncompilation languages according to their succinctness and their polytime\ntransformations and queries. We argue that such analysis is necessary for\nplacing new compilation approaches within the context of existing ones. We also\ngo beyond classical, flat target compilation languages based on CNF and DNF,\nand consider a richer, nested class based on directed acyclic graphs (such as\nOBDDs), which we show to include a relatively large number of target\ncompilation languages.\n"
  },
  {
    "id": "1106.1820",
    "title": "Inferring Strategies for Sentence Ordering in Multidocument News\n  Summarization",
    "abstract": "  The problem of organizing information for multidocument summarization so that\nthe generated summary is coherent has received relatively little attention.\nWhile sentence ordering for single document summarization can be determined\nfrom the ordering of sentences in the input article, this is not the case for\nmultidocument summarization where summary sentences may be drawn from different\ninput articles. In this paper, we propose a methodology for studying the\nproperties of ordering information in the news genre and describe experiments\ndone on a corpus of multiple acceptable orderings we developed for the task.\nBased on these experiments, we implemented a strategy for ordering information\nthat combines constraints from chronological order of events and topical\nrelatedness. Evaluation of our augmented algorithm shows a significant\nimprovement of the ordering over two baseline strategies.\n"
  },
  {
    "id": "1106.1821",
    "title": "Collective Intelligence, Data Routing and Braess' Paradox",
    "abstract": "  We consider the problem of designing the the utility functions of the\nutility-maximizing agents in a multi-agent system so that they work\nsynergistically to maximize a global utility. The particular problem domain we\nexplore is the control of network routing by placing agents on all the routers\nin the network. Conventional approaches to this task have the agents all use\nthe Ideal Shortest Path routing Algorithm (ISPA). We demonstrate that in many\ncases, due to the side-effects of one agent's actions on another agent's\nperformance, having agents use ISPA's is suboptimal as far as global aggregate\ncost is concerned, even when they are only used to route infinitesimally small\namounts of traffic. The utility functions of the individual agents are not\n\"aligned\" with the global utility, intuitively speaking. As a particular\nexample of this we present an instance of Braess' paradox in which adding new\nlinks to a network whose agents all use the ISPA results in a decrease in\noverall throughput. We also demonstrate that load-balancing, in which the\nagents' decisions are collectively made to optimize the global cost incurred by\nall traffic currently being routed, is suboptimal as far as global cost\naveraged across time is concerned. This is also due to 'side-effects', in this\ncase of current routing decision on future traffic. The mathematics of\nCollective Intelligence (COIN) is concerned precisely with the issue of\navoiding such deleterious side-effects in multi-agent systems, both over time\nand space. We present key concepts from that mathematics and use them to derive\nan algorithm whose ideal version should have better performance than that of\nhaving all agents use the ISPA, even in the infinitesimal limit. We present\nexperiments verifying this, and also showing that a machine-learning-based\nversion of this COIN algorithm in which costs are only imprecisely estimated\nvia empirical means (a version potentially applicable in the real world) also\noutperforms the ISPA, despite having access to less information than does the\nISPA. In particular, this COIN algorithm almost always avoids Braess' paradox.\n"
  },
  {
    "id": "1106.1822",
    "title": "Efficient Solution Algorithms for Factored MDPs",
    "abstract": "  This paper addresses the problem of planning under uncertainty in large\nMarkov Decision Processes (MDPs). Factored MDPs represent a complex state space\nusing state variables and the transition model using a dynamic Bayesian\nnetwork. This representation often allows an exponential reduction in the\nrepresentation size of structured MDPs, but the complexity of exact solution\nalgorithms for such MDPs can grow exponentially in the representation size. In\nthis paper, we present two approximate solution algorithms that exploit\nstructure in factored MDPs. Both use an approximate value function represented\nas a linear combination of basis functions, where each basis function involves\nonly a small subset of the domain variables. A key contribution of this paper\nis that it shows how the basic operations of both algorithms can be performed\nefficiently in closed form, by exploiting both additive and context-specific\nstructure in a factored MDP. A central element of our algorithms is a novel\nlinear program decomposition technique, analogous to variable elimination in\nBayesian networks, which reduces an exponentially large LP to a provably\nequivalent, polynomial-sized one. One algorithm uses approximate linear\nprogramming, and the second approximate dynamic programming. Our dynamic\nprogramming algorithm is novel in that it uses an approximation based on\nmax-norm, a technique that more directly minimizes the terms that appear in\nerror bounds for approximate MDP algorithms. We provide experimental results on\nproblems with over 10^40 states, demonstrating a promising indication of the\nscalability of our approach, and compare our algorithm to an existing\nstate-of-the-art approach, showing, in some problems, exponential gains in\ncomputation time.\n"
  },
  {
    "id": "1106.1853",
    "title": "Intelligent decision: towards interpreting the Pe Algorithm",
    "abstract": "  The human intelligence lies in the algorithm, the nature of algorithm lies in\nthe classification, and the classification is equal to outlier detection. A lot\nof algorithms have been proposed to detect outliers, meanwhile a lot of\ndefinitions. Unsatisfying point is that definitions seem vague, which makes the\nsolution an ad hoc one. We analyzed the nature of outliers, and give two clear\ndefinitions. We then develop an efficient RDD algorithm, which converts outlier\nproblem to pattern and degree problem. Furthermore, a collapse mechanism was\nintroduced by IIR algorithm, which can be united seamlessly with the RDD\nalgorithm and serve for the final decision. Both algorithms are originated from\nthe study on general AI. The combined edition is named as Pe algorithm, which\nis the basis of the intelligent decision. Here we introduce longest k-turn\nsubsequence problem and corresponding solution as an example to interpret the\nfunction of Pe algorithm in detecting curve-type outliers. We also give a\ncomparison between IIR algorithm and Pe algorithm, where we can get a better\nunderstanding at both algorithms. A short discussion about intelligence is\nadded to demonstrate the function of the Pe algorithm. Related experimental\nresults indicate its robustness.\n"
  },
  {
    "id": "1106.1998",
    "title": "A Linear Time Natural Evolution Strategy for Non-Separable Functions",
    "abstract": "  We present a novel Natural Evolution Strategy (NES) variant, the Rank-One NES\n(R1-NES), which uses a low rank approximation of the search distribution\ncovariance matrix. The algorithm allows computation of the natural gradient\nwith cost linear in the dimensionality of the parameter space, and excels in\nsolving high-dimensional non-separable problems, including the best result to\ndate on the Rosenbrock function (512 dimensions).\n"
  },
  {
    "id": "1106.2647",
    "title": "From Causal Models To Counterfactual Structures",
    "abstract": "  Galles and Pearl claimed that \"for recursive models, the causal model\nframework does not add any restrictions to counterfactuals, beyond those\nimposed by Lewis's [possible-worlds] framework.\" This claim is examined\ncarefully, with the goal of clarifying the exact relationship between causal\nmodels and Lewis's framework. Recursive models are shown to correspond\nprecisely to a subclass of (possible-world) counterfactual structures. On the\nother hand, a slight generalization of recursive models, models where all\nequations have unique solutions, is shown to be incomparable in expressive\npower to counterfactual structures, despite the fact that the Galles and Pearl\narguments should apply to them as well. The problem with the Galles and Pearl\nargument is identified: an axiom that they viewed as irrelevant, because it\ninvolved disjunction (which was not in their language), is not irrelevant at\nall.\n"
  },
  {
    "id": "1106.2652",
    "title": "Actual causation and the art of modeling",
    "abstract": "  We look more carefully at the modeling of causality using structural\nequations. It is clear that the structural equations can have a major impact on\nthe conclusions we draw about causality. In particular, the choice of variables\nand their values can also have a significant impact on causality. These choices\nare, to some extent, subjective. We consider what counts as an appropriate\nchoice. More generally, we consider what makes a model an appropriate model,\nespecially if we want to take defaults into account, as was argued is necessary\nin recent work.\n"
  },
  {
    "id": "1106.2692",
    "title": "Generating Schemata of Resolution Proofs",
    "abstract": "  Two distinct algorithms are presented to extract (schemata of) resolution\nproofs from closed tableaux for propositional schemata. The first one handles\nthe most efficient version of the tableau calculus but generates very complex\nderivations (denoted by rather elaborate rewrite systems). The second one has\nthe advantage that much simpler systems can be obtained, however the considered\nproof procedure is less efficient.\n"
  },
  {
    "id": "1106.3361",
    "title": "Random forest models of the retention constants in the thin layer\n  chromatography",
    "abstract": "  In the current study we examine an application of the machine learning\nmethods to model the retention constants in the thin layer chromatography\n(TLC). This problem can be described with hundreds or even thousands of\ndescriptors relevant to various molecular properties, most of them redundant\nand not relevant for the retention constant prediction. Hence we employed\nfeature selection to significantly reduce the number of attributes.\nAdditionally we have tested application of the bagging procedure to the feature\nselection. The random forest regression models were built using selected\nvariables. The resulting models have better correlation with the experimental\ndata than the reference models obtained with linear regression. The\ncross-validation confirms robustness of the models.\n"
  },
  {
    "id": "1106.3876",
    "title": "Uncertainty in Ontologies: Dempster-Shafer Theory for Data Fusion\n  Applications",
    "abstract": "  Nowadays ontologies present a growing interest in Data Fusion applications.\nAs a matter of fact, the ontologies are seen as a semantic tool for describing\nand reasoning about sensor data, objects, relations and general domain\ntheories. In addition, uncertainty is perhaps one of the most important\ncharacteristics of the data and information handled by Data Fusion. However,\nthe fundamental nature of ontologies implies that ontologies describe only\nasserted and veracious facts of the world. Different probabilistic, fuzzy and\nevidential approaches already exist to fill this gap; this paper recaps the\nmost popular tools. However none of the tools meets exactly our purposes.\nTherefore, we constructed a Dempster-Shafer ontology that can be imported into\nany specific domain ontology and that enables us to instantiate it in an\nuncertain manner. We also developed a Java application that enables reasoning\nabout these uncertain ontological instances.\n"
  },
  {
    "id": "1106.3932",
    "title": "Coincidences and the encounter problem: A formal account",
    "abstract": "  Individuals have an intuitive perception of what makes a good coincidence.\nThough the sensitivity to coincidences has often been presented as resulting\nfrom an erroneous assessment of probability, it appears to be a genuine\ncompetence, based on non-trivial computations. The model presented here\nsuggests that coincidences occur when subjects perceive complexity drops.\nCo-occurring events are, together, simpler than if considered separately. This\nmodel leads to a possible redefinition of subjective probability.\n"
  },
  {
    "id": "1106.4218",
    "title": "Rooting opinions in the minds: a cognitive model and a formal account of\n  opinions and their dynamics",
    "abstract": "  The study of opinions, their formation and change, is one of the defining\ntopics addressed by social psychology, but in recent years other disciplines,\nlike computer science and complexity, have tried to deal with this issue.\nDespite the flourishing of different models and theories in both fields,\nseveral key questions still remain unanswered. The understanding of how\nopinions change and the way they are affected by social influence are\nchallenging issues requiring a thorough analysis of opinion per se but also of\nthe way in which they travel between agents' minds and are modulated by these\nexchanges. To account for the two-faceted nature of opinions, which are mental\nentities undergoing complex social processes, we outline a preliminary model in\nwhich a cognitive theory of opinions is put forward and it is paired with a\nformal description of them and of their spreading among minds. Furthermore,\ninvestigating social influence also implies the necessity to account for the\nway in which people change their minds, as a consequence of interacting with\nother people, and the need to explain the higher or lower persistence of such\nchanges.\n"
  },
  {
    "id": "1106.4221",
    "title": "Understanding opinions. A cognitive and formal account",
    "abstract": "  The study of opinions, their formation and change, is one of the defining\ntopics addressed by social psychology, but in recent years other disciplines,\nas computer science and complexity, have addressed this challenge. Despite the\nflourishing of different models and theories in both fields, several key\nquestions still remain unanswered. The aim of this paper is to challenge the\ncurrent theories on opinion by putting forward a cognitively grounded model\nwhere opinions are described as specific mental representations whose main\nproperties are put forward. A comparison with reputation will be also\npresented.\n"
  },
  {
    "id": "1106.4557",
    "title": "Learning When Training Data are Costly: The Effect of Class Distribution\n  on Tree Induction",
    "abstract": "  For large, real-world inductive learning problems, the number of training\nexamples often must be limited due to the costs associated with procuring,\npreparing, and storing the training examples and/or the computational costs\nassociated with learning from them. In such circumstances, one question of\npractical importance is: if only n training examples can be selected, in what\nproportion should the classes be represented? In this article we help to answer\nthis question by analyzing, for a fixed training-set size, the relationship\nbetween the class distribution of the training data and the performance of\nclassification trees induced from these data. We study twenty-six data sets\nand, for each, determine the best class distribution for learning. The\nnaturally occurring class distribution is shown to generally perform well when\nclassifier performance is evaluated using undifferentiated error rate (0/1\nloss). However, when the area under the ROC curve is used to evaluate\nclassifier performance, a balanced distribution is shown to perform well. Since\nneither of these choices for class distribution always generates the\nbest-performing classifier, we introduce a budget-sensitive progressive\nsampling algorithm for selecting training examples based on the class\nassociated with each example. An empirical analysis of this algorithm shows\nthat the class distribution of the resulting training set yields classifiers\nwith good (nearly-optimal) classification performance.\n"
  },
  {
    "id": "1106.4561",
    "title": "PDDL2.1: An Extension to PDDL for Expressing Temporal Planning Domains",
    "abstract": "  In recent years research in the planning community has moved increasingly\ntoward s application of planners to realistic problems involving both time and\nmany typ es of resources. For example, interest in planning demonstrated by the\nspace res earch community has inspired work in observation scheduling,\nplanetary rover ex ploration and spacecraft control domains. Other temporal and\nresource-intensive domains including logistics planning, plant control and\nmanufacturing have also helped to focus the community on the modelling and\nreasoning issues that must be confronted to make planning technology meet the\nchallenges of application. The International Planning Competitions have acted\nas an important motivating fo rce behind the progress that has been made in\nplanning since 1998. The third com petition (held in 2002) set the planning\ncommunity the challenge of handling tim e and numeric resources. This\nnecessitated the development of a modelling langua ge capable of expressing\ntemporal and numeric properties of planning domains. In this paper we describe\nthe language, PDDL2.1, that was used in the competition. We describe the syntax\nof the language, its formal semantics and the validation of concurrent plans.\nWe observe that PDDL2.1 has considerable modelling power --- exceeding the\ncapabilities of current planning technology --- and presents a number of\nimportant challenges to the research community.\n"
  },
  {
    "id": "1106.4569",
    "title": "The Communicative Multiagent Team Decision Problem: Analyzing Teamwork\n  Theories and Models",
    "abstract": "  Despite the significant progress in multiagent teamwork, existing research\ndoes not address the optimality of its prescriptions nor the complexity of the\nteamwork problem. Without a characterization of the optimality-complexity\ntradeoffs, it is impossible to determine whether the assumptions and\napproximations made by a particular theory gain enough efficiency to justify\nthe losses in overall performance. To provide a tool for use by multiagent\nresearchers in evaluating this tradeoff, we present a unified framework, the\nCOMmunicative Multiagent Team Decision Problem (COM-MTDP). The COM-MTDP model\ncombines and extends existing multiagent theories, such as decentralized\npartially observable Markov decision processes and economic team theory. In\naddition to their generality of representation, COM-MTDPs also support the\nanalysis of both the optimality of team performance and the computational\ncomplexity of the agents' decision problem. In analyzing complexity, we present\na breakdown of the computational complexity of constructing optimal teams under\nvarious classes of problem domains, along the dimensions of observability and\ncommunication cost. In analyzing optimality, we exploit the COM-MTDP's ability\nto encode existing teamwork theories and models to encode two instantiations of\njoint intentions theory taken from the literature. Furthermore, the COM-MTDP\nmodel provides a basis for the development of novel team coordination\nalgorithms. We derive a domain-independent criterion for optimal communication\nand provide a comparative analysis of the two joint intentions instantiations\nwith respect to this optimal policy. We have implemented a reusable,\ndomain-independent software package based on COM-MTDPs to analyze teamwork\ncoordination strategies, and we demonstrate its use by encoding and evaluating\nthe two joint intentions strategies within an example domain.\n"
  },
  {
    "id": "1106.4573",
    "title": "Towards Adjustable Autonomy for the Real World",
    "abstract": "  Adjustable autonomy refers to entities dynamically varying their own\nautonomy, transferring decision-making control to other entities (typically\nagents transferring control to human users) in key situations. Determining\nwhether and when such transfers-of-control should occur is arguably the\nfundamental research problem in adjustable autonomy. Previous work has\ninvestigated various approaches to addressing this problem but has often\nfocused on individual agent-human interactions. Unfortunately, domains\nrequiring collaboration between teams of agents and humans reveal two key\nshortcomings of these previous approaches. First, these approaches use rigid\none-shot transfers of control that can result in unacceptable coordination\nfailures in multiagent settings. Second, they ignore costs (e.g., in terms of\ntime delays or effects on actions) to an agent's team due to such\ntransfers-of-control. To remedy these problems, this article presents a novel\napproach to adjustable autonomy, based on the notion of a transfer-of-control\nstrategy. A transfer-of-control strategy consists of a conditional sequence of\ntwo types of actions: (i) actions to transfer decision-making control (e.g.,\nfrom an agent to a user or vice versa) and (ii) actions to change an agent's\npre-specified coordination constraints with team members, aimed at minimizing\nmiscoordination costs. The goal is for high-quality individual decisions to be\nmade with minimal disruption to the coordination of the team. We present a\nmathematical model of transfer-of-control strategies. The model guides and\ninforms the operationalization of the strategies using Markov Decision\nProcesses, which select an optimal strategy, given an uncertain environment and\ncosts to the individuals and teams. The approach has been carefully evaluated,\nincluding via its use in a real-world, deployed multi-agent system that assists\na research group in its daily activities.\n"
  },
  {
    "id": "1106.4575",
    "title": "An Analysis of Phase Transition in NK Landscapes",
    "abstract": "  In this paper, we analyze the decision version of the NK landscape model from\nthe perspective of threshold phenomena and phase transitions under two random\ndistributions, the uniform probability model and the fixed ratio model. For the\nuniform probability model, we prove that the phase transition is easy in the\nsense that there is a polynomial algorithm that can solve a random instance of\nthe problem with the probability asymptotic to 1 as the problem size tends to\ninfinity. For the fixed ratio model, we establish several upper bounds for the\nsolubility threshold, and prove that random instances with parameters above\nthese upper bounds can be solved polynomially. This, together with our\nempirical study for random instances generated below and in the phase\ntransition region, suggests that the phase transition of the fixed ratio model\nis also easy.\n"
  },
  {
    "id": "1106.4576",
    "title": "Expert-Guided Subgroup Discovery: Methodology and Application",
    "abstract": "  This paper presents an approach to expert-guided subgroup discovery. The main\nstep of the subgroup discovery process, the induction of subgroup descriptions,\nis performed by a heuristic beam search algorithm, using a novel parametrized\ndefinition of rule quality which is analyzed in detail. The other important\nsteps of the proposed subgroup discovery process are the detection of\nstatistically significant properties of selected subgroups and subgroup\nvisualization: statistically significant properties are used to enrich the\ndescriptions of induced subgroups, while the visualization shows subgroup\nproperties in the form of distributions of the numbers of examples in the\nsubgroups. The approach is illustrated by the results obtained for a medical\nproblem of early detection of patient risk groups.\n"
  },
  {
    "id": "1106.4578",
    "title": "Propositional Independence - Formula-Variable Independence and\n  Forgetting",
    "abstract": "  Independence -- the study of what is relevant to a given problem of reasoning\n-- has received an increasing attention from the AI community. In this paper,\nwe consider two basic forms of independence, namely, a syntactic one and a\nsemantic one. We show features and drawbacks of them. In particular, while the\nsyntactic form of independence is computationally easy to check, there are\ncases in which things that intuitively are not relevant are not recognized as\nsuch. We also consider the problem of forgetting, i.e., distilling from a\nknowledge base only the part that is relevant to the set of queries constructed\nfrom a subset of the alphabet. While such process is computationally hard, it\nallows for a simplification of subsequent reasoning, and can thus be viewed as\na form of compilation: once the relevant part of a knowledge base has been\nextracted, all reasoning tasks to be performed can be simplified.\n"
  },
  {
    "id": "1106.4863",
    "title": "Monte Carlo Methods for Tempo Tracking and Rhythm Quantization",
    "abstract": "  We present a probabilistic generative model for timing deviations in\nexpressive music performance. The structure of the proposed model is equivalent\nto a switching state space model. The switch variables correspond to discrete\nnote locations as in a musical score. The continuous hidden variables denote\nthe tempo. We formulate two well known music recognition problems, namely tempo\ntracking and automatic transcription (rhythm quantization) as filtering and\nmaximum a posteriori (MAP) state estimation tasks. Exact computation of\nposterior features such as the MAP state is intractable in this model class, so\nwe introduce Monte Carlo methods for integration and optimization. We compare\nMarkov Chain Monte Carlo (MCMC) methods (such as Gibbs sampling, simulated\nannealing and iterative improvement) and sequential Monte Carlo methods\n(particle filters). Our simulation results suggest better results with\nsequential methods. The methods can be applied in both online and batch\nscenarios such as tempo tracking and transcription and are thus potentially\nuseful in a number of music applications such as adaptive automatic\naccompaniment, score typesetting and music information retrieval.\n"
  },
  {
    "id": "1106.4864",
    "title": "Exploiting Contextual Independence In Probabilistic Inference",
    "abstract": "  Bayesian belief networks have grown to prominence because they provide\ncompact representations for many problems for which probabilistic inference is\nappropriate, and there are algorithms to exploit this compactness. The next\nstep is to allow compact representations of the conditional probabilities of a\nvariable given its parents. In this paper we present such a representation that\nexploits contextual independence in terms of parent contexts; which variables\nact as parents may depend on the value of other variables. The internal\nrepresentation is in terms of contextual factors (confactors) that is simply a\npair of a context and a table. The algorithm, contextual variable elimination,\nis based on the standard variable elimination algorithm that eliminates the\nnon-query variables in turn, but when eliminating a variable, the tables that\nneed to be multiplied can depend on the context. This algorithm reduces to\nstandard variable elimination when there is no contextual independence\nstructure to exploit. We show how this can be much more efficient than variable\nelimination when there is structure to exploit. We explain why this new method\ncan exploit more structure than previous methods for structured belief network\ninference and an analogous algorithm that uses trees.\n"
  },
  {
    "id": "1106.4865",
    "title": "Bound Propagation",
    "abstract": "  In this article we present an algorithm to compute bounds on the marginals of\na graphical model. For several small clusters of nodes upper and lower bounds\non the marginal values are computed independently of the rest of the network.\nThe range of allowed probability distributions over the surrounding nodes is\nrestricted using earlier computed bounds. As we will show, this can be\nconsidered as a set of constraints in a linear programming problem of which the\nobjective function is the marginal probability of the center nodes. In this way\nknowledge about the maginals of neighbouring clusters is passed to other\nclusters thereby tightening the bounds on their marginals. We show that sharp\nbounds can be obtained for undirected and directed graphs that are used for\npractical applications, but for which exact computations are infeasible.\n"
  },
  {
    "id": "1106.4866",
    "title": "On Polynomial Sized MDP Succinct Policies",
    "abstract": "  Policies of Markov Decision Processes (MDPs) determine the next action to\nexecute from the current state and, possibly, the history (the past states).\nWhen the number of states is large, succinct representations are often used to\ncompactly represent both the MDPs and the policies in a reduced amount of\nspace. In this paper, some problems related to the size of succinctly\nrepresented policies are analyzed. Namely, it is shown that some MDPs have\npolicies that can only be represented in space super-polynomial in the size of\nthe MDP, unless the polynomial hierarchy collapses. This fact motivates the\nstudy of the problem of deciding whether a given MDP has a policy of a given\nsize and reward. Since some algorithms for MDPs work by finding a succinct\nrepresentation of the value function, the problem of deciding the existence of\na succinct representation of a value function of a given size and reward is\nalso considered.\n"
  },
  {
    "id": "1106.4867",
    "title": "Compiling Causal Theories to Successor State Axioms and STRIPS-Like\n  Systems",
    "abstract": "  We describe a system for specifying the effects of actions. Unlike those\ncommonly used in AI planning, our system uses an action description language\nthat allows one to specify the effects of actions using domain rules, which are\nstate constraints that can entail new action effects from old ones.\nDeclaratively, an action domain in our language corresponds to a nonmonotonic\ncausal theory in the situation calculus. Procedurally, such an action domain is\ncompiled into a set of logical theories, one for each action in the domain,\nfrom which fully instantiated successor state-like axioms and STRIPS-like\nsystems are then generated. We expect the system to be a useful tool for\nknowledge engineers writing action specifications for classical AI planning\nsystems, GOLOG systems, and other systems where formal specifications of\nactions are needed.\n"
  },
  {
    "id": "1106.4868",
    "title": "VHPOP: Versatile Heuristic Partial Order Planner",
    "abstract": "  VHPOP is a partial order causal link (POCL) planner loosely based on UCPOP.\nIt draws from the experience gained in the early to mid 1990's on flaw\nselection strategies for POCL planning, and combines this with more recent\ndevelopments in the field of domain independent planning such as distance based\nheuristics and reachability analysis. We present an adaptation of the additive\nheuristic for plan space planning, and modify it to account for possible reuse\nof existing actions in a plan. We also propose a large set of novel flaw\nselection strategies, and show how these can help us solve more problems than\npreviously possible by POCL planners. VHPOP also supports planning with\ndurative actions by incorporating standard techniques for temporal constraint\nreasoning. We demonstrate that the same heuristic techniques used to boost the\nperformance of classical POCL planning can be effective in domains with\ndurative actions as well. The result is a versatile heuristic POCL planner\ncompetitive with established CSP-based and heuristic state space planners.\n"
  },
  {
    "id": "1106.4869",
    "title": "SHOP2: An HTN Planning System",
    "abstract": "  The SHOP2 planning system received one of the awards for distinguished\nperformance in the 2002 International Planning Competition. This paper\ndescribes the features of SHOP2 which enabled it to excel in the competition,\nespecially those aspects of SHOP2 that deal with temporal and metric planning\ndomains.\n"
  },
  {
    "id": "1106.4871",
    "title": "An Architectural Approach to Ensuring Consistency in Hierarchical\n  Execution",
    "abstract": "  Hierarchical task decomposition is a method used in many agent systems to\norganize agent knowledge. This work shows how the combination of a hierarchy\nand persistent assertions of knowledge can lead to difficulty in maintaining\nlogical consistency in asserted knowledge. We explore the problematic\nconsequences of persistent assumptions in the reasoning process and introduce\nnovel potential solutions. Having implemented one of the possible solutions,\nDynamic Hierarchical Justification, its effectiveness is demonstrated with an\nempirical analysis.\n"
  },
  {
    "id": "1106.4872",
    "title": "Wrapper Maintenance: A Machine Learning Approach",
    "abstract": "  The proliferation of online information sources has led to an increased use\nof wrappers for extracting data from Web sources. While most of the previous\nresearch has focused on quick and efficient generation of wrappers, the\ndevelopment of tools for wrapper maintenance has received less attention. This\nis an important research problem because Web sources often change in ways that\nprevent the wrappers from extracting data correctly. We present an efficient\nalgorithm that learns structural information about data from positive examples\nalone. We describe how this information can be used for two wrapper maintenance\napplications: wrapper verification and reinduction. The wrapper verification\nsystem detects when a wrapper is not extracting correct data, usually because\nthe Web source has changed its format. The reinduction algorithm automatically\nrecovers from changes in the Web source by identifying data on Web pages so\nthat a new wrapper may be generated for this source. To validate our approach,\nwe monitored 27 wrappers over a period of a year. The verification algorithm\ncorrectly discovered 35 of the 37 wrapper changes, and made 16 mistakes,\nresulting in precision of 0.73 and recall of 0.95. We validated the reinduction\nalgorithm on ten Web sources. We were able to successfully reinduce the\nwrappers, obtaining precision and recall values of 0.90 and 0.80 on the data\nextraction task.\n"
  },
  {
    "id": "1106.5111",
    "title": "Exploiting Reputation in Distributed Virtual Environments",
    "abstract": "  The cognitive research on reputation has shown several interesting properties\nthat can improve both the quality of services and the security in distributed\nelectronic environments. In this paper, the impact of reputation on\ndecision-making under scarcity of information will be shown. First, a cognitive\ntheory of reputation will be presented, then a selection of simulation\nexperimental results from different studies will be discussed. Such results\nconcern the benefits of reputation when agents need to find out good sellers in\na virtual market-place under uncertainty and informational cheating.\n"
  },
  {
    "id": "1106.5112",
    "title": "The All Relevant Feature Selection using Random Forest",
    "abstract": "  In this paper we examine the application of the random forest classifier for\nthe all relevant feature selection problem. To this end we first examine two\nrecently proposed all relevant feature selection algorithms, both being a\nrandom forest wrappers, on a series of synthetic data sets with varying size.\nWe show that reasonable accuracy of predictions can be achieved and that\nheuristic algorithms that were designed to handle the all relevant problem,\nhave performance that is close to that of the reference ideal algorithm. Then,\nwe apply one of the algorithms to four families of semi-synthetic data sets to\nassess how the properties of particular data set influence results of feature\nselection. Finally we test the procedure using a well-known gene expression\ndata set. The relevance of nearly all previously established important genes\nwas confirmed, moreover the relevance of several new ones is discovered.\n"
  },
  {
    "id": "1106.5256",
    "title": "Structure and Complexity in Planning with Unary Operators",
    "abstract": "  Unary operator domains -- i.e., domains in which operators have a single\neffect -- arise naturally in many control problems. In its most general form,\nthe problem of STRIPS planning in unary operator domains is known to be as hard\nas the general STRIPS planning problem -- both are PSPACE-complete. However,\nunary operator domains induce a natural structure, called the domain's causal\ngraph. This graph relates between the preconditions and effect of each domain\noperator. Causal graphs were exploited by Williams and Nayak in order to\nanalyze plan generation for one of the controllers in NASA's Deep-Space One\nspacecraft. There, they utilized the fact that when this graph is acyclic, a\nserialization ordering over any subgoal can be obtained quickly. In this paper\nwe conduct a comprehensive study of the relationship between the structure of a\ndomain's causal graph and the complexity of planning in this domain. On the\npositive side, we show that a non-trivial polynomial time plan generation\nalgorithm exists for domains whose causal graph induces a polytree with a\nconstant bound on its node indegree. On the negative side, we show that even\nplan existence is hard when the graph is a directed-path singly connected DAG.\nMore generally, we show that the number of paths in the causal graph is closely\nrelated to the complexity of planning in the associated domain. Finally we\nrelate our results to the question of complexity of planning with serializable\nsubgoals.\n"
  },
  {
    "id": "1106.5257",
    "title": "Answer Set Planning Under Action Costs",
    "abstract": "  Recently, planning based on answer set programming has been proposed as an\napproach towards realizing declarative planning systems. In this paper, we\npresent the language Kc, which extends the declarative planning language K by\naction costs. Kc provides the notion of admissible and optimal plans, which are\nplans whose overall action costs are within a given limit resp. minimum over\nall plans (i.e., cheapest plans). As we demonstrate, this novel language allows\nfor expressing some nontrivial planning tasks in a declarative way.\nFurthermore, it can be utilized for representing planning problems under other\noptimality criteria, such as computing ``shortest'' plans (with the least\nnumber of steps), and refinement combinations of cheapest and fastest plans. We\nstudy complexity aspects of the language Kc and provide a transformation to\nlogic programs, such that planning problems are solved via answer set\nprogramming. Furthermore, we report experimental results on selected problems.\nOur experience is encouraging that answer set planning may be a valuable\napproach to expressive planning systems in which intricate planning problems\ncan be naturally specified and solved.\n"
  },
  {
    "id": "1106.5258",
    "title": "Learning to Coordinate Efficiently: A Model-based Approach",
    "abstract": "  In common-interest stochastic games all players receive an identical payoff.\nPlayers participating in such games must learn to coordinate with each other in\norder to receive the highest-possible value. A number of reinforcement learning\nalgorithms have been proposed for this problem, and some have been shown to\nconverge to good solutions in the limit. In this paper we show that using very\nsimple model-based algorithms, much better (i.e., polynomial) convergence rates\ncan be attained. Moreover, our model-based algorithms are guaranteed to\nconverge to the optimal value, unlike many of the existing algorithms.\n"
  },
  {
    "id": "1106.5260",
    "title": "SAPA: A Multi-objective Metric Temporal Planner",
    "abstract": "  SAPA is a domain-independent heuristic forward chaining planner that can\nhandle durative actions, metric resource constraints, and deadline goals. It is\ndesigned to be capable of handling the multi-objective nature of metric\ntemporal planning. Our technical contributions include (i) planning-graph based\nmethods for deriving heuristics that are sensitive to both cost and makespan\n(ii) techniques for adjusting the heuristic estimates to take action\ninteractions and metric resource limitations into account and (iii) a linear\ntime greedy post-processing technique to improve execution flexibility of the\nsolution plans. An implementation of SAPA using many of the techniques\npresented in this paper was one of the best domain independent planners for\ndomains with metric and temporal constraints in the third International\nPlanning Competition, held at AIPS-02. We describe the technical details of\nextracting the heuristics and present an empirical evaluation of the current\nimplementation of SAPA.\n"
  },
  {
    "id": "1106.5261",
    "title": "A New General Method to Generate Random Modal Formulae for Testing\n  Decision Procedures",
    "abstract": "  The recent emergence of heavily-optimized modal decision procedures has\nhighlighted the key role of empirical testing in this domain. Unfortunately,\nthe introduction of extensive empirical tests for modal logics is recent, and\nso far none of the proposed test generators is very satisfactory. To cope with\nthis fact, we present a new random generation method that provides benefits\nover previous methods for generating empirical tests. It fixes and much\ngeneralizes one of the best-known methods, the random CNF_[]m test, allowing\nfor generating a much wider variety of problems, covering in principle the\nwhole input space. Our new method produces much more suitable test sets for the\ncurrent generation of modal decision procedures. We analyze the features of the\nnew method by means of an extensive collection of empirical tests.\n"
  },
  {
    "id": "1106.5262",
    "title": "AltAltp: Online Parallelization of Plans with Heuristic State Search",
    "abstract": "  Despite their near dominance, heuristic state search planners still lag\nbehind disjunctive planners in the generation of parallel plans in classical\nplanning. The reason is that directly searching for parallel solutions in state\nspace planners would require the planners to branch on all possible subsets of\nparallel actions, thus increasing the branching factor exponentially. We\npresent a variant of our heuristic state search planner AltAlt, called AltAltp\nwhich generates parallel plans by using greedy online parallelization of\npartial plans. The greedy approach is significantly informed by the use of\nnovel distance heuristics that AltAltp derives from a graphplan-style planning\ngraph for the problem. While this approach is not guaranteed to provide optimal\nparallel plans, empirical results show that AltAltp is capable of generating\ngood quality parallel plans at a fraction of the cost incurred by the\ndisjunctive planners.\n"
  },
  {
    "id": "1106.5263",
    "title": "New Polynomial Classes for Logic-Based Abduction",
    "abstract": "  We address the problem of propositional logic-based abduction, i.e., the\nproblem of searching for a best explanation for a given propositional\nobservation according to a given propositional knowledge base. We give a\ngeneral algorithm, based on the notion of projection; then we study\nrestrictions over the representations of the knowledge base and of the query,\nand find new polynomial classes of abduction problems.\n"
  },
  {
    "id": "1106.5265",
    "title": "Planning Through Stochastic Local Search and Temporal Action Graphs in\n  LPG",
    "abstract": "  We present some techniques for planning in domains specified with the recent\nstandard language PDDL2.1, supporting 'durative actions' and numerical\nquantities. These techniques are implemented in LPG, a domain-independent\nplanner that took part in the 3rd International Planning Competition (IPC). LPG\nis an incremental, any time system producing multi-criteria quality plans. The\ncore of the system is based on a stochastic local search method and on a\ngraph-based representation called 'Temporal Action Graphs' (TA-graphs). This\npaper focuses on temporal planning, introducing TA-graphs and proposing some\ntechniques to guide the search in LPG using this representation. The\nexperimental results of the 3rd IPC, as well as further results presented in\nthis paper, show that our techniques can be very effective. Often LPG\noutperforms all other fully-automated planners of the 3rd IPC in terms of speed\nto derive a solution, or quality of the solutions that can be produced.\n"
  },
  {
    "id": "1106.5266",
    "title": "TALplanner in IPC-2002: Extensions and Control Rules",
    "abstract": "  TALplanner is a forward-chaining planner that relies on domain knowledge in\nthe shape of temporal logic formulas in order to prune irrelevant parts of the\nsearch space. TALplanner recently participated in the third International\nPlanning Competition, which had a clear emphasis on increasing the complexity\nof the problem domains being used as benchmark tests and the expressivity\nrequired to represent these domains in a planning system. Like many other\nplanners, TALplanner had support for some but not all aspects of this increase\nin expressivity, and a number of changes to the planner were required. After a\nshort introduction to TALplanner, this article describes some of the changes\nthat were made before and during the competition. We also describe the process\nof introducing suitable domain knowledge for several of the competition\ndomains.\n"
  },
  {
    "id": "1106.5268",
    "title": "Temporal Decision Trees: Model-based Diagnosis of Dynamic Systems\n  On-Board",
    "abstract": "  The automatic generation of decision trees based on off-line reasoning on\nmodels of a domain is a reasonable compromise between the advantages of using a\nmodel-based approach in technical domains and the constraints imposed by\nembedded applications. In this paper we extend the approach to deal with\ntemporal information. We introduce a notion of temporal decision tree, which is\ndesigned to make use of relevant information as long as it is acquired, and we\npresent an algorithm for compiling such trees from a model-based reasoning\nsystem.\n"
  },
  {
    "id": "1106.5269",
    "title": "Optimal Schedules for Parallelizing Anytime Algorithms: The Case of\n  Shared Resources",
    "abstract": "  The performance of anytime algorithms can be improved by simultaneously\nsolving several instances of algorithm-problem pairs. These pairs may include\ndifferent instances of a problem (such as starting from a different initial\nstate), different algorithms (if several alternatives exist), or several runs\nof the same algorithm (for non-deterministic algorithms). In this paper we\npresent a methodology for designing an optimal scheduling policy based on the\nstatistical characteristics of the algorithms involved. We formally analyze the\ncase where the processes share resources (a single-processor model), and\nprovide an algorithm for optimal scheduling. We analyze, theoretically and\nempirically, the behavior of our scheduling algorithm for various distribution\ntypes. Finally, we present empirical results of applying our scheduling\nalgorithm to the Latin Square problem.\n"
  },
  {
    "id": "1106.5270",
    "title": "Decision-Theoretic Bidding Based on Learned Density Models in\n  Simultaneous, Interacting Auctions",
    "abstract": "  Auctions are becoming an increasingly popular method for transacting\nbusiness, especially over the Internet. This article presents a general\napproach to building autonomous bidding agents to bid in multiple simultaneous\nauctions for interacting goods. A core component of our approach learns a model\nof the empirical price dynamics based on past data and uses the model to\nanalytically calculate, to the greatest extent possible, optimal bids. We\nintroduce a new and general boosting-based algorithm for conditional density\nestimation problems of this kind, i.e., supervised learning problems in which\nthe goal is to estimate the entire conditional distribution of the real-valued\nlabel. This approach is fully implemented as ATTac-2001, a top-scoring agent in\nthe second Trading Agent Competition (TAC-01). We present experiments\ndemonstrating the effectiveness of our boosting-based price predictor relative\nto several reasonable alternatives.\n"
  },
  {
    "id": "1106.5271",
    "title": "The Metric-FF Planning System: Translating \"Ignoring Delete Lists\" to\n  Numeric State Variables",
    "abstract": "  Planning with numeric state variables has been a challenge for many years,\nand was a part of the 3rd International Planning Competition (IPC-3). Currently\none of the most popular and successful algorithmic techniques in STRIPS\nplanning is to guide search by a heuristic function, where the heuristic is\nbased on relaxing the planning task by ignoring the delete lists of the\navailable actions. We present a natural extension of ``ignoring delete lists''\nto numeric state variables, preserving the relevant theoretical properties of\nthe STRIPS relaxation under the condition that the numeric task at hand is\n``monotonic''. We then identify a subset of the numeric IPC-3 competition\nlanguage, ``linear tasks'', where monotonicity can be achieved by\npre-processing. Based on that, we extend the algorithms used in the heuristic\nplanning system FF to linear tasks. The resulting system Metric-FF is,\naccording to the IPC-3 results which we discuss, one of the two currently most\nefficient numeric planners.\n"
  },
  {
    "id": "1106.5312",
    "title": "Manipulation of Nanson's and Baldwin's Rules",
    "abstract": "  Nanson's and Baldwin's voting rules select a winner by successively\neliminating candidates with low Borda scores. We show that these rules have a\nnumber of desirable computational properties. In particular, with unweighted\nvotes, it is NP-hard to manipulate either rule with one manipulator, whilst\nwith weighted votes, it is NP-hard to manipulate either rule with a small\nnumber of candidates and a coalition of manipulators. As only a couple of other\nvoting rules are known to be NP-hard to manipulate with a single manipulator,\nNanson's and Baldwin's rules appear to be particularly resistant to\nmanipulation from a theoretical perspective. We also propose a number of\napproximation methods for manipulating these two rules. Experiments demonstrate\nthat both rules are often difficult to manipulate in practice. These results\nsuggest that elimination style voting rules deserve further study.\n"
  },
  {
    "id": "1106.5427",
    "title": "Theory and Algorithms for Partial Order Based Reduction in Planning",
    "abstract": "  Search is a major technique for planning. It amounts to exploring a state\nspace of planning domains typically modeled as a directed graph. However,\nprohibitively large sizes of the search space make search expensive. Developing\nbetter heuristic functions has been the main technique for improving search\nefficiency. Nevertheless, recent studies have shown that improving heuristics\nalone has certain fundamental limits on improving search efficiency. Recently,\na new direction of research called partial order based reduction (POR) has been\nproposed as an alternative to improving heuristics. POR has shown promise in\nspeeding up searches.\n  POR has been extensively studied in model checking research and is a key\nenabling technique for scalability of model checking systems. Although the POR\ntheory has been extensively studied in model checking, it has never been\ndeveloped systematically for planning before. In addition, the conditions for\nPOR in the model checking theory are abstract and not directly applicable in\nplanning. Previous works on POR algorithms for planning did not establish the\nconnection between these algorithms and existing theory in model checking.\n  In this paper, we develop a theory for POR in planning. The new theory we\ndevelop connects the stubborn set theory in model checking and POR methods in\nplanning. We show that previous POR algorithms in planning can be explained by\nthe new theory. Based on the new theory, we propose a new, stronger POR\nalgorithm. Experimental results on various planning domains show further search\ncost reduction using the new algorithm.\n"
  },
  {
    "id": "1106.5890",
    "title": "A Comparison of Lex Bounds for Multiset Variables in Constraint\n  Programming",
    "abstract": "  Set and multiset variables in constraint programming have typically been\nrepresented using subset bounds. However, this is a weak representation that\nneglects potentially useful information about a set such as its cardinality.\nFor set variables, the length-lex (LL) representation successfully provides\ninformation about the length (cardinality) and position in the lexicographic\nordering. For multiset variables, where elements can be repeated, we consider\nricher representations that take into account additional information. We study\neight different representations in which we maintain bounds according to one of\nthe eight different orderings: length-(co)lex (LL/LC), variety-(co)lex (VL/VC),\nlength-variety-(co)lex (LVL/LVC), and variety-length-(co)lex (VLL/VLC)\norderings. These representations integrate together information about the\ncardinality, variety (number of distinct elements in the multiset), and\nposition in some total ordering. Theoretical and empirical comparisons of\nexpressiveness and compactness of the eight representations suggest that\nlength-variety-(co)lex (LVL/LVC) and variety-length-(co)lex (VLL/VLC) usually\ngive tighter bounds after constraint propagation. We implement the eight\nrepresentations and evaluate them against the subset bounds representation with\ncardinality and variety reasoning. Results demonstrate that they offer\nsignificantly better pruning and runtime.\n"
  },
  {
    "id": "1106.5998",
    "title": "The 3rd International Planning Competition: Results and Analysis",
    "abstract": "  This paper reports the outcome of the third in the series of biennial\ninternational planning competitions, held in association with the International\nConference on AI Planning and Scheduling (AIPS) in 2002. In addition to\ndescribing the domains, the planners and the objectives of the competition, the\npaper includes analysis of the results. The results are analysed from several\nperspectives, in order to address the questions of comparative performance\nbetween planners, comparative difficulty of domains, the degree of agreement\nbetween planners about the relative difficulty of individual problem instances\nand the question of how well planners scale relative to one another over\nincreasingly difficult problems. The paper addresses these questions through\nstatistical analysis of the raw results of the competition, in order to\ndetermine which results can be considered to be adequately supported by the\ndata. The paper concludes with a discussion of some challenges for the future\nof the competition series.\n"
  },
  {
    "id": "1106.6022",
    "title": "Use of Markov Chains to Design an Agent Bidding Strategy for Continuous\n  Double Auctions",
    "abstract": "  As computational agents are developed for increasingly complicated e-commerce\napplications, the complexity of the decisions they face demands advances in\nartificial intelligence techniques. For example, an agent representing a seller\nin an auction should try to maximize the seller's profit by reasoning about a\nvariety of possibly uncertain pieces of information, such as the maximum prices\nvarious buyers might be willing to pay, the possible prices being offered by\ncompeting sellers, the rules by which the auction operates, the dynamic arrival\nand matching of offers to buy and sell, and so on. A naive application of\nmultiagent reasoning techniques would require the seller's agent to explicitly\nmodel all of the other agents through an extended time horizon, rendering the\nproblem intractable for many realistically-sized problems. We have instead\ndevised a new strategy that an agent can use to determine its bid price based\non a more tractable Markov chain model of the auction process. We have\nexperimentally identified the conditions under which our new strategy works\nwell, as well as how well it works in comparison to the optimal performance the\nagent could have achieved had it known the future. Our results show that our\nnew strategy in general performs well, outperforming other tractable heuristic\nstrategies in a majority of experiments, and is particularly effective in a\n'seller?s market', where many buy offers are available.\n"
  },
  {
    "id": "1107.0018",
    "title": "A New Technique for Combining Multiple Classifiers using The\n  Dempster-Shafer Theory of Evidence",
    "abstract": "  This paper presents a new classifier combination technique based on the\nDempster-Shafer theory of evidence. The Dempster-Shafer theory of evidence is a\npowerful method for combining measures of evidence from different classifiers.\nHowever, since each of the available methods that estimates the evidence of\nclassifiers has its own limitations, we propose here a new implementation which\nadapts to training data so that the overall mean square error is minimized. The\nproposed technique is shown to outperform most available classifier combination\nmethods when tested on three different classification problems.\n"
  },
  {
    "id": "1107.0019",
    "title": "Searching for Bayesian Network Structures in the Space of Restricted\n  Acyclic Partially Directed Graphs",
    "abstract": "  Although many algorithms have been designed to construct Bayesian network\nstructures using different approaches and principles, they all employ only two\nmethods: those based on independence criteria, and those based on a scoring\nfunction and a search procedure (although some methods combine the two). Within\nthe score+search paradigm, the dominant approach uses local search methods in\nthe space of directed acyclic graphs (DAGs), where the usual choices for\ndefining the elementary modifications (local changes) that can be applied are\narc addition, arc deletion, and arc reversal. In this paper, we propose a new\nlocal search method that uses a different search space, and which takes account\nof the concept of equivalence between network structures: restricted acyclic\npartially directed graphs (RPDAGs). In this way, the number of different\nconfigurations of the search space is reduced, thus improving efficiency.\nMoreover, although the final result must necessarily be a local optimum given\nthe nature of the search method, the topology of the new search space, which\navoids making early decisions about the directions of the arcs, may help to\nfind better local optima than those obtained by searching in the DAG space.\nDetailed results of the evaluation of the proposed search method on several\ntest problems, including the well-known Alarm Monitoring System, are also\npresented.\n"
  },
  {
    "id": "1107.0020",
    "title": "Learning to Order BDD Variables in Verification",
    "abstract": "  The size and complexity of software and hardware systems have significantly\nincreased in the past years. As a result, it is harder to guarantee their\ncorrect behavior. One of the most successful methods for automated verification\nof finite-state systems is model checking. Most of the current model-checking\nsystems use binary decision diagrams (BDDs) for the representation of the\ntested model and in the verification process of its properties. Generally, BDDs\nallow a canonical compact representation of a boolean function (given an order\nof its variables). The more compact the BDD is, the better performance one gets\nfrom the verifier. However, finding an optimal order for a BDD is an\nNP-complete problem. Therefore, several heuristic methods based on expert\nknowledge have been developed for variable ordering. We propose an alternative\napproach in which the variable ordering algorithm gains 'ordering experience'\nfrom training models and uses the learned knowledge for finding good orders.\nOur methodology is based on offline learning of pair precedence classifiers\nfrom training models, that is, learning which variable pair permutation is more\nlikely to lead to a good order. For each training model, a number of training\nsequences are evaluated. Every training model variable pair permutation is then\ntagged based on its performance on the evaluated orders. The tagged\npermutations are then passed through a feature extractor and are given as\nexamples to a classifier creation algorithm. Given a model for which an order\nis requested, the ordering algorithm consults each precedence classifier and\nconstructs a pair precedence table which is used to create the order. Our\nalgorithm was integrated with SMV, which is one of the most widely used\nverification systems. Preliminary empirical evaluation of our methodology,\nusing real benchmark models, shows performance that is better than random\nordering and is competitive with existing algorithms that use expert knowledge.\nWe believe that in sub-domains of models (alu, caches, etc.) our system will\nprove even more valuable. This is because it features the ability to learn\nsub-domain knowledge, something that no other ordering algorithm does.\n"
  },
  {
    "id": "1107.0021",
    "title": "Decentralized Supply Chain Formation: A Market Protocol and Competitive\n  Equilibrium Analysis",
    "abstract": "  Supply chain formation is the process of determining the structure and terms\nof exchange relationships to enable a multilevel, multiagent production\nactivity. We present a simple model of supply chains, highlighting two\ncharacteristic features: hierarchical subtask decomposition, and resource\ncontention. To decentralize the formation process, we introduce a market price\nsystem over the resources produced along the chain. In a competitive\nequilibrium for this system, agents choose locally optimal allocations with\nrespect to prices, and outcomes are optimal overall. To determine prices, we\ndefine a market protocol based on distributed, progressive auctions, and\nmyopic, non-strategic agent bidding policies. In the presence of resource\ncontention, this protocol produces better solutions than the greedy protocols\ncommon in the artificial intelligence and multiagent systems literature. The\nprotocol often converges to high-value supply chains, and when competitive\nequilibria exist, typically to approximate competitive equilibria. However,\ncomplementarities in agent production technologies can cause the protocol to\nwastefully allocate inputs to agents that do not produce their outputs. A\nsubsequent decommitment phase recovers a significant fraction of the lost\nsurplus.\n"
  },
  {
    "id": "1107.0023",
    "title": "CP-nets: A Tool for Representing and Reasoning withConditional Ceteris\n  Paribus Preference Statements",
    "abstract": "  Information about user preferences plays a key role in automated decision\nmaking. In many domains it is desirable to assess such preferences in a\nqualitative rather than quantitative way. In this paper, we propose a\nqualitative graphical representation of preferences that reflects conditional\ndependence and independence of preference statements under a ceteris paribus\n(all else being equal) interpretation. Such a representation is often compact\nand arguably quite natural in many circumstances. We provide a formal semantics\nfor this model, and describe how the structure of the network can be exploited\nin several inference tasks, such as determining whether one outcome dominates\n(is preferred to) another, ordering a set outcomes according to the preference\nrelation, and constructing the best outcome subject to available evidence.\n"
  },
  {
    "id": "1107.0024",
    "title": "Complexity Results and Approximation Strategies for MAP Explanations",
    "abstract": "  MAP is the problem of finding a most probable instantiation of a set of\nvariables given evidence. MAP has always been perceived to be significantly\nharder than the related problems of computing the probability of a variable\ninstantiation Pr, or the problem of computing the most probable explanation\n(MPE). This paper investigates the complexity of MAP in Bayesian networks.\nSpecifically, we show that MAP is complete for NP^PP and provide further\nnegative complexity results for algorithms based on variable elimination. We\nalso show that MAP remains hard even when MPE and Pr become easy. For example,\nwe show that MAP is NP-complete when the networks are restricted to polytrees,\nand even then can not be effectively approximated. Given the difficulty of\ncomputing MAP exactly, and the difficulty of approximating MAP while providing\nuseful guarantees on the resulting approximation, we investigate best effort\napproximations. We introduce a generic MAP approximation framework. We provide\ntwo instantiations of the framework; one for networks which are amenable to\nexact inference Pr, and one for networks for which even exact inference is too\nhard. This allows MAP approximation on networks that are too complex to even\nexactly solve the easier problems, Pr and MPE. Experimental results indicate\nthat using these approximation algorithms provides much better solutions than\nstandard techniques, and provide accurate MAP estimates in many cases.\n"
  },
  {
    "id": "1107.0025",
    "title": "Taming Numbers and Durations in the Model Checking Integrated Planning\n  System",
    "abstract": "  The Model Checking Integrated Planning System (MIPS) is a temporal least\ncommitment heuristic search planner based on a flexible object-oriented\nworkbench architecture. Its design clearly separates explicit and symbolic\ndirected exploration algorithms from the set of on-line and off-line computed\nestimates and associated data structures. MIPS has shown distinguished\nperformance in the last two international planning competitions. In the last\nevent the description language was extended from pure propositional planning to\ninclude numerical state variables, action durations, and plan quality objective\nfunctions. Plans were no longer sequences of actions but time-stamped\nschedules. As a participant of the fully automated track of the competition,\nMIPS has proven to be a general system; in each track and every benchmark\ndomain it efficiently computed plans of remarkable quality. This article\nintroduces and analyzes the most important algorithmic novelties that were\nnecessary to tackle the new layers of expressiveness in the benchmark problems\nand to achieve a high level of performance. The extensions include critical\npath analysis of sequentially generated plans to generate corresponding optimal\nparallel plans. The linear time algorithm to compute the parallel plan bypasses\nknown NP hardness results for partial ordering by scheduling plans with respect\nto the set of actions and the imposed precedence relations. The efficiency of\nthis algorithm also allows us to improve the exploration guidance: for each\nencountered planning state the corresponding approximate sequential plan is\nscheduled. One major strength of MIPS is its static analysis phase that grounds\nand simplifies parameterized predicates, functions and operators, that infers\nknowledge to minimize the state description length, and that detects domain\nobject symmetries. The latter aspect is analyzed in detail. MIPS has been\ndeveloped to serve as a complete and optimal state space planner, with\nadmissible estimates, exploration engines and branching cuts. In the\ncompetition version, however, certain performance compromises had to be made,\nincluding floating point arithmetic, weighted heuristic search exploration\naccording to an inadmissible estimate and parameterized optimization.\n"
  },
  {
    "id": "1107.0026",
    "title": "IDL-Expressions: A Formalism for Representing and Parsing Finite\n  Languages in Natural Language Processing",
    "abstract": "  We propose a formalism for representation of finite languages, referred to as\nthe class of IDL-expressions, which combines concepts that were only considered\nin isolation in existing formalisms. The suggested applications are in natural\nlanguage processing, more specifically in surface natural language generation\nand in machine translation, where a sentence is obtained by first generating a\nlarge set of candidate sentences, represented in a compact way, and then by\nfiltering such a set through a parser. We study several formal properties of\nIDL-expressions and compare this new formalism with more standard ones. We also\npresent a novel parsing algorithm for IDL-expressions and prove a non-trivial\nupper bound on its time complexity.\n"
  },
  {
    "id": "1107.0027",
    "title": "Effective Dimensions of Hierarchical Latent Class Models",
    "abstract": "  Hierarchical latent class (HLC) models are tree-structured Bayesian networks\nwhere leaf nodes are observed while internal nodes are latent. There are no\ntheoretically well justified model selection criteria for HLC models in\nparticular and Bayesian networks with latent nodes in general. Nonetheless,\nempirical studies suggest that the BIC score is a reasonable criterion to use\nin practice for learning HLC models. Empirical studies also suggest that\nsometimes model selection can be improved if standard model dimension is\nreplaced with effective model dimension in the penalty term of the BIC score.\nEffective dimensions are difficult to compute. In this paper, we prove a\ntheorem that relates the effective dimension of an HLC model to the effective\ndimensions of a number of latent class models. The theorem makes it\ncomputationally feasible to compute the effective dimensions of large HLC\nmodels. The theorem can also be used to compute the effective dimensions of\ngeneral tree models.\n"
  },
  {
    "id": "1107.0030",
    "title": "Coherent Integration of Databases by Abductive Logic Programming",
    "abstract": "  We introduce an abductive method for a coherent integration of independent\ndata-sources. The idea is to compute a list of data-facts that should be\ninserted to the amalgamated database or retracted from it in order to restore\nits consistency. This method is implemented by an abductive solver, called\nAsystem, that applies SLDNFA-resolution on a meta-theory that relates\ndifferent, possibly contradicting, input databases. We also give a pure\nmodel-theoretic analysis of the possible ways to `recover' consistent data from\nan inconsistent database in terms of those models of the database that exhibit\nas minimal inconsistent information as reasonably possible. This allows us to\ncharacterize the `recovered databases' in terms of the `preferred' (i.e., most\nconsistent) models of the theory. The outcome is an abductive-based application\nthat is sound and complete with respect to a corresponding model-based,\npreferential semantics, and -- to the best of our knowledge -- is more\nexpressive (thus more general) than any other implementation of coherent\nintegration of databases.\n"
  },
  {
    "id": "1107.0031",
    "title": "Grounded Semantic Composition for Visual Scenes",
    "abstract": "  We present a visually-grounded language understanding model based on a study\nof how people verbally describe objects in scenes. The emphasis of the model is\non the combination of individual word meanings to produce meanings for complex\nreferring expressions. The model has been implemented, and it is able to\nunderstand a broad range of spatial referring expressions. We describe our\nimplementation of word level visually-grounded semantics and their embedding in\na compositional parsing framework. The implemented system selects the correct\nreferents in response to natural language expressions for a large percentage of\ntest cases. In an analysis of the system's successes and failures we reveal how\nvisual context influences the semantics of utterances and propose future\nextensions to the model that take such context into account.\n"
  },
  {
    "id": "1107.0034",
    "title": "Price Prediction in a Trading Agent Competition",
    "abstract": "  The 2002 Trading Agent Competition (TAC) presented a challenging market game\nin the domain of travel shopping. One of the pivotal issues in this domain is\nuncertainty about hotel prices, which have a significant influence on the\nrelative cost of alternative trip schedules. Thus, virtually all participants\nemploy some method for predicting hotel prices. We survey approaches employed\nin the tournament, finding that agents apply an interesting diversity of\ntechniques, taking into account differing sources of evidence bearing on\nprices. Based on data provided by entrants on their agents' actual predictions\nin the TAC-02 finals and semifinals, we analyze the relative efficacy of these\napproaches. The results show that taking into account game-specific information\nabout flight prices is a major distinguishing factor. Machine learning methods\neffectively induce the relationship between flight and hotel prices from game\ndata, and a purely analytical approach based on competitive equilibrium\nanalysis achieves equal accuracy with no historical data. Employing a new\nmeasure of prediction quality, we relate absolute accuracy to bottom-line\nperformance in the game.\n"
  },
  {
    "id": "1107.0035",
    "title": "Compositional Model Repositories via Dynamic Constraint Satisfaction\n  with Order-of-Magnitude Preferences",
    "abstract": "  The predominant knowledge-based approach to automated model construction,\ncompositional modelling, employs a set of models of particular functional\ncomponents. Its inference mechanism takes a scenario describing the constituent\ninteracting components of a system and translates it into a useful mathematical\nmodel. This paper presents a novel compositional modelling approach aimed at\nbuilding model repositories. It furthers the field in two respects. Firstly, it\nexpands the application domain of compositional modelling to systems that can\nnot be easily described in terms of interacting functional components, such as\necological systems. Secondly, it enables the incorporation of user preferences\ninto the model selection process. These features are achieved by casting the\ncompositional modelling problem as an activity-based dynamic preference\nconstraint satisfaction problem, where the dynamic constraints describe the\nrestrictions imposed over the composition of partial models and the preferences\ncorrespond to those of the user of the automated modeller. In addition, the\npreference levels are represented through the use of symbolic values that\ndiffer in orders of magnitude.\n"
  },
  {
    "id": "1107.0037",
    "title": "Competitive Coevolution through Evolutionary Complexification",
    "abstract": "  Two major goals in machine learning are the discovery and improvement of\nsolutions to complex problems. In this paper, we argue that complexification,\ni.e. the incremental elaboration of solutions through adding new structure,\nachieves both these goals. We demonstrate the power of complexification through\nthe NeuroEvolution of Augmenting Topologies (NEAT) method, which evolves\nincreasingly complex neural network architectures. NEAT is applied to an\nopen-ended coevolutionary robot duel domain where robot controllers compete\nhead to head. Because the robot duel domain supports a wide range of\nstrategies, and because coevolution benefits from an escalating arms race, it\nserves as a suitable testbed for studying complexification. When compared to\nthe evolution of networks with fixed structure, complexifying evolution\ndiscovers significantly more sophisticated strategies. The results suggest that\nin order to discover and improve complex solutions, evolution, and search in\ngeneral, should be allowed to complexify as well as optimize.\n"
  },
  {
    "id": "1107.0038",
    "title": "Dual Modelling of Permutation and Injection Problems",
    "abstract": "  When writing a constraint program, we have to choose which variables should\nbe the decision variables, and how to represent the constraints on these\nvariables. In many cases, there is considerable choice for the decision\nvariables. Consider, for example, permutation problems in which we have as many\nvalues as variables, and each variable takes an unique value. In such problems,\nwe can choose between a primal and a dual viewpoint. In the dual viewpoint,\neach dual variable represents one of the primal values, whilst each dual value\nrepresents one of the primal variables. Alternatively, by means of channelling\nconstraints to link the primal and dual variables, we can have a combined model\nwith both sets of variables. In this paper, we perform an extensive theoretical\nand empirical study of such primal, dual and combined models for two classes of\nproblems: permutation problems and injection problems. Our results show that it\noften be advantageous to use multiple viewpoints, and to have constraints which\nchannel between them to maintain consistency. They also illustrate a general\nmethodology for comparing different constraint models.\n"
  },
  {
    "id": "1107.0040",
    "title": "Generalizing Boolean Satisfiability I: Background and Survey of Existing\n  Work",
    "abstract": "  This is the first of three planned papers describing ZAP, a satisfiability\nengine that substantially generalizes existing tools while retaining the\nperformance characteristics of modern high-performance solvers. The fundamental\nidea underlying ZAP is that many problems passed to such engines contain rich\ninternal structure that is obscured by the Boolean representation used; our\ngoal is to define a representation in which this structure is apparent and can\neasily be exploited to improve computational performance. This paper is a\nsurvey of the work underlying ZAP, and discusses previous attempts to improve\nthe performance of the Davis-Putnam-Logemann-Loveland algorithm by exploiting\nthe structure of the problem being solved. We examine existing ideas including\nextensions of the Boolean language to allow cardinality constraints,\npseudo-Boolean representations, symmetry, and a limited form of quantification.\nWhile this paper is intended as a survey, our research results are contained in\nthe two subsequent articles, with the theoretical structure of ZAP described in\nthe second paper in this series, and ZAP's implementation described in the\nthird.\n"
  },
  {
    "id": "1107.0041",
    "title": "PHA*: Finding the Shortest Path with A* in An Unknown Physical\n  Environment",
    "abstract": "  We address the problem of finding the shortest path between two points in an\nunknown real physical environment, where a traveling agent must move around in\nthe environment to explore unknown territory. We introduce the Physical-A*\nalgorithm (PHA*) for solving this problem. PHA* expands all the mandatory nodes\nthat A* would expand and returns the shortest path between the two points.\nHowever, due to the physical nature of the problem, the complexity of the\nalgorithm is measured by the traveling effort of the moving agent and not by\nthe number of generated nodes, as in standard A*. PHA* is presented as a\ntwo-level algorithm, such that its high level, A*, chooses the next node to be\nexpanded and its low level directs the agent to that node in order to explore\nit. We present a number of variations for both the high-level and low-level\nprocedures and evaluate their performance theoretically and experimentally. We\nshow that the travel cost of our best variation is fairly close to the optimal\ntravel cost, assuming that the mandatory nodes of A* are known in advance. We\nthen generalize our algorithm to the multi-agent case, where a number of\ncooperative agents are designed to solve the problem. Specifically, we provide\nan experimental implementation for such a system. It should be noted that the\nproblem addressed here is not a navigation problem, but rather a problem of\nfinding the shortest path between two points for future usage.\n"
  },
  {
    "id": "1107.0042",
    "title": "Restricted Value Iteration: Theory and Algorithms",
    "abstract": "  Value iteration is a popular algorithm for finding near optimal policies for\nPOMDPs. It is inefficient due to the need to account for the entire belief\nspace, which necessitates the solution of large numbers of linear programs. In\nthis paper, we study value iteration restricted to belief subsets. We show\nthat, together with properly chosen belief subsets, restricted value iteration\nyields near-optimal policies and we give a condition for determining whether a\ngiven belief subset would bring about savings in space and time. We also apply\nrestricted value iteration to two interesting classes of POMDPs, namely\ninformative POMDPs and near-discernible POMDPs.\n"
  },
  {
    "id": "1107.0043",
    "title": "A Maximal Tractable Class of Soft Constraints",
    "abstract": "  Many researchers in artificial intelligence are beginning to explore the use\nof soft constraints to express a set of (possibly conflicting) problem\nrequirements. A soft constraint is a function defined on a collection of\nvariables which associates some measure of desirability with each possible\ncombination of values for those variables. However, the crucial question of the\ncomputational complexity of finding the optimal solution to a collection of\nsoft constraints has so far received very little attention. In this paper we\nidentify a class of soft binary constraints for which the problem of finding\nthe optimal solution is tractable. In other words, we show that for any given\nset of such constraints, there exists a polynomial time algorithm to determine\nthe assignment having the best overall combined measure of desirability. This\ntractable class includes many commonly-occurring soft constraints, such as 'as\nnear as possible' or 'as soon as possible after', as well as crisp constraints\nsuch as 'greater than'. Finally, we show that this tractable class is maximal,\nin the sense that adding any other form of soft binary constraint which is not\nin the class gives rise to a class of problems which is NP-hard.\n"
  },
  {
    "id": "1107.0044",
    "title": "Towards Understanding and Harnessing the Potential of Clause Learning",
    "abstract": "  Efficient implementations of DPLL with the addition of clause learning are\nthe fastest complete Boolean satisfiability solvers and can handle many\nsignificant real-world problems, such as verification, planning and design.\nDespite its importance, little is known of the ultimate strengths and\nlimitations of the technique. This paper presents the first precise\ncharacterization of clause learning as a proof system (CL), and begins the task\nof understanding its power by relating it to the well-studied resolution proof\nsystem. In particular, we show that with a new learning scheme, CL can provide\nexponentially shorter proofs than many proper refinements of general resolution\n(RES) satisfying a natural property. These include regular and Davis-Putnam\nresolution, which are already known to be much stronger than ordinary DPLL. We\nalso show that a slight variant of CL with unlimited restarts is as powerful as\nRES itself. Translating these analytical results to practice, however, presents\na challenge because of the nondeterministic nature of clause learning\nalgorithms. We propose a novel way of exploiting the underlying problem\nstructure, in the form of a high level problem description such as a graph or\nPDDL specification, to guide clause learning algorithms toward faster\nsolutions. We show that this leads to exponential speed-ups on grid and\nrandomized pebbling problems, as well as substantial improvements on certain\nordering formulas.\n"
  },
  {
    "id": "1107.0045",
    "title": "Graduality in Argumentation",
    "abstract": "  Argumentation is based on the exchange and valuation of interacting\narguments, followed by the selection of the most acceptable of them (for\nexample, in order to take a decision, to make a choice). Starting from the\nframework proposed by Dung in 1995, our purpose is to introduce 'graduality' in\nthe selection of the best arguments, i.e., to be able to partition the set of\nthe arguments in more than the two usual subsets of 'selected' and\n'non-selected' arguments in order to represent different levels of selection.\nOur basic idea is that an argument is all the more acceptable if it can be\npreferred to its attackers. First, we discuss general principles underlying a\n'gradual' valuation of arguments based on their interactions. Following these\nprinciples, we define several valuation models for an abstract argumentation\nsystem. Then, we introduce 'graduality' in the concept of acceptability of\narguments. We propose new acceptability classes and a refinement of existing\nclasses taking advantage of an available 'gradual' valuation.\n"
  },
  {
    "id": "1107.0046",
    "title": "Explicit Learning Curves for Transduction and Application to Clustering\n  and Compression Algorithms",
    "abstract": "  Inductive learning is based on inferring a general rule from a finite data\nset and using it to label new data. In transduction one attempts to solve the\nproblem of using a labeled training set to label a set of unlabeled points,\nwhich are given to the learner prior to learning. Although transduction seems\nat the outset to be an easier task than induction, there have not been many\nprovably useful algorithms for transduction. Moreover, the precise relation\nbetween induction and transduction has not yet been determined. The main\ntheoretical developments related to transduction were presented by Vapnik more\nthan twenty years ago. One of Vapnik's basic results is a rather tight error\nbound for transductive classification based on an exact computation of the\nhypergeometric tail. While tight, this bound is given implicitly via a\ncomputational routine. Our first contribution is a somewhat looser but explicit\ncharacterization of a slightly extended PAC-Bayesian version of Vapnik's\ntransductive bound. This characterization is obtained using concentration\ninequalities for the tail of sums of random variables obtained by sampling\nwithout replacement. We then derive error bounds for compression schemes such\nas (transductive) support vector machines and for transduction algorithms based\non clustering. The main observation used for deriving these new error bounds\nand algorithms is that the unlabeled test points, which in the transductive\nsetting are known in advance, can be used in order to construct useful data\ndependent prior distributions over the hypothesis space.\n"
  },
  {
    "id": "1107.0047",
    "title": "Decentralized Control of Cooperative Systems: Categorization and\n  Complexity Analysis",
    "abstract": "  Decentralized control of cooperative systems captures the operation of a\ngroup of decision makers that share a single global objective. The difficulty\nin solving optimally such problems arises when the agents lack full\nobservability of the global state of the system when they operate. The general\nproblem has been shown to be NEXP-complete. In this paper, we identify classes\nof decentralized control problems whose complexity ranges between NEXP and P.\nIn particular, we study problems characterized by independent transitions,\nindependent observations, and goal-oriented objective functions. Two algorithms\nare shown to solve optimally useful classes of goal-oriented decentralized\nprocesses in polynomial time. This paper also studies information sharing among\nthe decision-makers, which can improve their performance. We distinguish\nbetween three ways in which agents can exchange information: indirect\ncommunication, direct communication and sharing state features that are not\ncontrolled by the agents. Our analysis shows that for every class of problems\nwe consider, introducing direct or indirect communication does not change the\nworst-case complexity. The results provide a better understanding of the\ncomplexity of decentralized control problems that arise in practice and\nfacilitate the development of planning algorithms for these problems.\n"
  },
  {
    "id": "1107.0048",
    "title": "Reinforcement Learning for Agents with Many Sensors and Actuators Acting\n  in Categorizable Environments",
    "abstract": "  In this paper, we confront the problem of applying reinforcement learning to\nagents that perceive the environment through many sensors and that can perform\nparallel actions using many actuators as is the case in complex autonomous\nrobots. We argue that reinforcement learning can only be successfully applied\nto this case if strong assumptions are made on the characteristics of the\nenvironment in which the learning is performed, so that the relevant sensor\nreadings and motor commands can be readily identified. The introduction of such\nassumptions leads to strongly-biased learning systems that can eventually lose\nthe generality of traditional reinforcement-learning algorithms. In this line,\nwe observe that, in realistic situations, the reward received by the robot\ndepends only on a reduced subset of all the executed actions and that only a\nreduced subset of the sensor inputs (possibly different in each situation and\nfor each action) are relevant to predict the reward. We formalize this property\nin the so called 'categorizability assumption' and we present an algorithm that\ntakes advantage of the categorizability of the environment, allowing a decrease\nin the learning time with respect to existing reinforcement-learning\nalgorithms. Results of the application of the algorithm to a couple of\nsimulated realistic-robotic problems (landmark-based navigation and the\nsix-legged robot gait generation) are reported to validate our approach and to\ncompare it to existing flat and generalization-based reinforcement-learning\napproaches.\n"
  },
  {
    "id": "1107.0050",
    "title": "Additive Pattern Database Heuristics",
    "abstract": "  We explore a method for computing admissible heuristic evaluation functions\nfor search problems. It utilizes pattern databases, which are precomputed\ntables of the exact cost of solving various subproblems of an existing problem.\nUnlike standard pattern database heuristics, however, we partition our problems\ninto disjoint subproblems, so that the costs of solving the different\nsubproblems can be added together without overestimating the cost of solving\nthe original problem. Previously, we showed how to statically partition the\nsliding-tile puzzles into disjoint groups of tiles to compute an admissible\nheuristic, using the same partition for each state and problem instance. Here\nwe extend the method and show that it applies to other domains as well. We also\npresent another method for additive heuristics which we call dynamically\npartitioned pattern databases. Here we partition the problem into disjoint\nsubproblems for each state of the search dynamically. We discuss the pros and\ncons of each of these methods and apply both methods to three different problem\ndomains: the sliding-tile puzzles, the 4-peg Towers of Hanoi problem, and\nfinding an optimal vertex cover of a graph. We find that in some problem\ndomains, static partitioning is most effective, while in others dynamic\npartitioning is a better choice. In each of these problem domains, either\nstatically partitioned or dynamically partitioned pattern database heuristics\nare the best known heuristics for the problem.\n"
  },
  {
    "id": "1107.0051",
    "title": "On Prediction Using Variable Order Markov Models",
    "abstract": "  This paper is concerned with algorithms for prediction of discrete sequences\nover a finite alphabet, using variable order Markov models. The class of such\nalgorithms is large and in principle includes any lossless compression\nalgorithm. We focus on six prominent prediction algorithms, including Context\nTree Weighting (CTW), Prediction by Partial Match (PPM) and Probabilistic\nSuffix Trees (PSTs). We discuss the properties of these algorithms and compare\ntheir performance using real life sequences from three domains: proteins,\nEnglish text and music pieces. The comparison is made with respect to\nprediction quality as measured by the average log-loss. We also compare\nclassification algorithms based on these predictors with respect to a number of\nlarge protein classification tasks. Our results indicate that a \"decomposed\"\nCTW (a variant of the CTW algorithm) and PPM outperform all other algorithms in\nsequence prediction tasks. Somewhat surprisingly, a different algorithm, which\nis a modification of the Lempel-Ziv compression algorithm, significantly\noutperforms all algorithms on the protein classification problems.\n"
  },
  {
    "id": "1107.0052",
    "title": "Ordered Landmarks in Planning",
    "abstract": "  Many known planning tasks have inherent constraints concerning the best order\nin which to achieve the goals. A number of research efforts have been made to\ndetect such constraints and to use them for guiding search, in the hope of\nspeeding up the planning process. We go beyond the previous approaches by\nconsidering ordering constraints not only over the (top-level) goals, but also\nover the sub-goals that will necessarily arise during planning. Landmarks are\nfacts that must be true at some point in every valid solution plan. We extend\nKoehler and Hoffmann's definition of reasonable orders between top level goals\nto the more general case of landmarks. We show how landmarks can be found, how\ntheir reasonable orders can be approximated, and how this information can be\nused to decompose a given planning task into several smaller sub-tasks. Our\nmethodology is completely domain- and planner-independent. The implementation\ndemonstrates that the approach can yield significant runtime performance\nimprovements when used as a control loop around state-of-the-art sub-optimal\nplanning systems, as exemplified by FF and LPG.\n"
  },
  {
    "id": "1107.0053",
    "title": "Finding Approximate POMDP solutions Through Belief Compression",
    "abstract": "  Standard value function approaches to finding policies for Partially\nObservable Markov Decision Processes (POMDPs) are generally considered to be\nintractable for large models. The intractability of these algorithms is to a\nlarge extent a consequence of computing an exact, optimal policy over the\nentire belief space. However, in real-world POMDP problems, computing the\noptimal policy for the full belief space is often unnecessary for good control\neven for problems with complicated policy classes. The beliefs experienced by\nthe controller often lie near a structured, low-dimensional subspace embedded\nin the high-dimensional belief space. Finding a good approximation to the\noptimal value function for only this subspace can be much easier than computing\nthe full value function. We introduce a new method for solving large-scale\nPOMDPs by reducing the dimensionality of the belief space. We use Exponential\nfamily Principal Components Analysis (Collins, Dasgupta and Schapire, 2002) to\nrepresent sparse, high-dimensional belief spaces using small sets of learned\nfeatures of the belief state. We then plan only in terms of the low-dimensional\nbelief features. By planning in this low-dimensional space, we can find\npolicies for POMDP models that are orders of magnitude larger than models that\ncan be handled by conventional techniques. We demonstrate the use of this\nalgorithm on a synthetic problem and on mobile robot navigation tasks.\n"
  },
  {
    "id": "1107.0054",
    "title": "A Comprehensive Trainable Error Model for Sung Music Queries",
    "abstract": "  We propose a model for errors in sung queries, a variant of the hidden Markov\nmodel (HMM). This is a solution to the problem of identifying the degree of\nsimilarity between a (typically error-laden) sung query and a potential target\nin a database of musical works, an important problem in the field of music\ninformation retrieval. Similarity metrics are a critical component of\nquery-by-humming (QBH) applications which search audio and multimedia databases\nfor strong matches to oral queries. Our model comprehensively expresses the\ntypes of error or variation between target and query: cumulative and\nnon-cumulative local errors, transposition, tempo and tempo changes,\ninsertions, deletions and modulation. The model is not only expressive, but\nautomatically trainable, or able to learn and generalize from query examples.\nWe present results of simulations, designed to assess the discriminatory\npotential of the model, and tests with real sung queries, to demonstrate\nrelevance to real-world applications.\n"
  },
  {
    "id": "1107.0055",
    "title": "Phase Transitions and Backbones of the Asymmetric Traveling Salesman\n  Problem",
    "abstract": "  In recent years, there has been much interest in phase transitions of\ncombinatorial problems. Phase transitions have been successfully used to\nanalyze combinatorial optimization problems, characterize their typical-case\nfeatures and locate the hardest problem instances. In this paper, we study\nphase transitions of the asymmetric Traveling Salesman Problem (ATSP), an\nNP-hard combinatorial optimization problem that has many real-world\napplications. Using random instances of up to 1,500 cities in which intercity\ndistances are uniformly distributed, we empirically show that many properties\nof the problem, including the optimal tour cost and backbone size, experience\nsharp transitions as the precision of intercity distances increases across a\ncritical value. Our experimental results on the costs of the ATSP tours and\nassignment problem agree with the theoretical result that the asymptotic cost\nof assignment problem is pi ^2 /6 the number of cities goes to infinity. In\naddition, we show that the average computational cost of the well-known\nbranch-and-bound subtour elimination algorithm for the problem also exhibits a\nthrashing behavior, transitioning from easy to difficult as the distance\nprecision increases. These results answer positively an open question regarding\nthe existence of phase transitions in the ATSP, and provide guidance on how\ndifficult ATSP problem instances should be generated.\n"
  },
  {
    "id": "1107.0134",
    "title": "The Influence of Global Constraints on Similarity Measures for\n  Time-Series Databases",
    "abstract": "  A time series consists of a series of values or events obtained over repeated\nmeasurements in time. Analysis of time series represents and important tool in\nmany application areas, such as stock market analysis, process and quality\ncontrol, observation of natural phenomena, medical treatments, etc. A vital\ncomponent in many types of time-series analysis is the choice of an appropriate\ndistance/similarity measure. Numerous measures have been proposed to date, with\nthe most successful ones based on dynamic programming. Being of quadratic time\ncomplexity, however, global constraints are often employed to limit the search\nspace in the matrix during the dynamic programming procedure, in order to speed\nup computation. Furthermore, it has been reported that such constrained\nmeasures can also achieve better accuracy. In this paper, we investigate two\nrepresentative time-series distance/similarity measures based on dynamic\nprogramming, Dynamic Time Warping (DTW) and Longest Common Subsequence (LCS),\nand the effects of global constraints on them. Through extensive experiments on\na large number of time-series data sets, we demonstrate how global constrains\ncan significantly reduce the computation time of DTW and LCS. We also show\nthat, if the constraint parameter is tight enough (less than 10-15% of\ntime-series length), the constrained measure becomes significantly different\nfrom its unconstrained counterpart, in the sense of producing qualitatively\ndifferent 1-nearest neighbor graphs. This observation explains the potential\nfor accuracy gains when using constrained measures, highlighting the need for\ncareful tuning of constraint parameters in order to achieve a good trade-off\nbetween speed and accuracy.\n"
  },
  {
    "id": "1107.0194",
    "title": "Law of Connectivity in Machine Learning",
    "abstract": "  We present in this paper our law that there is always a connection present\nbetween two entities, with a selfconnection being present at least in each\nnode. An entity is an object, physical or imaginary, that is connected by a\npath (or connection) and which is important for achieving the desired result of\nthe scenario. In machine learning, we state that for any scenario, a subject\nentity is always, directly or indirectly, connected and affected by single or\nmultiple independent / dependent entities, and their impact on the subject\nentity is dependent on various factors falling into the categories such as the\nexistenc\n"
  },
  {
    "id": "1107.0268",
    "title": "Simple Algorithm Portfolio for SAT",
    "abstract": "  The importance of algorithm portfolio techniques for SAT has long been noted,\nand a number of very successful systems have been devised, including the most\nsuccessful one --- SATzilla. However, all these systems are quite complex (to\nunderstand, reimplement, or modify). In this paper we propose a new algorithm\nportfolio for SAT that is extremely simple, but in the same time so efficient\nthat it outperforms SATzilla. For a new SAT instance to be solved, our\nportfolio finds its k-nearest neighbors from the training set and invokes a\nsolver that performs the best at those instances. The main distinguishing\nfeature of our algorithm portfolio is the locality of the selection procedure\n--- the selection of a SAT solver is based only on few instances similar to the\ninput one.\n"
  },
  {
    "id": "1107.1020",
    "title": "A Novel Multicriteria Group Decision Making Approach With Intuitionistic\n  Fuzzy SIR Method",
    "abstract": "  The superiority and inferiority ranking (SIR) method is a generation of the\nwell-known PROMETHEE method, which can be more efficient to deal with\nmulti-criterion decision making (MCDM) problem. Intuitionistic fuzzy sets\n(IFSs), as an important extension of fuzzy sets (IFs), include both membership\nfunctions and non-membership functions and can be used to, more precisely\ndescribe uncertain information. In real world, decision situations are usually\nunder uncertain environment and involve multiple individuals who have their own\npoints of view on handing of decision problems. In order to solve uncertainty\ngroup MCDM problem, we propose a novel intuitionistic fuzzy SIR method in this\npaper. This approach uses intuitionistic fuzzy aggregation operators and SIR\nranking methods to handle uncertain information; integrate individual opinions\ninto group opinions; make decisions on multiple-criterion; and finally\nstructure a specific decision map. The proposed approach is illustrated in a\nsimulation of group decision making problem related to supply chain management.\n"
  },
  {
    "id": "1107.1686",
    "title": "Proceedings of the Doctoral Consortium and Poster Session of the 5th\n  International Symposium on Rules (RuleML 2011@IJCAI)",
    "abstract": "  This volume contains the papers presented at the first edition of the\nDoctoral Consortium of the 5th International Symposium on Rules (RuleML\n2011@IJCAI) held on July 19th, 2011 in Barcelona, as well as the poster session\npapers of the RuleML 2011@IJCAI main conference.\n"
  },
  {
    "id": "1107.1950",
    "title": "Knowledge Embedding and Retrieval Strategies in an Informledge System",
    "abstract": "  Informledge System (ILS) is a knowledge network with autonomous nodes and\nintelligent links that integrate and structure the pieces of knowledge. In this\npaper, we put forward the strategies for knowledge embedding and retrieval in\nan ILS. ILS is a powerful knowledge network system dealing with logical storage\nand connectivity of information units to form knowledge using autonomous nodes\nand multi-lateral links. In ILS, the autonomous nodes known as Knowledge\nNetwork Nodes (KNN)s play vital roles which are not only used in storage,\nparsing and in forming the multi-lateral linkages between knowledge points but\nalso in helping the realization of intelligent retrieval of linked information\nunits in the form of knowledge. Knowledge built in to the ILS forms the shape\nof sphere. The intelligence incorporated into the links of a KNN helps in\nretrieving various knowledge threads from a specific set of KNNs. A developed\nentity of information realized through KNN forms in to the shape of a knowledge\ncone\n"
  },
  {
    "id": "1107.2086",
    "title": "Extend Commitment Protocols with Temporal Regulations: Why and How",
    "abstract": "  The proposal of Elisa Marengo's thesis is to extend commitment protocols to\nexplicitly account for temporal regulations. This extension will satisfy two\nneeds: (1) it will allow representing, in a flexible and modular way, temporal\nregulations with a normative force, posed on the interaction, so as to\nrepresent conventions, laws and suchlike; (2) it will allow committing to\ncomplex conditions, which describe not only what will be achieved but to some\nextent also how. These two aspects will be deeply investigated in the proposal\nof a unified framework, which is part of the ongoing work and will be included\nin the thesis.\n"
  },
  {
    "id": "1107.2087",
    "title": "Rule-Based Semantic Sensing",
    "abstract": "  Rule-Based Systems have been in use for decades to solve a variety of\nproblems but not in the sensor informatics domain. Rules aid the aggregation of\nlow-level sensor readings to form a more complete picture of the real world and\nhelp to address 10 identified challenges for sensor network middleware. This\npaper presents the reader with an overview of a system architecture and a pilot\napplication to demonstrate the usefulness of a system integrating rules with\nsensor middleware.\n"
  },
  {
    "id": "1107.2088",
    "title": "Advancing Multi-Context Systems by Inconsistency Management",
    "abstract": "  Multi-Context Systems are an expressive formalism to model (possibly)\nnon-monotonic information exchange between heterogeneous knowledge bases. Such\ninformation exchange, however, often comes with unforseen side-effects leading\nto violation of constraints, making the system inconsistent, and thus unusable.\nAlthough there are many approaches to assess and repair a single inconsistent\nknowledge base, the heterogeneous nature of Multi-Context Systems poses\nproblems which have not yet been addressed in a satisfying way: How to identify\nand explain a inconsistency that spreads over multiple knowledge bases with\ndifferent logical formalisms (e.g., logic programs and ontologies)? What are\nthe causes of inconsistency if inference/information exchange is non-monotonic\n(e.g., absent information as cause)? How to deal with inconsistency if access\nto knowledge bases is restricted (e.g., companies exchange information, but do\nnot allow arbitrary modifications to their knowledge bases)? Many traditional\napproaches solely aim for a consistent system, but automatic removal of\ninconsistency is not always desireable. Therefore a human operator has to be\nsupported in finding the erroneous parts contributing to the inconsistency. In\nmy thesis those issues will be adressed mainly from a foundational perspective,\nwhile our research project also provides algorithms and prototype\nimplementations.\n"
  },
  {
    "id": "1107.2089",
    "title": "Rule-based query answering method for a knowledge base of economic\n  crimes",
    "abstract": "  We present a description of the PhD thesis which aims to propose a rule-based\nquery answering method for relational data. In this approach we use an\nadditional knowledge which is represented as a set of rules and describes the\nsource data at concept (ontological) level. Queries are posed in the terms of\nabstract level. We present two methods. The first one uses hybrid reasoning and\nthe second one exploits only forward chaining. These two methods are\ndemonstrated by the prototypical implementation of the system coupled with the\nJess engine. Tests are performed on the knowledge base of the selected economic\ncrimes: fraudulent disbursement and money laundering.\n"
  },
  {
    "id": "1107.2090",
    "title": "Semantic-ontological combination of Business Rules and Business\n  Processes in IT Service Management",
    "abstract": "  IT Service Management deals with managing a broad range of items related to\ncomplex system environments. As there is both, a close connection to business\ninterests and IT infrastructure, the application of semantic expressions which\nare seamlessly integrated within applications for managing ITSM environments,\ncan help to improve transparency and profitability. This paper focuses on the\nchallenges regarding the integration of semantics and ontologies within ITSM\nenvironments. It will describe the paradigm of relationships and inheritance\nwithin complex service trees and will present an approach of ontologically\nexpressing them. Furthermore, the application of SBVR-based rules as executable\nSQL triggers will be discussed. Finally, the broad range of topics for further\nresearch, derived from the findings, will be presented.\n"
  },
  {
    "id": "1107.2997",
    "title": "An Ontology-driven Framework for Supporting Complex Decision Process",
    "abstract": "  The study proposes a framework of ONTOlogy-based Group Decision Support\nSystem (ONTOGDSS) for decision process which exhibits the complex structure of\ndecision-problem and decision-group. It is capable of reducing the complexity\nof problem structure and group relations. The system allows decision makers to\nparticipate in group decision-making through the web environment, via the\nontology relation. It facilitates the management of decision process as a\nwhole, from criteria generation, alternative evaluation, and opinion\ninteraction to decision aggregation. The embedded ontology structure in\nONTOGDSS provides the important formal description features to facilitate\ndecision analysis and verification. It examines the software architecture, the\nselection methods, the decision path, etc. Finally, the ontology application of\nthis system is illustrated with specific real case to demonstrate its\npotentials towards decision-making development.\n"
  },
  {
    "id": "1107.3302",
    "title": "A Temporal Neuro-Fuzzy Monitoring System to Manufacturing Systems",
    "abstract": "  Fault diagnosis and failure prognosis are essential techniques in improving\nthe safety of many manufacturing systems. Therefore, on-line fault detection\nand isolation is one of the most important tasks in safety-critical and\nintelligent control systems. Computational intelligence techniques are being\ninvestigated as extension of the traditional fault diagnosis methods. This\npaper discusses the Temporal Neuro-Fuzzy Systems (TNFS) fault diagnosis within\nan application study of a manufacturing system. The key issues of finding a\nsuitable structure for detecting and isolating ten realistic actuator faults\nare described. Within this framework, data-processing interactive software of\nsimulation baptized NEFDIAG (NEuro Fuzzy DIAGnosis) version 1.0 is developed.\n  This software devoted primarily to creation, training and test of a\nclassification Neuro-Fuzzy system of industrial process failures. NEFDIAG can\nbe represented like a special type of fuzzy perceptron, with three layers used\nto classify patterns and failures. The system selected is the workshop of\nSCIMAT clinker, cement factory in Algeria.\n"
  },
  {
    "id": "1107.3663",
    "title": "Towards Open-Text Semantic Parsing via Multi-Task Learning of Structured\n  Embeddings",
    "abstract": "  Open-text (or open-domain) semantic parsers are designed to interpret any\nstatement in natural language by inferring a corresponding meaning\nrepresentation (MR). Unfortunately, large scale systems cannot be easily\nmachine-learned due to lack of directly supervised data. We propose here a\nmethod that learns to assign MRs to a wide range of text (using a dictionary of\nmore than 70,000 words, which are mapped to more than 40,000 entities) thanks\nto a training scheme that combines learning from WordNet and ConceptNet with\nlearning from raw text. The model learns structured embeddings of words,\nentities and MRs via a multi-task training process operating on these diverse\nsources of data that integrates all the learnt knowledge into a single system.\nThis work ends up combining methods for knowledge acquisition, semantic\nparsing, and word-sense disambiguation. Experiments on various tasks indicate\nthat our approach is indeed successful and can form a basis for future more\nsophisticated systems.\n"
  },
  {
    "id": "1107.3894",
    "title": "Online Anomaly Detection Systems Using Incremental Commute Time",
    "abstract": "  Commute Time Distance (CTD) is a random walk based metric on graphs. CTD has\nfound widespread applications in many domains including personalized search,\ncollaborative filtering and making search engines robust against manipulation.\nOur interest is inspired by the use of CTD as a metric for anomaly detection.\nIt has been shown that CTD can be used to simultaneously identify both global\nand local anomalies. Here we propose an accurate and efficient approximation\nfor computing the CTD in an incremental fashion in order to facilitate\nreal-time applications. An online anomaly detection algorithm is designed where\nthe CTD of each new arriving data point to any point in the current graph can\nbe estimated in constant time ensuring a real-time response. Moreover, the\nproposed approach can also be applied in many other applications that utilize\ncommute time distance.\n"
  },
  {
    "id": "1107.4035",
    "title": "Towards Completely Lifted Search-based Probabilistic Inference",
    "abstract": "  The promise of lifted probabilistic inference is to carry out probabilistic\ninference in a relational probabilistic model without needing to reason about\neach individual separately (grounding out the representation) by treating the\nundistinguished individuals as a block. Current exact methods still need to\nground out in some cases, typically because the representation of the\nintermediate results is not closed under the lifted operations. We set out to\nanswer the question as to whether there is some fundamental reason why lifted\nalgorithms would need to ground out undifferentiated individuals. We have two\nmain results: (1) We completely characterize the cases where grounding is\npolynomial in a population size, and show how we can do lifted inference in\ntime polynomial in the logarithm of the population size for these cases. (2)\nFor the case of no-argument and single-argument parametrized random variables\nwhere the grounding is not polynomial in a population size, we present lifted\ninference which is polynomial in the population size whereas grounding is\nexponential. Neither of these cases requires reasoning separately about the\nindividuals that are not explicitly mentioned.\n"
  },
  {
    "id": "1107.4161",
    "title": "Local Optima Networks of the Quadratic Assignment Problem",
    "abstract": "  Using a recently proposed model for combinatorial landscapes, Local Optima\nNetworks (LON), we conduct a thorough analysis of two types of instances of the\nQuadratic Assignment Problem (QAP). This network model is a reduction of the\nlandscape in which the nodes correspond to the local optima, and the edges\naccount for the notion of adjacency between their basins of attraction. The\nmodel was inspired by the notion of 'inherent network' of potential energy\nsurfaces proposed in physical-chemistry. The local optima networks extracted\nfrom the so called uniform and real-like QAP instances, show features clearly\ndistinguishing these two types of instances. Apart from a clear confirmation\nthat the search difficulty increases with the problem dimension, the analysis\nprovides new confirming evidence explaining why the real-like instances are\neasier to solve exactly using heuristic search, while the uniform instances are\neasier to solve approximately. Although the local optima network model is still\nunder development, we argue that it provides a novel view of combinatorial\nlandscapes, opening up the possibilities for new analytical tools and\nunderstanding of problem difficulty in combinatorial optimization.\n"
  },
  {
    "id": "1107.4162",
    "title": "Local Optima Networks of NK Landscapes with Neutrality",
    "abstract": "  In previous work we have introduced a network-based model that abstracts many\ndetails of the underlying landscape and compresses the landscape information\ninto a weighted, oriented graph which we call the local optima network. The\nvertices of this graph are the local optima of the given fitness landscape,\nwhile the arcs are transition probabilities between local optima basins. Here\nwe extend this formalism to neutral fitness landscapes, which are common in\ndifficult combinatorial search spaces. By using two known neutral variants of\nthe NK family (i.e. NKp and NKq) in which the amount of neutrality can be tuned\nby a parameter, we show that our new definitions of the optima networks and the\nassociated basins are consistent with the previous definitions for the\nnon-neutral case. Moreover, our empirical study and statistical analysis show\nthat the features of neutral landscapes interpolate smoothly between landscapes\nwith maximum neutrality and non-neutral ones. We found some unknown structural\ndifferences between the two studied families of neutral landscapes. But\noverall, the network features studied confirmed that neutrality, in landscapes\nwith percolating neutral networks, may enhance heuristic search. Our current\nmethodology requires the exhaustive enumeration of the underlying search space.\nTherefore, sampling techniques should be developed before this analysis can\nhave practical implications. We argue, however, that the proposed model offers\na new perspective into the problem difficulty of combinatorial optimization\nproblems and may inspire the design of more effective search heuristics.\n"
  },
  {
    "id": "1107.4163",
    "title": "Centric selection: a way to tune the exploration/exploitation trade-off",
    "abstract": "  In this paper, we study the exploration / exploitation trade-off in cellular\ngenetic algorithms. We define a new selection scheme, the centric selection,\nwhich is tunable and allows controlling the selective pressure with a single\nparameter. The equilibrium model is used to study the influence of the centric\nselection on the selective pressure and a new model which takes into account\nproblem dependent statistics and selective pressure in order to deal with the\nexploration / exploitation trade-off is proposed: the punctuated equilibria\nmodel. Performances on the quadratic assignment problem and NK-Landscapes put\nin evidence an optimal exploration / exploitation trade-off on both of the\nclasses of problems. The punctuated equilibria model is used to explain these\nresults.\n"
  },
  {
    "id": "1107.4164",
    "title": "NK landscapes difficulty and Negative Slope Coefficient: How Sampling\n  Influences the Results",
    "abstract": "  Negative Slope Coefficient is an indicator of problem hardness that has been\nintroduced in 2004 and that has returned promising results on a large set of\nproblems. It is based on the concept of fitness cloud and works by partitioning\nthe cloud into a number of bins representing as many different regions of the\nfitness landscape. The measure is calculated by joining the bins centroids by\nsegments and summing all their negative slopes. In this paper, for the first\ntime, we point out a potential problem of the Negative Slope Coefficient: we\nstudy its value for different instances of the well known NK-landscapes and we\nshow how this indicator is dramatically influenced by the minimum number of\npoints contained into a bin. Successively, we formally justify this behavior of\nthe Negative Slope Coefficient and we discuss pros and cons of this measure.\n"
  },
  {
    "id": "1107.4303",
    "title": "Interactive ontology debugging: two query strategies for efficient fault\n  localization",
    "abstract": "  Effective debugging of ontologies is an important prerequisite for their\nbroad application, especially in areas that rely on everyday users to create\nand maintain knowledge bases, such as the Semantic Web. In such systems\nontologies capture formalized vocabularies of terms shared by its users.\nHowever in many cases users have different local views of the domain, i.e. of\nthe context in which a given term is used. Inappropriate usage of terms\ntogether with natural complications when formulating and understanding logical\ndescriptions may result in faulty ontologies. Recent ontology debugging\napproaches use diagnosis methods to identify causes of the faults. In most\ndebugging scenarios these methods return many alternative diagnoses, thus\nplacing the burden of fault localization on the user. This paper demonstrates\nhow the target diagnosis can be identified by performing a sequence of\nobservations, that is, by querying an oracle about entailments of the target\nontology. To identify the best query we propose two query selection strategies:\na simple \"split-in-half\" strategy and an entropy-based strategy. The latter\nallows knowledge about typical user errors to be exploited to minimize the\nnumber of queries. Our evaluation showed that the entropy-based method\nsignificantly reduces the number of required queries compared to the\n\"split-in-half\" approach. We experimented with different probability\ndistributions of user errors and different qualities of the a-priori\nprobabilities. Our measurements demonstrated the superiority of entropy-based\nquery selection even in cases where all fault probabilities are equal, i.e.\nwhere no information about typical user errors is available.\n"
  },
  {
    "id": "1107.4502",
    "title": "MeLinDa: an interlinking framework for the web of data",
    "abstract": "  The web of data consists of data published on the web in such a way that they\ncan be interpreted and connected together. It is thus critical to establish\nlinks between these data, both for the web of data and for the semantic web\nthat it contributes to feed. We consider here the various techniques developed\nfor that purpose and analyze their commonalities and differences. We propose a\ngeneral framework and show how the diverse techniques fit in the framework.\nFrom this framework we consider the relation between data interlinking and\nontology matching. Although, they can be considered similar at a certain level\n(they both relate formal entities), they serve different purposes, but would\nfind a mutual benefit at collaborating. We thus present a scheme under which it\nis possible for data linking tools to take advantage of ontology alignments.\n"
  },
  {
    "id": "1107.4553",
    "title": "Solving Linear Constraints in Elementary Abelian p-Groups of Symmetries",
    "abstract": "  Symmetries occur naturally in CSP or SAT problems and are not very difficult\nto discover, but using them to prune the search space tends to be very\nchallenging. Indeed, this usually requires finding specific elements in a group\nof symmetries that can be huge, and the problem of their very existence is\nNP-hard. We formulate such an existence problem as a constraint problem on one\nvariable (the symmetry to be used) ranging over a group, and try to find\nrestrictions that may be solved in polynomial time. By considering a simple\nform of constraints (restricted by a cardinality k) and the class of groups\nthat have the structure of Fp-vector spaces, we propose a partial algorithm\nbased on linear algebra. This polynomial algorithm always applies when k=p=2,\nbut may fail otherwise as we prove the problem to be NP-hard for all other\nvalues of k and p. Experiments show that this approach though restricted should\nallow for an efficient use of at least some groups of symmetries. We conclude\nwith a few directions to be explored to efficiently solve this problem on the\ngeneral case.\n"
  },
  {
    "id": "1107.4865",
    "title": "Actual Causation in CP-logic",
    "abstract": "  Given a causal model of some domain and a particular story that has taken\nplace in this domain, the problem of actual causation is deciding which of the\npossible causes for some effect actually caused it. One of the most influential\napproaches to this problem has been developed by Halpern and Pearl in the\ncontext of structural models. In this paper, I argue that this is actually not\nthe best setting for studying this problem. As an alternative, I offer the\nprobabilistic logic programming language of CP-logic. Unlike structural models,\nCP-logic incorporates the deviant/default distinction that is generally\nconsidered an important aspect of actual causation, and it has an explicitly\ndynamic semantics, which helps to formalize the stories that serve as input to\nan actual causation problem.\n"
  },
  {
    "id": "1107.4937",
    "title": "Instantiation Schemes for Nested Theories",
    "abstract": "  This paper investigates under which conditions instantiation-based proof\nprocedures can be combined in a nested way, in order to mechanically construct\nnew instantiation procedures for richer theories. Interesting applications in\nthe field of verification are emphasized, particularly for handling extensions\nof the theory of arrays.\n"
  },
  {
    "id": "1107.5462",
    "title": "HyFlex: A Benchmark Framework for Cross-domain Heuristic Search",
    "abstract": "  Automating the design of heuristic search methods is an active research field\nwithin computer science, artificial intelligence and operational research. In\norder to make these methods more generally applicable, it is important to\neliminate or reduce the role of the human expert in the process of designing an\neffective methodology to solve a given computational search problem.\nResearchers developing such methodologies are often constrained on the number\nof problem domains on which to test their adaptive, self-configuring\nalgorithms; which can be explained by the inherent difficulty of implementing\ntheir corresponding domain specific software components.\n  This paper presents HyFlex, a software framework for the development of\ncross-domain search methodologies. The framework features a common software\ninterface for dealing with different combinatorial optimisation problems, and\nprovides the algorithm components that are problem specific. In this way, the\nalgorithm designer does not require a detailed knowledge the problem domains,\nand thus can concentrate his/her efforts in designing adaptive general-purpose\nheuristic search algorithms. Four hard combinatorial problems are fully\nimplemented (maximum satisfiability, one dimensional bin packing, permutation\nflow shop and personnel scheduling), each containing a varied set of instance\ndata (including real-world industrial applications) and an extensive set of\nproblem specific heuristics and search operators. The framework forms the basis\nfor the first International Cross-domain Heuristic Search Challenge (CHeSC),\nand it is currently in use by the international research community. In summary,\nHyFlex represents a valuable new benchmark of heuristic search generality, with\nwhich adaptive cross-domain algorithms are being easily developed, and reliably\ncompared.\n"
  },
  {
    "id": "1107.5474",
    "title": "Selecting Attributes for Sport Forecasting using Formal Concept Analysis",
    "abstract": "  In order to address complex systems, apply pattern recongnition on their\nevolution could play an key role to understand their dynamics. Global patterns\nare required to detect emergent concepts and trends, some of them with\nqualitative nature. Formal Concept Analysis (FCA) is a theory whose goal is to\ndiscover and to extract Knowledge from qualitative data. It provides tools for\nreasoning with implication basis (and association rules). Implications and\nassociation rules are usefull to reasoning on previously selected attributes,\nproviding a formal foundation for logical reasoning. In this paper we analyse\nhow to apply FCA reasoning to increase confidence in sports betting, by means\nof detecting temporal regularities from data. It is applied to build a\nKnowledge-Based system for confidence reasoning.\n"
  },
  {
    "id": "1107.5766",
    "title": "Information, Utility & Bounded Rationality",
    "abstract": "  Perfectly rational decision-makers maximize expected utility, but crucially\nignore the resource costs incurred when determining optimal actions. Here we\nemploy an axiomatic framework for bounded rational decision-making based on a\nthermodynamic interpretation of resource costs as information costs. This leads\nto a variational \"free utility\" principle akin to thermodynamical free energy\nthat trades off utility and information costs. We show that bounded optimal\ncontrol solutions can be derived from this variational principle, which leads\nin general to stochastic policies. Furthermore, we show that risk-sensitive and\nrobust (minimax) control schemes fall out naturally from this framework if the\nenvironment is considered as a bounded rational and perfectly rational\nopponent, respectively. When resource costs are ignored, the maximum expected\nutility principle is recovered.\n"
  },
  {
    "id": "1107.5930",
    "title": "Technical Note: Towards ROC Curves in Cost Space",
    "abstract": "  ROC curves and cost curves are two popular ways of visualising classifier\nperformance, finding appropriate thresholds according to the operating\ncondition, and deriving useful aggregated measures such as the area under the\nROC curve (AUC) or the area under the optimal cost curve. In this note we\npresent some new findings and connections between ROC space and cost space, by\nusing the expected loss over a range of operating conditions. In particular, we\nshow that ROC curves can be transferred to cost space by means of a very\nnatural way of understanding how thresholds should be chosen, by selecting the\nthreshold such that the proportion of positive predictions equals the operating\ncondition (either in the form of cost proportion or skew). We call these new\ncurves {ROC Cost Curves}, and we demonstrate that the expected loss as measured\nby the area under these curves is linearly related to AUC. This opens up a\nseries of new possibilities and clarifies the notion of cost curve and its\nrelation to ROC analysis. In addition, we show that for a classifier that\nassigns the scores in an evenly-spaced way, these curves are equal to the Brier\nCurves. As a result, this establishes the first clear connection between AUC\nand the Brier score.\n"
  },
  {
    "id": "1108.0155",
    "title": "Reasoning in the OWL 2 Full Ontology Language using First-Order\n  Automated Theorem Proving",
    "abstract": "  OWL 2 has been standardized by the World Wide Web Consortium (W3C) as a\nfamily of ontology languages for the Semantic Web. The most expressive of these\nlanguages is OWL 2 Full, but to date no reasoner has been implemented for this\nlanguage. Consistency and entailment checking are known to be undecidable for\nOWL 2 Full. We have translated a large fragment of the OWL 2 Full semantics\ninto first-order logic, and used automated theorem proving systems to do\nreasoning based on this theory. The results are promising, and indicate that\nthis approach can be applied in practice for effective OWL reasoning, beyond\nthe capabilities of current Semantic Web reasoners.\n  This is an extended version of a paper with the same title that has been\npublished at CADE 2011, LNAI 6803, pp. 446-460. The extended version provides\nappendices with additional resources that were used in the reported evaluation.\n"
  },
  {
    "id": "1108.1488",
    "title": "'Just Enough' Ontology Engineering",
    "abstract": "  This paper introduces 'just enough' principles and 'systems engineering'\napproach to the practice of ontology development to provide a minimal yet\ncomplete, lightweight, agile and integrated development process, supportive of\nstakeholder management and implementation independence.\n"
  },
  {
    "id": "1108.2865",
    "title": "Conscious Machines and Consciousness Oriented Programming",
    "abstract": "  In this paper, we investigate the following question: how could you write\nsuch computer programs that can work like conscious beings? The motivation\nbehind this question is that we want to create such applications that can see\nthe future. The aim of this paper is to provide an overall conceptual framework\nfor this new approach to machine consciousness. So we introduce a new\nprogramming paradigm called Consciousness Oriented Programming (COP).\n"
  },
  {
    "id": "1108.3019",
    "title": "A First Approach on Modelling Staff Proactiveness in Retail Simulation\n  Models",
    "abstract": "  There has been a noticeable shift in the relative composition of the industry\nin the developed countries in recent years; manufacturing is decreasing while\nthe service sector is becoming more important. However, currently most\nsimulation models for investigating service systems are still built in the same\nway as manufacturing simulation models, using a process-oriented world view,\ni.e. they model the flow of passive entities through a system. These kinds of\nmodels allow studying aspects of operational management but are not well suited\nfor studying the dynamics that appear in service systems due to human\nbehaviour. For these kinds of studies we require tools that allow modelling the\nsystem and entities using an object-oriented world view, where intelligent\nobjects serve as abstract \"actors\" that are goal directed and can behave\nproactively. In our work we combine process-oriented discrete event simulation\nmodelling and object-oriented agent based simulation modelling to investigate\nthe impact of people management practices on retail productivity. In this\npaper, we reveal in a series of experiments what impact considering proactivity\ncan have on the output accuracy of simulation models of human centric systems.\nThe model and data we use for this investigation are based on a case study in a\nUK department store. We show that considering proactivity positively influences\nthe validity of these kinds of models and therefore allows analysts to make\nbetter recommendations regarding strategies to apply people management\npractises.\n"
  },
  {
    "id": "1108.3278",
    "title": "Reiter's Default Logic Is a Logic of Autoepistemic Reasoning And a Good\n  One, Too",
    "abstract": "  A fact apparently not observed earlier in the literature of nonmonotonic\nreasoning is that Reiter, in his default logic paper, did not directly\nformalize informal defaults. Instead, he translated a default into a certain\nnatural language proposition and provided a formalization of the latter. A few\nyears later, Moore noted that propositions like the one used by Reiter are\nfundamentally different than defaults and exhibit a certain autoepistemic\nnature. Thus, Reiter had developed his default logic as a formalization of\nautoepistemic propositions rather than of defaults.\n  The first goal of this paper is to show that some problems of Reiter's\ndefault logic as a formal way to reason about informal defaults are directly\nattributable to the autoepistemic nature of default logic and to the mismatch\nbetween informal defaults and the Reiter's formal defaults, the latter being a\nformal expression of the autoepistemic propositions Reiter used as a\nrepresentation of informal defaults.\n  The second goal of our paper is to compare the work of Reiter and Moore.\nWhile each of them attempted to formalize autoepistemic propositions, the modes\nof reasoning in their respective logics were different. We revisit Moore's and\nReiter's intuitions and present them from the perspective of autotheoremhood,\nwhere theories can include propositions referring to the theory's own theorems.\nWe then discuss the formalization of this perspective in the logics of Moore\nand Reiter, respectively, using the unifying semantic framework for default and\nautoepistemic logics that we developed earlier. We argue that Reiter's default\nlogic is a better formalization of Moore's intuitions about autoepistemic\npropositions than Moore's own autoepistemic logic.\n"
  },
  {
    "id": "1108.3279",
    "title": "Revisiting Epistemic Specifications",
    "abstract": "  In 1991, Michael Gelfond introduced the language of epistemic specifications.\nThe goal was to develop tools for modeling problems that require some form of\nmeta-reasoning, that is, reasoning over multiple possible worlds. Despite their\nrelevance to knowledge representation, epistemic specifications have received\nrelatively little attention so far. In this paper, we revisit the formalism of\nepistemic specification. We offer a new definition of the formalism, propose\nseveral semantics (one of which, under syntactic restrictions we assume, turns\nout to be equivalent to the original semantics by Gelfond), derive some\ncomplexity results and, finally, show the effectiveness of the formalism for\nmodeling problems requiring meta-reasoning considered recently by Faber and\nWoltran. All these results show that epistemic specifications deserve much more\nattention that has been afforded to them so far.\n"
  },
  {
    "id": "1108.3281",
    "title": "Origins of Answer-Set Programming - Some Background And Two Personal\n  Accounts",
    "abstract": "  We discuss the evolution of aspects of nonmonotonic reasoning towards the\ncomputational paradigm of answer-set programming (ASP). We give a general\noverview of the roots of ASP and follow up with the personal perspective on\nresearch developments that helped verbalize the main principles of ASP and\ndifferentiated it from the classical logic programming.\n"
  },
  {
    "id": "1108.3711",
    "title": "Doing Better Than UCT: Rational Monte Carlo Sampling in Trees",
    "abstract": "  UCT, a state-of-the art algorithm for Monte Carlo tree sampling (MCTS), is\nbased on UCB, a sampling policy for the Multi-armed Bandit Problem (MAB) that\nminimizes the accumulated regret. However, MCTS differs from MAB in that only\nthe final choice, rather than all arm pulls, brings a reward, that is, the\nsimple regret, as opposite to the cumulative regret, must be minimized. This\nongoing work aims at applying meta-reasoning techniques to MCTS, which is\nnon-trivial. We begin by introducing policies for multi-armed bandits with\nlower simple regret than UCB, and an algorithm for MCTS which combines\ncumulative and simple regret minimization and outperforms UCT. We also develop\na sampling scheme loosely based on a myopic version of perfect value of\ninformation. Finite-time and asymptotic analysis of the policies is provided,\nand the algorithms are compared empirically.\n"
  },
  {
    "id": "1108.3757",
    "title": "Self-Organizing Mixture Networks for Representation of Grayscale Digital\n  Images",
    "abstract": "  Self-Organizing Maps are commonly used for unsupervised learning purposes.\nThis paper is dedicated to the certain modification of SOM called SOMN\n(Self-Organizing Mixture Networks) used as a mechanism for representing\ngrayscale digital images. Any grayscale digital image regarded as a\ndistribution function can be approximated by the corresponding Gaussian\nmixture. In this paper, the use of SOMN is proposed in order to obtain such\napproximations for input grayscale images in unsupervised manner.\n"
  },
  {
    "id": "1108.4279",
    "title": "Detection and emergence",
    "abstract": "  Two different conceptions of emergence are reconciled as two instances of the\nphenomenon of detection. In the process of comparing these two conceptions, we\nfind that the notions of complexity and detection allow us to form a unified\ndefinition of emergence that clearly delineates the role of the observer.\n"
  },
  {
    "id": "1108.4804",
    "title": "dynPARTIX - A Dynamic Programming Reasoner for Abstract Argumentation",
    "abstract": "  The aim of this paper is to announce the release of a novel system for\nabstract argumentation which is based on decomposition and dynamic programming.\nWe provide first experimental evaluations to show the feasibility of this\napproach.\n"
  },
  {
    "id": "1108.4942",
    "title": "Making Use of Advances in Answer-Set Programming for Abstract\n  Argumentation Systems",
    "abstract": "  Dung's famous abstract argumentation frameworks represent the core formalism\nfor many problems and applications in the field of argumentation which\nsignificantly evolved within the last decade. Recent work in the field has thus\nfocused on implementations for these frameworks, whereby one of the main\napproaches is to use Answer-Set Programming (ASP). While some of the\nargumentation semantics can be nicely expressed within the ASP language, others\nrequired rather cumbersome encoding techniques. Recent advances in ASP systems,\nin particular, the metasp optimization frontend for the ASP-package\ngringo/claspD provides direct commands to filter answer sets satisfying certain\nsubset-minimality (or -maximality) constraints. This allows for much simpler\nencodings compared to the ones in standard ASP language. In this paper, we\nexperimentally compare the original encodings (for the argumentation semantics\nbased on preferred, semi-stable, and respectively, stage extensions) with new\nmetasp encodings. Moreover, we provide novel encodings for the recently\nintroduced resolution-based grounded semantics. Our experimental results\nindicate that the metasp approach works well in those cases where the\ncomplexity of the encoded problem is adequately mirrored within the metasp\napproach.\n"
  },
  {
    "id": "1108.5002",
    "title": "Verbal Characterization of Probabilistic Clusters using Minimal\n  Discriminative Propositions",
    "abstract": "  In a knowledge discovery process, interpretation and evaluation of the mined\nresults are indispensable in practice. In the case of data clustering, however,\nit is often difficult to see in what aspect each cluster has been formed. This\npaper proposes a method for automatic and objective characterization or\n\"verbalization\" of the clusters obtained by mixture models, in which we collect\nconjunctions of propositions (attribute-value pairs) that help us interpret or\nevaluate the clusters. The proposed method provides us with a new, in-depth and\nconsistent tool for cluster interpretation/evaluation, and works for various\ntypes of datasets including continuous attributes and missing values.\nExperimental results with a couple of standard datasets exhibit the utility of\nthe proposed method, and the importance of the feedbacks from the\ninterpretation/evaluation step.\n"
  },
  {
    "id": "1108.5250",
    "title": "Single-trial EEG Discrimination between Wrist and Finger Movement\n  Imagery and Execution in a Sensorimotor BCI",
    "abstract": "  A brain-computer interface (BCI) may be used to control a prosthetic or\northotic hand using neural activity from the brain. The core of this\nsensorimotor BCI lies in the interpretation of the neural information extracted\nfrom electroencephalogram (EEG). It is desired to improve on the interpretation\nof EEG to allow people with neuromuscular disorders to perform daily\nactivities. This paper investigates the possibility of discriminating between\nthe EEG associated with wrist and finger movements. The EEG was recorded from\ntest subjects as they executed and imagined five essential hand movements using\nboth hands. Independent component analysis (ICA) and time-frequency techniques\nwere used to extract spectral features based on event-related\n(de)synchronisation (ERD/ERS), while the Bhattacharyya distance (BD) was used\nfor feature reduction. Mahalanobis distance (MD) clustering and artificial\nneural networks (ANN) were used as classifiers and obtained average accuracies\nof 65 % and 71 % respectively. This shows that EEG discrimination between wrist\nand finger movements is possible. The research introduces a new combination of\nmotor tasks to BCI research.\n"
  },
  {
    "id": "1108.5586",
    "title": "FdConfig: A Constraint-Based Interactive Product Configurator",
    "abstract": "  We present a constraint-based approach to interactive product configuration.\nOur configurator tool FdConfig is based on feature models for the\nrepresentation of the product domain. Such models can be directly mapped into\nconstraint satisfaction problems and dealt with by appropriate constraint\nsolvers. During the interactive configuration process the user generates new\nconstraints as a result of his configuration decisions and even may retract\nconstraints posted earlier. We discuss the configuration process, explain the\nunderlying techniques and show optimizations.\n"
  },
  {
    "id": "1108.5626",
    "title": "Nested HEX-Programs",
    "abstract": "  Answer-Set Programming (ASP) is an established declarative programming\nparadigm. However, classical ASP lacks subprogram calls as in procedural\nprogramming, and access to external computations (like remote procedure calls)\nin general. The feature is desired for increasing modularity and---assuming\nproper access in place---(meta-)reasoning over subprogram results. While\nHEX-programs extend classical ASP with external source access, they do not\nsupport calls of (sub-)programs upfront. We present nested HEX-programs, which\nextend HEX-programs to serve the desired feature, in a user-friendly manner.\nNotably, the answer sets of called sub-programs can be individually accessed.\nThis is particularly useful for applications that need to reason over answer\nsets like belief set merging, user-defined aggregate functions, or preferences\nof answer sets.\n"
  },
  {
    "id": "1108.5717",
    "title": "Structure Selection from Streaming Relational Data",
    "abstract": "  Statistical relational learning techniques have been successfully applied in\na wide range of relational domains. In most of these applications, the human\ndesigners capitalized on their background knowledge by following a\ntrial-and-error trajectory, where relational features are manually defined by a\nhuman engineer, parameters are learned for those features on the training data,\nthe resulting model is validated, and the cycle repeats as the engineer adjusts\nthe set of features. This paper seeks to streamline application development in\nlarge relational domains by introducing a light-weight approach that\nefficiently evaluates relational features on pieces of the relational graph\nthat are streamed to it one at a time. We evaluate our approach on two social\nmedia tasks and demonstrate that it leads to more accurate models that are\nlearned faster.\n"
  },
  {
    "id": "1108.5794",
    "title": "A Constraint Logic Programming Approach for Computing Ordinal\n  Conditional Functions",
    "abstract": "  In order to give appropriate semantics to qualitative conditionals of the\nform \"if A then normally B\", ordinal conditional functions (OCFs) ranking the\npossible worlds according to their degree of plausibility can be used. An OCF\naccepting all conditionals of a knowledge base R can be characterized as the\nsolution of a constraint satisfaction problem. We present a high-level,\ndeclarative approach using constraint logic programming techniques for solving\nthis constraint satisfaction problem. In particular, the approach developed\nhere supports the generation of all minimal solutions; these minimal solutions\nare of special interest as they provide a basis for model-based inference from\nR.\n"
  },
  {
    "id": "1108.5825",
    "title": "Confidentiality-Preserving Data Publishing for Credulous Users by\n  Extended Abduction",
    "abstract": "  Publishing private data on external servers incurs the problem of how to\navoid unwanted disclosure of confidential data. We study a problem of\nconfidentiality in extended disjunctive logic programs and show how it can be\nsolved by extended abduction. In particular, we analyze how credulous\nnon-monotonic reasoning affects confidentiality.\n"
  },
  {
    "id": "1108.6007",
    "title": "Domain-specific Languages in a Finite Domain Constraint Programming\n  System",
    "abstract": "  In this paper, we present domain-specific languages (DSLs) that we devised\nfor their use in the implementation of a finite domain constraint programming\nsystem, available as library(clpfd) in SWI-Prolog and YAP-Prolog. These DSLs\nare used in propagator selection and constraint reification. In these areas,\nthey lead to concise specifications that are easy to read and reason about. At\ncompilation time, these specifications are translated to Prolog code, reducing\ninterpretative run-time overheads. The devised languages can be used in the\nimplementation of other finite domain constraint solvers as well and may\ncontribute to their correctness, conciseness and efficiency.\n"
  },
  {
    "id": "1108.6208",
    "title": "Coprocessor - a Standalone SAT Preprocessor",
    "abstract": "  In this work a stand-alone preprocessor for SAT is presented that is able to\nperform most of the known preprocessing techniques. Preprocessing a formula in\nSAT is important for performance since redundancy can be removed. The\npreprocessor is part of the SAT solver riss and is called Coprocessor. Not only\nriss, but also MiniSat 2.2 benefit from it, because the SatELite preprocessor\nof MiniSat does not implement recent techniques. By using more advanced\ntechniques, Coprocessor is able to reduce the redundancy in a formula further\nand improves the overall solving performance.\n"
  },
  {
    "id": "1109.1231",
    "title": "A Combinatorial Optimisation Approach to Designing Dual-Parented\n  Long-Reach Passive Optical Networks",
    "abstract": "  We present an application focused on the design of resilient long-reach\npassive optical networks. We specifically consider dual-parented networks\nwhereby each customer must be connected to two metro sites via local exchange\nsites. An important property of such a placement is resilience to single metro\nnode failure. The objective of the application is to determine the optimal\nposition of a set of metro nodes such that the total optical fibre length is\nminimized. We prove that this problem is NP-Complete. We present two\nalternative combinatorial optimisation approaches to finding an optimal metro\nnode placement using: a mixed integer linear programming (MIP) formulation of\nthe problem; and, a hybrid approach that uses clustering as a preprocessing\nstep. We consider a detailed case-study based on a network for Ireland. The\nhybrid approach scales well and finds solutions that are close to optimal, with\na runtime that is two orders-of-magnitude better than the MIP model.\n"
  },
  {
    "id": "1109.1314",
    "title": "Measuring Intelligence through Games",
    "abstract": "  Artificial general intelligence (AGI) refers to research aimed at tackling\nthe full problem of artificial intelligence, that is, create truly intelligent\nagents. This sets it apart from most AI research which aims at solving\nrelatively narrow domains, such as character recognition, motion planning, or\nincreasing player satisfaction in games. But how do we know when an agent is\ntruly intelligent? A common point of reference in the AGI community is Legg and\nHutter's formal definition of universal intelligence, which has the appeal of\nsimplicity and generality but is unfortunately incomputable. Games of various\nkinds are commonly used as benchmarks for \"narrow\" AI research, as they are\nconsidered to have many important properties. We argue that many of these\nproperties carry over to the testing of general intelligence as well. We then\nsketch how such testing could practically be carried out. The central part of\nthis sketch is an extension of universal intelligence to deal with finite time,\nand the use of sampling of the space of games expressed in a suitably biased\ngame description language.\n"
  },
  {
    "id": "1109.1498",
    "title": "Structured Knowledge Representation for Image Retrieval",
    "abstract": "  We propose a structured approach to the problem of retrieval of images by\ncontent and present a description logic that has been devised for the semantic\nindexing and retrieval of images containing complex objects. As other\napproaches do, we start from low-level features extracted with image analysis\nto detect and characterize regions in an image. However, in contrast with\nfeature-based approaches, we provide a syntax to describe segmented regions as\nbasic objects and complex objects as compositions of basic ones. Then we\nintroduce a companion extensional semantics for defining reasoning services,\nsuch as retrieval, classification, and subsumption. These services can be used\nfor both exact and approximate matching, using similarity measures. Using our\nlogical approach as a formal specification, we implemented a complete\nclient-server image retrieval system, which allows a user to pose both queries\nby sketch and queries by example. A set of experiments has been carried out on\na testbed of images to assess the retrieval capabilities of the system in\ncomparison with expert users ranking. Results are presented adopting a\nwell-established measure of quality borrowed from textual information\nretrieval.\n"
  },
  {
    "id": "1109.1922",
    "title": "Predicting the Energy Output of Wind Farms Based on Weather Data:\n  Important Variables and their Correlation",
    "abstract": "  Wind energy plays an increasing role in the supply of energy world-wide. The\nenergy output of a wind farm is highly dependent on the weather condition\npresent at the wind farm. If the output can be predicted more accurately,\nenergy suppliers can coordinate the collaborative production of different\nenergy sources more efficiently to avoid costly overproductions.\n  With this paper, we take a computer science perspective on energy prediction\nbased on weather data and analyze the important parameters as well as their\ncorrelation on the energy output. To deal with the interaction of the different\nparameters we use symbolic regression based on the genetic programming tool\nDataModeler.\n  Our studies are carried out on publicly available weather and energy data for\na wind farm in Australia. We reveal the correlation of the different variables\nfor the energy output. The model obtained for energy prediction gives a very\nreliable prediction of the energy output for newly given weather data.\n"
  },
  {
    "id": "1109.1966",
    "title": "The path inference filter: model-based low-latency map matching of probe\n  vehicle data",
    "abstract": "  We consider the problem of reconstructing vehicle trajectories from sparse\nsequences of GPS points, for which the sampling interval is between 10 seconds\nand 2 minutes. We introduce a new class of algorithms, called altogether path\ninference filter (PIF), that maps GPS data in real time, for a variety of\ntrade-offs and scenarios, and with a high throughput. Numerous prior approaches\nin map-matching can be shown to be special cases of the path inference filter\npresented in this article. We present an efficient procedure for automatically\ntraining the filter on new data, with or without ground truth observations. The\nframework is evaluated on a large San Francisco taxi dataset and is shown to\nimprove upon the current state of the art. This filter also provides insights\nabout driving patterns of drivers. The path inference filter has been deployed\nat an industrial scale inside the Mobile Millennium traffic information system,\nand is used to map fleets of data in San Francisco, Sacramento, Stockholm and\nPorto.\n"
  },
  {
    "id": "1109.2048",
    "title": "An Expressive Language and Efficient Execution System for Software\n  Agents",
    "abstract": "  Software agents can be used to automate many of the tedious, time-consuming\ninformation processing tasks that humans currently have to complete manually.\nHowever, to do so, agent plans must be capable of representing the myriad of\nactions and control flows required to perform those tasks. In addition, since\nthese tasks can require integrating multiple sources of remote information ?\ntypically, a slow, I/O-bound process ? it is desirable to make execution as\nefficient as possible. To address both of these needs, we present a flexible\nsoftware agent plan language and a highly parallel execution system that enable\nthe efficient execution of expressive agent plans. The plan language allows\ncomplex tasks to be more easily expressed by providing a variety of operators\nfor flexibly processing the data as well as supporting subplans (for\nmodularity) and recursion (for indeterminate looping). The executor is based on\na streaming dataflow model of execution to maximize the amount of operator and\ndata parallelism possible at runtime. We have implemented both the language and\nexecutor in a system called THESEUS. Our results from testing THESEUS show that\nstreaming dataflow execution can yield significant speedups over both\ntraditional serial (von Neumann) as well as non-streaming dataflow-style\nexecution that existing software and robot agent execution systems currently\nsupport. In addition, we show how plans written in the language we present can\nrepresent certain types of subtasks that cannot be accomplished using the\nlanguages supported by network query engines. Finally, we demonstrate that the\nincreased expressivity of our plan language does not hamper performance;\nspecifically, we show how data can be integrated from multiple remote sources\njust as efficiently using our architecture as is possible with a\nstate-of-the-art streaming-dataflow network query engine.\n"
  },
  {
    "id": "1109.2049",
    "title": "Structure-Based Local Search Heuristics for Circuit-Level Boolean\n  Satisfiability",
    "abstract": "  This work focuses on improving state-of-the-art in stochastic local search\n(SLS) for solving Boolean satisfiability (SAT) instances arising from\nreal-world industrial SAT application domains. The recently introduced SLS\nmethod CRSat has been shown to noticeably improve on previously suggested SLS\ntechniques in solving such real-world instances by combining\njustification-based local search with limited Boolean constraint propagation on\nthe non-clausal formula representation form of Boolean circuits. In this work,\nwe study possibilities of further improving the performance of CRSat by\nexploiting circuit-level structural knowledge for developing new search\nheuristics for CRSat. To this end, we introduce and experimentally evaluate a\nvariety of search heuristics, many of which are motivated by circuit-level\nheuristics originally developed in completely different contexts, e.g., for\nelectronic design automation applications. To the best of our knowledge, most\nof the heuristics are novel in the context of SLS for SAT and, more generally,\nSLS for constraint satisfaction problems.\n"
  },
  {
    "id": "1109.2127",
    "title": "Integrating Learning from Examples into the Search for Diagnostic\n  Policies",
    "abstract": "  This paper studies the problem of learning diagnostic policies from training\nexamples. A diagnostic policy is a complete description of the decision-making\nactions of a diagnostician (i.e., tests followed by a diagnostic decision) for\nall possible combinations of test results. An optimal diagnostic policy is one\nthat minimizes the expected total cost, which is the sum of measurement costs\nand misdiagnosis costs. In most diagnostic settings, there is a tradeoff\nbetween these two kinds of costs. This paper formalizes diagnostic decision\nmaking as a Markov Decision Process (MDP). The paper introduces a new family of\nsystematic search algorithms based on the AO* algorithm to solve this MDP. To\nmake AO* efficient, the paper describes an admissible heuristic that enables\nAO* to prune large parts of the search space. The paper also introduces several\ngreedy algorithms including some improvements over previously-published\nmethods. The paper then addresses the question of learning diagnostic policies\nfrom examples. When the probabilities of diseases and test results are computed\nfrom training data, there is a great danger of overfitting. To reduce\noverfitting, regularizers are integrated into the search algorithms. Finally,\nthe paper compares the proposed methods on five benchmark diagnostic data sets.\nThe studies show that in most cases the systematic search methods produce\nbetter diagnostic policies than the greedy methods. In addition, the studies\nshow that for training sets of realistic size, the systematic search algorithms\nare practical on todays desktop computers.\n"
  },
  {
    "id": "1109.2131",
    "title": "On the Practical use of Variable Elimination in Constraint Optimization\n  Problems: 'Still-life' as a Case Study",
    "abstract": "  Variable elimination is a general technique for constraint processing. It is\noften discarded because of its high space complexity. However, it can be\nextremely useful when combined with other techniques. In this paper we study\nthe applicability of variable elimination to the challenging problem of finding\nstill-lifes. We illustrate several alternatives: variable elimination as a\nstand-alone algorithm, interleaved with search, and as a source of good quality\nlower bounds. We show that these techniques are the best known option both\ntheoretically and empirically. In our experiments we have been able to solve\nthe n=20 instance, which is far beyond reach with alternative approaches.\n"
  },
  {
    "id": "1109.2134",
    "title": "Generalizing Boolean Satisfiability II: Theory",
    "abstract": "  This is the second of three planned papers describing ZAP, a satisfiability\nengine that substantially generalizes existing tools while retaining the\nperformance characteristics of modern high performance solvers. The fundamental\nidea underlying ZAP is that many problems passed to such engines contain rich\ninternal structure that is obscured by the Boolean representation used; our\ngoal is to define a representation in which this structure is apparent and can\neasily be exploited to improve computational performance. This paper presents\nthe theoretical basis for the ideas underlying ZAP, arguing that existing ideas\nin this area exploit a single, recurring structure in that multiple database\naxioms can be obtained by operating on a single axiom using a subgroup of the\ngroup of permutations on the literals in the problem. We argue that the group\nstructure precisely captures the general structure at which earlier approaches\nhinted, and give numerous examples of its use. We go on to extend the\nDavis-Putnam-Logemann-Loveland inference procedure to this broader setting, and\nshow that earlier computational improvements are either subsumed or left intact\nby the new method. The third paper in this series discusses ZAPs implementation\nand presents experimental performance results.\n"
  },
  {
    "id": "1109.2137",
    "title": "Relational Dynamic Bayesian Networks",
    "abstract": "  Stochastic processes that involve the creation of objects and relations over\ntime are widespread, but relatively poorly studied. For example, accurate fault\ndiagnosis in factory assembly processes requires inferring the probabilities of\nerroneous assembly operations, but doing this efficiently and accurately is\ndifficult. Modeled as dynamic Bayesian networks, these processes have discrete\nvariables with very large domains and extremely high dimensionality. In this\npaper, we introduce relational dynamic Bayesian networks (RDBNs), which are an\nextension of dynamic Bayesian networks (DBNs) to first-order logic. RDBNs are a\ngeneralization of dynamic probabilistic relational models (DPRMs), which we had\nproposed in our previous work to model dynamic uncertain domains. We first\nextend the Rao-Blackwellised particle filtering described in our earlier work\nto RDBNs. Next, we lift the assumptions associated with Rao-Blackwellization in\nRDBNs and propose two new forms of particle filtering. The first one uses\nabstraction hierarchies over the predicates to smooth the particle filters\nestimates. The second employs kernel density estimation with a kernel function\nspecifically designed for relational domains. Experiments show these two\nmethods greatly outperform standard particle filtering on the task of assembly\nplan execution monitoring.\n"
  },
  {
    "id": "1109.2138",
    "title": "Reasoning about Action: An Argumentation - Theoretic Approach",
    "abstract": "  We present a uniform non-monotonic solution to the problems of reasoning\nabout action on the basis of an argumentation-theoretic approach. Our theory is\nprovably correct relative to a sensible minimisation policy introduced on top\nof a temporal propositional logic. Sophisticated problem domains can be\nformalised in our framework. As much attention of researchers in the field has\nbeen paid to the traditional and basic problems in reasoning about actions such\nas the frame, the qualification and the ramification problems, approaches to\nthese problems within our formalisation lie at heart of the expositions\npresented in this paper.\n"
  },
  {
    "id": "1109.2139",
    "title": "Solving Set Constraint Satisfaction Problems using ROBDDs",
    "abstract": "  In this paper we present a new approach to modeling finite set domain\nconstraint problems using Reduced Ordered Binary Decision Diagrams (ROBDDs). We\nshow that it is possible to construct an efficient set domain propagator which\ncompactly represents many set domains and set constraints using ROBDDs. We\ndemonstrate that the ROBDD-based approach provides unprecedented flexibility in\nmodeling constraint satisfaction problems, leading to performance improvements.\nWe also show that the ROBDD-based modeling approach can be extended to the\nmodeling of integer and multiset constraint problems in a straightforward\nmanner. Since domain propagation is not always practical, we also show how to\nincorporate less strict consistency notions into the ROBDD framework, such as\nset bounds, cardinality bounds and lexicographic bounds consistency. Finally,\nwe present experimental results that demonstrate the ROBDD-based solver\nperforms better than various more conventional constraint solvers on several\nstandard set constraint problems.\n"
  },
  {
    "id": "1109.2140",
    "title": "Learning Concept Hierarchies from Text Corpora using Formal Concept\n  Analysis",
    "abstract": "  We present a novel approach to the automatic acquisition of taxonomies or\nconcept hierarchies from a text corpus. The approach is based on Formal Concept\nAnalysis (FCA), a method mainly used for the analysis of data, i.e. for\ninvestigating and processing explicitly given information. We follow Harris\ndistributional hypothesis and model the context of a certain term as a vector\nrepresenting syntactic dependencies which are automatically acquired from the\ntext corpus with a linguistic parser. On the basis of this context information,\nFCA produces a lattice that we convert into a special kind of partial order\nconstituting a concept hierarchy. The approach is evaluated by comparing the\nresulting concept hierarchies with hand-crafted taxonomies for two domains:\ntourism and finance. We also directly compare our approach with hierarchical\nagglomerative clustering as well as with Bi-Section-KMeans as an instance of a\ndivisive clustering algorithm. Furthermore, we investigate the impact of using\ndifferent measures weighting the contribution of each attribute as well as of\napplying a particular smoothing technique to cope with data sparseness.\n"
  },
  {
    "id": "1109.2142",
    "title": "Generalizing Boolean Satisfiability III: Implementation",
    "abstract": "  This is the third of three papers describing ZAP, a satisfiability engine\nthat substantially generalizes existing tools while retaining the performance\ncharacteristics of modern high-performance solvers. The fundamental idea\nunderlying ZAP is that many problems passed to such engines contain rich\ninternal structure that is obscured by the Boolean representation used; our\ngoal has been to define a representation in which this structure is apparent\nand can be exploited to improve computational performance. The first paper\nsurveyed existing work that (knowingly or not) exploited problem structure to\nimprove the performance of satisfiability engines, and the second paper showed\nthat this structure could be understood in terms of groups of permutations\nacting on individual clauses in any particular Boolean theory. We conclude the\nseries by discussing the techniques needed to implement our ideas, and by\nreporting on their performance on a variety of problem instances.\n"
  },
  {
    "id": "1109.2143",
    "title": "Ignorability in Statistical and Probabilistic Inference",
    "abstract": "  When dealing with incomplete data in statistical learning, or incomplete\nobservations in probabilistic inference, one needs to distinguish the fact that\na certain event is observed from the fact that the observed event has happened.\nSince the modeling and computational complexities entailed by maintaining this\nproper distinction are often prohibitive, one asks for conditions under which\nit can be safely ignored. Such conditions are given by the missing at random\n(mar) and coarsened at random (car) assumptions. In this paper we provide an\nin-depth analysis of several questions relating to mar/car assumptions. Main\npurpose of our study is to provide criteria by which one may evaluate whether a\ncar assumption is reasonable for a particular data collecting or observational\nprocess. This question is complicated by the fact that several distinct\nversions of mar/car assumptions exist. We therefore first provide an overview\nover these different versions, in which we highlight the distinction between\ndistributional and coarsening variable induced versions. We show that\ndistributional versions are less restrictive and sufficient for most\napplications. We then address from two different perspectives the question of\nwhen the mar/car assumption is warranted. First we provide a static analysis\nthat characterizes the admissibility of the car assumption in terms of the\nsupport structure of the joint probability distribution of complete data and\nincomplete observations. Here we obtain an equivalence characterization that\nimproves and extends a recent result by Grunwald and Halpern. We then turn to a\nprocedural analysis that characterizes the admissibility of the car assumption\nin terms of procedural models for the actual data (or observation) generating\nprocess. The main result of this analysis is that the stronger coarsened\ncompletely at random (ccar) condition is arguably the most reasonable\nassumption, as it alone corresponds to data coarsening procedures that satisfy\na natural robustness property.\n"
  },
  {
    "id": "1109.2145",
    "title": "Perseus: Randomized Point-based Value Iteration for POMDPs",
    "abstract": "  Partially observable Markov decision processes (POMDPs) form an attractive\nand principled framework for agent planning under uncertainty. Point-based\napproximate techniques for POMDPs compute a policy based on a finite set of\npoints collected in advance from the agents belief space. We present a\nrandomized point-based value iteration algorithm called Perseus. The algorithm\nperforms approximate value backup stages, ensuring that in each backup stage\nthe value of each point in the belief set is improved; the key observation is\nthat a single backup may improve the value of many belief points. Contrary to\nother point-based methods, Perseus backs up only a (randomly selected) subset\nof points in the belief set, sufficient for improving the value of each belief\npoint in the set. We show how the same idea can be extended to dealing with\ncontinuous action spaces. Experimental results show the potential of Perseus in\nlarge scale POMDP problems.\n"
  },
  {
    "id": "1109.2148",
    "title": "Logical Hidden Markov Models",
    "abstract": "  Logical hidden Markov models (LOHMMs) upgrade traditional hidden Markov\nmodels to deal with sequences of structured symbols in the form of logical\natoms, rather than flat characters.\n  This note formally introduces LOHMMs and presents solutions to the three\ncentral inference problems for LOHMMs: evaluation, most likely hidden state\nsequence and parameter estimation. The resulting representation and algorithms\nare experimentally evaluated on problems from the domain of bioinformatics.\n"
  },
  {
    "id": "1109.2153",
    "title": "mGPT: A Probabilistic Planner Based on Heuristic Search",
    "abstract": "  We describe the version of the GPT planner used in the probabilistic track of\nthe 4th International Planning Competition (IPC-4). This version, called mGPT,\nsolves Markov Decision Processes specified in the PPDDL language by extracting\nand using different classes of lower bounds along with various heuristic-search\nalgorithms. The lower bounds are extracted from deterministic relaxations where\nthe alternative probabilistic effects of an action are mapped into different,\nindependent, deterministic actions. The heuristic-search algorithms use these\nlower bounds for focusing the updates and delivering a consistent value\nfunction over all states reachable from the initial state and the greedy\npolicy.\n"
  },
  {
    "id": "1109.2154",
    "title": "Macro-FF: Improving AI Planning with Automatically Learned\n  Macro-Operators",
    "abstract": "  Despite recent progress in AI planning, many benchmarks remain challenging\nfor current planners. In many domains, the performance of a planner can greatly\nbe improved by discovering and exploiting information about the domain\nstructure that is not explicitly encoded in the initial PDDL formulation. In\nthis paper we present and compare two automated methods that learn relevant\ninformation from previous experience in a domain and use it to solve new\nproblem instances. Our methods share a common four-step strategy. First, a\ndomain is analyzed and structural information is extracted, then\nmacro-operators are generated based on the previously discovered structure. A\nfiltering and ranking procedure selects the most useful macro-operators.\nFinally, the selected macros are used to speed up future searches. We have\nsuccessfully used such an approach in the fourth international planning\ncompetition IPC-4. Our system, Macro-FF, extends Hoffmanns state-of-the-art\nplanner FF 2.3 with support for two kinds of macro-operators, and with\nengineering enhancements. We demonstrate the effectiveness of our ideas on\nbenchmarks from international planning competitions. Our results indicate a\nlarge reduction in search effort in those complex domains where structural\ninformation can be inferred.\n"
  },
  {
    "id": "1109.2155",
    "title": "Optiplan: Unifying IP-based and Graph-based Planning",
    "abstract": "  The Optiplan planning system is the first integer programming-based planner\nthat successfully participated in the international planning competition. This\nengineering note describes the architecture of Optiplan and provides the\ninteger programming formulation that enabled it to perform reasonably well in\nthe competition. We also touch upon some recent developments that make integer\nprogramming encodings significantly more competitive.\n"
  },
  {
    "id": "1109.2156",
    "title": "Approximate Policy Iteration with a Policy Language Bias: Solving\n  Relational Markov Decision Processes",
    "abstract": "  We study an approach to policy selection for large relational Markov Decision\nProcesses (MDPs). We consider a variant of approximate policy iteration (API)\nthat replaces the usual value-function learning step with a learning step in\npolicy space. This is advantageous in domains where good policies are easier to\nrepresent and learn than the corresponding value functions, which is often the\ncase for the relational MDPs we are interested in. In order to apply API to\nsuch problems, we introduce a relational policy language and corresponding\nlearner. In addition, we introduce a new bootstrapping routine for goal-based\nplanning domains, based on random walks. Such bootstrapping is necessary for\nmany large relational MDPs, where reward is extremely sparse, as API is\nineffective in such domains when initialized with an uninformed policy. Our\nexperiments show that the resulting system is able to find good policies for a\nnumber of classical planning domains and their stochastic variants by solving\nthem as extremely large relational MDPs. The experiments also point to some\nlimitations of our approach, suggesting future work.\n"
  },
  {
    "id": "1109.2346",
    "title": "Linking Search Space Structure, Run-Time Dynamics, and Problem\n  Difficulty: A Step Toward Demystifying Tabu Search",
    "abstract": "  Tabu search is one of the most effective heuristics for locating high-quality\nsolutions to a diverse array of NP-hard combinatorial optimization problems.\nDespite the widespread success of tabu search, researchers have a poor\nunderstanding of many key theoretical aspects of this algorithm, including\nmodels of the high-level run-time dynamics and identification of those search\nspace features that influence problem difficulty. We consider these questions\nin the context of the job-shop scheduling problem (JSP), a domain where tabu\nsearch algorithms have been shown to be remarkably effective. Previously, we\ndemonstrated that the mean distance between random local optima and the nearest\noptimal solution is highly correlated with problem difficulty for a well-known\ntabu search algorithm for the JSP introduced by Taillard. In this paper, we\ndiscuss various shortcomings of this measure and develop a new model of problem\ndifficulty that corrects these deficiencies. We show that Taillards algorithm\ncan be modeled with high fidelity as a simple variant of a straightforward\nrandom walk. The random walk model accounts for nearly all of the variability\nin the cost required to locate both optimal and sub-optimal solutions to random\nJSPs, and provides an explanation for differences in the difficulty of random\nversus structured JSPs. Finally, we discuss and empirically substantiate two\nnovel predictions regarding tabu search algorithm behavior. First, the method\nfor constructing the initial solution is highly unlikely to impact the\nperformance of tabu search. Second, tabu tenure should be selected to be as\nsmall as possible while simultaneously avoiding search stagnation; values\nlarger than necessary lead to significant degradations in performance.\n"
  },
  {
    "id": "1109.2347",
    "title": "Breaking Instance-Independent Symmetries In Exact Graph Coloring",
    "abstract": "  Code optimization and high level synthesis can be posed as constraint\nsatisfaction and optimization problems, such as graph coloring used in register\nallocation. Graph coloring is also used to model more traditional CSPs relevant\nto AI, such as planning, time-tabling and scheduling. Provably optimal\nsolutions may be desirable for commercial and defense applications.\nAdditionally, for applications such as register allocation and code\noptimization, naturally-occurring instances of graph coloring are often small\nand can be solved optimally. A recent wave of improvements in algorithms for\nBoolean satisfiability (SAT) and 0-1 Integer Linear Programming (ILP) suggests\ngeneric problem-reduction methods, rather than problem-specific heuristics,\nbecause (1) heuristics may be upset by new constraints, (2) heuristics tend to\nignore structure, and (3) many relevant problems are provably inapproximable.\n  Problem reductions often lead to highly symmetric SAT instances, and\nsymmetries are known to slow down SAT solvers. In this work, we compare several\navenues for symmetry breaking, in particular when certain kinds of symmetry are\npresent in all generated instances. Our focus on reducing CSPs to SAT allows us\nto leverage recent dramatic improvement in SAT solvers and automatically\nbenefit from future progress. We can use a variety of black-box SAT solvers\nwithout modifying their source code because our symmetry-breaking techniques\nare static, i.e., we detect symmetries and add symmetry breaking predicates\n(SBPs) during pre-processing.\n  An important result of our work is that among the types of\ninstance-independent SBPs we studied and their combinations, the simplest and\nleast complete constructions are the most effective. Our experiments also\nclearly indicate that instance-independent symmetries should mostly be\nprocessed together with instance-specific symmetries rather than at the\nspecification level, contrary to what has been suggested in the literature.\n"
  },
  {
    "id": "1109.2355",
    "title": "Decision-Theoretic Planning with non-Markovian Rewards",
    "abstract": "  A decision process in which rewards depend on history rather than merely on\nthe current state is called a decision process with non-Markovian rewards\n(NMRDP). In decision-theoretic planning, where many desirable behaviours are\nmore naturally expressed as properties of execution sequences rather than as\nproperties of states, NMRDPs form a more natural model than the commonly\nadopted fully Markovian decision process (MDP) model. While the more tractable\nsolution methods developed for MDPs do not directly apply in the presence of\nnon-Markovian rewards, a number of solution methods for NMRDPs have been\nproposed in the literature. These all exploit a compact specification of the\nnon-Markovian reward function in temporal logic, to automatically translate the\nNMRDP into an equivalent MDP which is solved using efficient MDP solution\nmethods. This paper presents NMRDPP (Non-Markovian Reward Decision Process\nPlanner), a software platform for the development and experimentation of\nmethods for decision-theoretic planning with non-Markovian rewards. The current\nversion of NMRDPP implements, under a single interface, a family of methods\nbased on existing as well as new approaches which we describe in detail. These\ninclude dynamic programming, heuristic search, and structured methods. Using\nNMRDPP, we compare the methods and identify certain problem features that\naffect their performance. NMRDPPs treatment of non-Markovian rewards is\ninspired by the treatment of domain-specific search control knowledge in the\nTLPlan planner, which it incorporates as a special case. In the First\nInternational Probabilistic Planning Competition, NMRDPP was able to compete\nand perform well in both the domain-independent and hand-coded tracks, using\nsearch control knowledge in the latter.\n"
  },
  {
    "id": "1109.2752",
    "title": "On Validating Boolean Optimizers",
    "abstract": "  Boolean optimization finds a wide range of application domains, that\nmotivated a number of different organizations of Boolean optimizers since the\nmid 90s. Some of the most successful approaches are based on iterative calls to\nan NP oracle, using either linear search, binary search or the identification\nof unsatisfiable sub-formulas. The increasing use of Boolean optimizers in\npractical settings raises the question of confidence in computed results. For\nexample, the issue of confidence is paramount in safety critical settings. One\nway of increasing the confidence of the results computed by Boolean optimizers\nis to develop techniques for validating the results. Recent work studied the\nvalidation of Boolean optimizers based on branch-and-bound search. This paper\ncomplements existing work, and develops methods for validating Boolean\noptimizers that are based on iterative calls to an NP oracle. This entails\nimplementing solutions for validating both satisfiable and unsatisfiable\nanswers from the NP oracle. The work described in this paper can be applied to\na wide range of Boolean optimizers, that find application in Pseudo-Boolean\nOptimization and in Maximum Satisfiability. Preliminary experimental results\nindicate that the impact of the proposed method in overall performance is\nnegligible.\n"
  },
  {
    "id": "1109.3094",
    "title": "On the use of reference points for the biobjective Inventory Routing\n  Problem",
    "abstract": "  The article presents a study on the biobjective inventory routing problem.\nContrary to most previous research, the problem is treated as a true\nmulti-objective optimization problem, with the goal of identifying\nPareto-optimal solutions. Due to the hardness of the problem at hand, a\nreference point based optimization approach is presented and implemented into\nan optimization and decision support system, which allows for the computation\nof a true subset of the optimal outcomes. Experimental investigation involving\nlocal search metaheuristics are conducted on benchmark data, and numerical\nresults are reported and analyzed.\n"
  },
  {
    "id": "1109.3313",
    "title": "Neigborhood Selection in Variable Neighborhood Search",
    "abstract": "  Variable neighborhood search (VNS) is a metaheuristic for solving\noptimization problems based on a simple principle: systematic changes of\nneighborhoods within the search, both in the descent to local minima and in the\nescape from the valleys which contain them. Designing these neighborhoods and\napplying them in a meaningful fashion is not an easy task. Moreover, an\nappropriate order in which they are applied must be determined. In this paper\nwe attempt to investigate this issue. Assume that we are given an optimization\nproblem that is intended to be solved by applying the VNS scheme, how many and\nwhich types of neighborhoods should be investigated and what could be\nappropriate selection criteria to apply these neighborhoods. More specifically,\ndoes it pay to \"look ahead\" (see, e.g., in the context of VNS and GRASP) when\nattempting to switch from one neighborhood to another?\n"
  },
  {
    "id": "1109.3532",
    "title": "A Characterization of the Combined Effects of Overlap and Imbalance on\n  the SVM Classifier",
    "abstract": "  In this paper we demonstrate that two common problems in Machine\nLearning---imbalanced and overlapping data distributions---do not have\nindependent effects on the performance of SVM classifiers. This result is\nnotable since it shows that a model of either of these factors must account for\nthe presence of the other. Our study of the relationship between these problems\nhas lead to the discovery of a previously unreported form of \"covert\"\noverfitting which is resilient to commonly used empirical regularization\ntechniques. We demonstrate the existance of this covert phenomenon through\nseveral methods based around the parametric regularization of trained SVMs. Our\nfindings in this area suggest a possible approach to quantifying overlap in\nreal world data sets.\n"
  },
  {
    "id": "1109.3700",
    "title": "Contradiction measures and specificity degrees of basic belief\n  assignments",
    "abstract": "  In the theory of belief functions, many measures of uncertainty have been\nintroduced. However, it is not always easy to understand what these measures\nreally try to represent. In this paper, we re-interpret some measures of\nuncertainty in the theory of belief functions. We present some interests and\ndrawbacks of the existing measures. On these observations, we introduce a\nmeasure of contradiction. Therefore, we present some degrees of non-specificity\nand Bayesianity of a mass. We propose a degree of specificity based on the\ndistance between a mass and its most specific associated mass. We also show how\nto use the degree of specificity to measure the specificity of a fusion rule.\nIllustrations on simple examples are given.\n"
  },
  {
    "id": "1109.3737",
    "title": "Learning where to Attend with Deep Architectures for Image Tracking",
    "abstract": "  We discuss an attentional model for simultaneous object tracking and\nrecognition that is driven by gaze data. Motivated by theories of perception,\nthe model consists of two interacting pathways: identity and control, intended\nto mirror the what and where pathways in neuroscience models. The identity\npathway models object appearance and performs classification using deep\n(factored)-Restricted Boltzmann Machines. At each point in time the\nobservations consist of foveated images, with decaying resolution toward the\nperiphery of the gaze. The control pathway models the location, orientation,\nscale and speed of the attended object. The posterior distribution of these\nstates is estimated with particle filtering. Deeper in the control pathway, we\nencounter an attentional mechanism that learns to select gazes so as to\nminimize tracking uncertainty. Unlike in our previous work, we introduce gaze\nselection strategies which operate in the presence of partial information and\non a continuous action space. We show that a straightforward extension of the\nexisting approach to the partial information setting results in poor\nperformance, and we propose an alternative method based on modeling the reward\nsurface as a Gaussian Process. This approach gives good performance in the\npresence of partial information and allows us to expand the action space from a\nsmall, discrete set of fixation points to a continuous domain.\n"
  },
  {
    "id": "1109.4335",
    "title": "Social choice rules driven by propositional logic",
    "abstract": "  Several rules for social choice are examined from a unifying point of view\nthat looks at them as procedures for revising a system of degrees of belief in\naccordance with certain specified logical constraints. Belief is here a social\nattribute, its degrees being measured by the fraction of people who share a\ngiven opinion. Different known rules and some new ones are obtained depending\non which particular constraints are assumed. These constraints allow to model\ndifferent notions of choiceness. In particular, we give a new method to deal\nwith approval-disapproval-preferential voting.\n"
  },
  {
    "id": "1109.4603",
    "title": "Explicit Approximations of the Gaussian Kernel",
    "abstract": "  We investigate training and using Gaussian kernel SVMs by approximating the\nkernel with an explicit finite- dimensional polynomial feature representation\nbased on the Taylor expansion of the exponential. Although not as efficient as\nthe recently-proposed random Fourier features [Rahimi and Recht, 2007] in terms\nof the number of features, we show how this polynomial representation can\nprovide a better approximation in terms of the computational cost involved.\nThis makes our \"Taylor features\" especially attractive for use on very large\ndata sets, in conjunction with online or stochastic training.\n"
  },
  {
    "id": "1109.5072",
    "title": "Analysis of first prototype universal intelligence tests: evaluating and\n  comparing AI algorithms and humans",
    "abstract": "  Today, available methods that assess AI systems are focused on using\nempirical techniques to measure the performance of algorithms in some specific\ntasks (e.g., playing chess, solving mazes or land a helicopter). However, these\nmethods are not appropriate if we want to evaluate the general intelligence of\nAI and, even less, if we compare it with human intelligence. The ANYNT project\nhas designed a new method of evaluation that tries to assess AI systems using\nwell known computational notions and problems which are as general as possible.\nThis new method serves to assess general intelligence (which allows us to learn\nhow to solve any new kind of problem we face) and not only to evaluate\nperformance on a set of specific tasks. This method not only focuses on\nmeasuring the intelligence of algorithms, but also to assess any intelligent\nsystem (human beings, animals, AI, aliens?,...), and letting us to place their\nresults on the same scale and, therefore, to be able to compare them. This new\napproach will allow us (in the future) to evaluate and compare any kind of\nintelligent system known or even to build/find, be it artificial or biological.\nThis master thesis aims at ensuring that this new method provides consistent\nresults when evaluating AI algorithms, this is done through the design and\nimplementation of prototypes of universal intelligence tests and their\napplication to different intelligent systems (AI algorithms and humans beings).\nFrom the study we analyze whether the results obtained by two different\nintelligent systems are properly located on the same scale and we propose\nchanges and refinements to these prototypes in order to, in the future, being\nable to achieve a truly universal intelligence test.\n"
  },
  {
    "id": "1109.5663",
    "title": "The Deterministic Part of IPC-4: An Overview",
    "abstract": "  We provide an overview of the organization and results of the deterministic\npart of the 4th International Planning Competition, i.e., of the part concerned\nwith evaluating systems doing deterministic planning. IPC-4 attracted even more\ncompeting systems than its already large predecessors, and the competition\nevent was revised in several important respects. After giving an introduction\nto the IPC, we briefly explain the main differences between the deterministic\npart of IPC-4 and its predecessors. We then introduce formally the language\nused, called PDDL2.2 that extends PDDL2.1 by derived predicates and timed\ninitial literals. We list the competing systems and overview the results of the\ncompetition. The entire set of data is far too large to be presented in full.\nWe provide a detailed summary; the complete data is available in an online\nappendix. We explain how we awarded the competition prizes.\n"
  },
  {
    "id": "1109.5665",
    "title": "PDDL2.1 - The Art of the Possible? Commentary on Fox and Long",
    "abstract": "  PDDL2.1 was designed to push the envelope of what planning algorithms can do,\nand it has succeeded. It adds two important features: durative actions,which\ntake time (and may have continuous effects); and objective functions for\nmeasuring the quality of plans. The concept of durative actions is flawed; and\nthe treatment of their semantics reveals too strong an attachment to the way\nmany contemporary planners work. Future PDDL innovators should focus on\nproducing a clean semantics for additions to the language, and let planner\nimplementers worry about coupling their algorithms to problems expressed in the\nlatest version of the language.\n"
  },
  {
    "id": "1109.5666",
    "title": "The Case for Durative Actions: A Commentary on PDDL2.1",
    "abstract": "  The addition of durative actions to PDDL2.1 sparked some controversy. Fox and\nLong argued that actions should be considered as instantaneous, but can start\nand stop processes. Ultimately, a limited notion of durative actions was\nincorporated into the language. I argue that this notion is still impoverished,\nand that the underlying philosophical position of regarding durative actions as\nbeing a shorthand for a start action, process, and stop action ignores the\nrealities of modelling and execution for complex systems.\n"
  },
  {
    "id": "1109.5711",
    "title": "Engineering a Conformant Probabilistic Planner",
    "abstract": "  We present a partial-order, conformant, probabilistic planner, Probapop which\ncompeted in the blind track of the Probabilistic Planning Competition in IPC-4.\nWe explain how we adapt distance based heuristics for use with probabilistic\ndomains. Probapop also incorporates heuristics based on probability of success.\nWe explain the successes and difficulties encountered during the design and\nimplementation of Probapop.\n"
  },
  {
    "id": "1109.5713",
    "title": "Where 'Ignoring Delete Lists' Works: Local Search Topology in Planning\n  Benchmarks",
    "abstract": "  Between 1998 and 2004, the planning community has seen vast progress in terms\nof the sizes of benchmark examples that domain-independent planners can tackle\nsuccessfully. The key technique behind this progress is the use of heuristic\nfunctions based on relaxing the planning task at hand, where the relaxation is\nto assume that all delete lists are empty. The unprecedented success of such\nmethods, in many commonly used benchmark examples, calls for an understanding\nof what classes of domains these methods are well suited for. In the\ninvestigation at hand, we derive a formal background to such an understanding.\nWe perform a case study covering a range of 30 commonly used STRIPS and ADL\nbenchmark domains, including all examples used in the first four international\nplanning competitions. We *prove* connections between domain structure and\nlocal search topology -- heuristic cost surface properties -- under an\nidealized version of the heuristic functions used in modern planners. The\nidealized heuristic function is called h^+, and differs from the practically\nused functions in that it returns the length of an *optimal* relaxed plan,\nwhich is NP-hard to compute. We identify several key characteristics of the\ntopology under h^+, concerning the existence/non-existence of unrecognized dead\nends, as well as the existence/non-existence of constant upper bounds on the\ndifficulty of escaping local minima and benches. These distinctions divide the\n(set of all) planning domains into a taxonomy of classes of varying h^+\ntopology. As it turns out, many of the 30 investigated domains lie in classes\nwith a relatively easy topology. Most particularly, 12 of the domains lie in\nclasses where FFs search algorithm, provided with h^+, is a polynomial solving\nmechanism. We also present results relating h^+ to its approximation as\nimplemented in FF. The behavior regarding dead ends is provably the same. We\nsummarize the results of an empirical investigation showing that, in many\ndomains, the topological qualities of h^+ are largely inherited by the\napproximation. The overall investigation gives a rare example of a successful\nanalysis of the connections between typical-case problem structure, and search\nperformance. The theoretical investigation also gives hints on how the\ntopological phenomena might be automatically recognizable by domain analysis\ntechniques. We outline some preliminary steps we made into that direction.\n"
  },
  {
    "id": "1109.5714",
    "title": "Binary Encodings of Non-binary Constraint Satisfaction Problems:\n  Algorithms and Experimental Results",
    "abstract": "  A non-binary Constraint Satisfaction Problem (CSP) can be solved directly\nusing extended versions of binary techniques. Alternatively, the non-binary\nproblem can be translated into an equivalent binary one. In this case, it is\ngenerally accepted that the translated problem can be solved by applying\nwell-established techniques for binary CSPs. In this paper we evaluate the\napplicability of the latter approach. We demonstrate that the use of standard\ntechniques for binary CSPs in the encodings of non-binary problems is\nproblematic and results in models that are very rarely competitive with the\nnon-binary representation. To overcome this, we propose specialized arc\nconsistency and search algorithms for binary encodings, and we evaluate them\ntheoretically and empirically. We consider three binary representations; the\nhidden variable encoding, the dual encoding, and the double encoding.\nTheoretical and empirical results show that, for certain classes of non-binary\nconstraints, binary encodings are a competitive option, and in many cases, a\nbetter one than the non-binary representation.\n"
  },
  {
    "id": "1109.5716",
    "title": "Distributed Reasoning in a Peer-to-Peer Setting: Application to the\n  Semantic Web",
    "abstract": "  In a peer-to-peer inference system, each peer can reason locally but can also\nsolicit some of its acquaintances, which are peers sharing part of its\nvocabulary. In this paper, we consider peer-to-peer inference systems in which\nthe local theory of each peer is a set of propositional clauses defined upon a\nlocal vocabulary. An important characteristic of peer-to-peer inference systems\nis that the global theory (the union of all peer theories) is not known (as\nopposed to partition-based reasoning systems). The main contribution of this\npaper is to provide the first consequence finding algorithm in a peer-to-peer\nsetting: DeCA. It is anytime and computes consequences gradually from the\nsolicited peer to peers that are more and more distant. We exhibit a sufficient\ncondition on the acquaintance graph of the peer-to-peer inference system for\nguaranteeing the completeness of this algorithm. Another important contribution\nis to apply this general distributed reasoning setting to the setting of the\nSemantic Web through the Somewhere semantic peer-to-peer data management\nsystem. The last contribution of this paper is to provide an experimental\nanalysis of the scalability of the peer-to-peer infrastructure that we propose,\non large networks of 1000 peers.\n"
  },
  {
    "id": "1109.5717",
    "title": "Dynamic Local Search for the Maximum Clique Problem",
    "abstract": "  In this paper, we introduce DLS-MC, a new stochastic local search algorithm\nfor the maximum clique problem. DLS-MC alternates between phases of iterative\nimprovement, during which suitable vertices are added to the current clique,\nand plateau search, during which vertices of the current clique are swapped\nwith vertices not contained in the current clique. The selection of vertices is\nsolely based on vertex penalties that are dynamically adjusted during the\nsearch, and a perturbation mechanism is used to overcome search stagnation. The\nbehaviour of DLS-MC is controlled by a single parameter, penalty delay, which\ncontrols the frequency at which vertex penalties are reduced. We show\nempirically that DLS-MC achieves substantial performance improvements over\nstate-of-the-art algorithms for the maximum clique problem over a large range\nof the commonly used DIMACS benchmark instances.\n"
  },
  {
    "id": "1109.5732",
    "title": "Representing Conversations for Scalable Overhearing",
    "abstract": "  Open distributed multi-agent systems are gaining interest in the academic\ncommunity and in industry. In such open settings, agents are often coordinated\nusing standardized agent conversation protocols. The representation of such\nprotocols (for analysis, validation, monitoring, etc) is an important aspect of\nmulti-agent applications. Recently, Petri nets have been shown to be an\ninteresting approach to such representation, and radically different approaches\nusing Petri nets have been proposed. However, their relative strengths and\nweaknesses have not been examined. Moreover, their scalability and suitability\nfor different tasks have not been addressed. This paper addresses both these\nchallenges. First, we analyze existing Petri net representations in terms of\ntheir scalability and appropriateness for overhearing, an important task in\nmonitoring open multi-agent systems. Then, building on the insights gained, we\nintroduce a novel representation using Colored Petri nets that explicitly\nrepresent legal joint conversation states and messages. This representation\napproach offers significant improvements in scalability and is particularly\nsuitable for overhearing. Furthermore, we show that this new representation\noffers a comprehensive coverage of all conversation features of FIPA\nconversation standards. We also present a procedure for transforming AUML\nconversation protocol diagrams (a standard human-readable representation), to\nour Colored Petri net representation.\n"
  },
  {
    "id": "1109.5750",
    "title": "Improving Heuristics Through Relaxed Search - An Analysis of TP4 and\n  HSP*a in the 2004 Planning Competition",
    "abstract": "  The hm admissible heuristics for (sequential and temporal) regression\nplanning are defined by a parameterized relaxation of the optimal cost function\nin the regression search space, where the parameter m offers a trade-off\nbetween the accuracy and computational cost of theheuristic. Existing methods\nfor computing the hm heuristic require time exponential in m, limiting them to\nsmall values (m andlt= 2). The hm heuristic can also be viewed as the optimal\ncost function in a relaxation of the search space: this paper presents relaxed\nsearch, a method for computing this function partially by searching in the\nrelaxed space. The relaxed search method, because it computes hm only\npartially, is computationally cheaper and therefore usable for higher values of\nm. The (complete) hm heuristic is combined with partial hm heuristics, for m =\n3,..., computed by relaxed search, resulting in a more accurate heuristic.\n  This use of the relaxed search method to improve on the hm heuristic is\nevaluated by comparing two optimal temporal planners: TP4, which does not use\nit, and HSP*a, which uses it but is otherwise identical to TP4. The comparison\nis made on the domains used in the 2004 International Planning Competition, in\nwhich both planners participated. Relaxed search is found to be cost effective\nin some of these domains, but not all. Analysis reveals a characterization of\nthe domains in which relaxed search can be expected to be cost effective, in\nterms of two measures on the original and relaxed search spaces. In the domains\nwhere relaxed search is cost effective, expanding small states is\ncomputationally cheaper than expanding large states and small states tend to\nhave small successor states.\n"
  },
  {
    "id": "1109.5920",
    "title": "Models and Strategies for Variants of the Job Shop Scheduling Problem",
    "abstract": "  Recently, a variety of constraint programming and Boolean satisfiability\napproaches to scheduling problems have been introduced. They have in common the\nuse of relatively simple propagation mechanisms and an adaptive way to focus on\nthe most constrained part of the problem. In some cases, these methods compare\nfavorably to more classical constraint programming methods relying on\npropagation algorithms for global unary or cumulative resource constraints and\ndedicated search heuristics. In particular, we described an approach that\ncombines restarting, with a generic adaptive heuristic and solution guided\nbranching on a simple model based on a decomposition of disjunctive\nconstraints. In this paper, we introduce an adaptation of this technique for an\nimportant subclass of job shop scheduling problems (JSPs), where the objective\nfunction involves minimization of earliness/tardiness costs. We further show\nthat our technique can be improved by adding domain specific information for\none variant of the JSP (involving time lag constraints). In particular we\nintroduce a dedicated greedy heuristic, and an improved model for the case\nwhere the maximal time lag is 0 (also referred to as no-wait JSPs).\n"
  },
  {
    "id": "1109.5951",
    "title": "An Approximation of the Universal Intelligence Measure",
    "abstract": "  The Universal Intelligence Measure is a recently proposed formal definition\nof intelligence. It is mathematically specified, extremely general, and\ncaptures the essence of many informal definitions of intelligence. It is based\non Hutter's Universal Artificial Intelligence theory, an extension of Ray\nSolomonoff's pioneering work on universal induction. Since the Universal\nIntelligence Measure is only asymptotically computable, building a practical\nintelligence test from it is not straightforward. This paper studies the\npractical issues involved in developing a real-world UIM-based performance\nmetric. Based on our investigation, we develop a prototype implementation which\nwe use to evaluate a number of different artificial agents.\n"
  },
  {
    "id": "1109.6029",
    "title": "An Improved Search Algorithm for Optimal Multiple-Sequence Alignment",
    "abstract": "  Multiple sequence alignment (MSA) is a ubiquitous problem in computational\nbiology. Although it is NP-hard to find an optimal solution for an arbitrary\nnumber of sequences, due to the importance of this problem researchers are\ntrying to push the limits of exact algorithms further. Since MSA can be cast as\na classical path finding problem, it is attracting a growing number of AI\nresearchers interested in heuristic search algorithms as a challenge with\nactual practical relevance. In this paper, we first review two previous,\ncomplementary lines of research. Based on Hirschbergs algorithm, Dynamic\nProgramming needs O(kN^(k-1)) space to store both the search frontier and the\nnodes needed to reconstruct the solution path, for k sequences of length N.\nBest first search, on the other hand, has the advantage of bounding the search\nspace that has to be explored using a heuristic. However, it is necessary to\nmaintain all explored nodes up to the final solution in order to prevent the\nsearch from re-expanding them at higher cost. Earlier approaches to reduce the\nClosed list are either incompatible with pruning methods for the Open list, or\nmust retain at least the boundary of the Closed list. In this article, we\npresent an algorithm that attempts at combining the respective advantages; like\nA* it uses a heuristic for pruning the search space, but reduces both the\nmaximum Open and Closed size to O(kN^(k-1)), as in Dynamic Programming. The\nunderlying idea is to conduct a series of searches with successively increasing\nupper bounds, but using the DP ordering as the key for the Open priority queue.\nWith a suitable choice of thresholds, in practice, a running time below four\ntimes that of A* can be expected. In our experiments we show that our algorithm\noutperforms one of the currently most successful algorithms for optimal\nmultiple sequence alignments, Partial Expansion A*, both in time and memory.\nMoreover, we apply a refined heuristic based on optimal alignments not only of\npairs of sequences, but of larger subsets. This idea is not new; however, to\nmake it practically relevant we show that it is equally important to bound the\nheuristic computation appropriately, or the overhead can obliterate any\npossible gain. Furthermore, we discuss a number of improvements in time and\nspace efficiency with regard to practical implementations. Our algorithm, used\nin conjunction with higher-dimensional heuristics, is able to calculate for the\nfirst time the optimal alignment for almost all of the problems in Reference 1\nof the benchmark database BAliBASE.\n"
  },
  {
    "id": "1109.6030",
    "title": "Probabilistic Hybrid Action Models for Predicting Concurrent\n  Percept-driven Robot Behavior",
    "abstract": "  This article develops Probabilistic Hybrid Action Models (PHAMs), a realistic\ncausal model for predicting the behavior generated by modern percept-driven\nrobot plans. PHAMs represent aspects of robot behavior that cannot be\nrepresented by most action models used in AI planning: the temporal structure\nof continuous control processes, their non-deterministic effects, several modes\nof their interferences, and the achievement of triggering conditions in\nclosed-loop robot plans.\n  The main contributions of this article are: (1) PHAMs, a model of concurrent\npercept-driven behavior, its formalization, and proofs that the model generates\nprobably, qualitatively accurate predictions; and (2) a resource-efficient\ninference method for PHAMs based on sampling projections from probabilistic\naction models and state descriptions. We show how PHAMs can be applied to\nplanning the course of action of an autonomous robot office courier based on\nanalytical and experimental results.\n"
  },
  {
    "id": "1109.6033",
    "title": "Generative Prior Knowledge for Discriminative Classification",
    "abstract": "  We present a novel framework for integrating prior knowledge into\ndiscriminative classifiers. Our framework allows discriminative classifiers\nsuch as Support Vector Machines (SVMs) to utilize prior knowledge specified in\nthe generative setting. The dual objective of fitting the data and respecting\nprior knowledge is formulated as a bilevel program, which is solved\n(approximately) via iterative application of second-order cone programming. To\ntest our approach, we consider the problem of using WordNet (a semantic\ndatabase of English language) to improve low-sample classification accuracy of\nnewsgroup categorization. WordNet is viewed as an approximate, but readily\navailable source of background knowledge, and our framework is capable of\nutilizing it in a flexible way.\n"
  },
  {
    "id": "1109.6051",
    "title": "The Fast Downward Planning System",
    "abstract": "  Fast Downward is a classical planning system based on heuristic search. It\ncan deal with general deterministic planning problems encoded in the\npropositional fragment of PDDL2.2, including advanced features like ADL\nconditions and effects and derived predicates (axioms). Like other well-known\nplanners such as HSP and FF, Fast Downward is a progression planner, searching\nthe space of world states of a planning task in the forward direction. However,\nunlike other PDDL planning systems, Fast Downward does not use the\npropositional PDDL representation of a planning task directly. Instead, the\ninput is first translated into an alternative representation called\nmulti-valued planning tasks, which makes many of the implicit constraints of a\npropositional planning task explicit. Exploiting this alternative\nrepresentation, Fast Downward uses hierarchical decompositions of planning\ntasks for computing its heuristic function, called the causal graph heuristic,\nwhich is very different from traditional HSP-like heuristics based on ignoring\nnegative interactions of operators.\n  In this article, we give a full account of Fast Downwards approach to solving\nmulti-valued planning tasks. We extend our earlier discussion of the causal\ngraph heuristic to tasks involving axioms and conditional effects and present\nsome novel techniques for search control that are used within Fast Downwards\nbest-first search algorithm: preferred operators transfer the idea of helpful\nactions from local search to global best-first search, deferred evaluation of\nheuristic functions mitigates the negative effect of large branching factors on\nsearch performance, and multi-heuristic best-first search combines several\nheuristic evaluation functions within a single search algorithm in an\northogonal way. We also describe efficient data structures for fast state\nexpansion (successor generators and axiom evaluators) and present a new\nnon-heuristic search algorithm called focused iterative-broadening search,\nwhich utilizes the information encoded in causal graphs in a novel way.\n  Fast Downward has proven remarkably successful: It won the \"classical (i.e.,\npropositional, non-optimising) track of the 4th International Planning\nCompetition at ICAPS 2004, following in the footsteps of planners such as FF\nand LPG. Our experiments show that it also performs very well on the benchmarks\nof the earlier planning competitions and provide some insights about the\nusefulness of the new search enhancements.\n"
  },
  {
    "id": "1109.6052",
    "title": "Asynchronous Partial Overlay: A New Algorithm for Solving Distributed\n  Constraint Satisfaction Problems",
    "abstract": "  Distributed Constraint Satisfaction (DCSP) has long been considered an\nimportant problem in multi-agent systems research. This is because many\nreal-world problems can be represented as constraint satisfaction and these\nproblems often present themselves in a distributed form. In this article, we\npresent a new complete, distributed algorithm called Asynchronous Partial\nOverlay (APO) for solving DCSPs that is based on a cooperative mediation\nprocess. The primary ideas behind this algorithm are that agents, when acting\nas a mediator, centralize small, relevant portions of the DCSP, that these\ncentralized subproblems overlap, and that agents increase the size of their\nsubproblems along critical paths within the DCSP as the problem solving\nunfolds. We present empirical evidence that shows that APO outperforms other\nknown, complete DCSP techniques.\n"
  },
  {
    "id": "1109.6344",
    "title": "Admissible and Restrained Revision",
    "abstract": "  As partial justification of their framework for iterated belief revision\nDarwiche and Pearl convincingly argued against Boutiliers natural revision and\nprovided a prototypical revision operator that fits into their scheme. We show\nthat the Darwiche-Pearl arguments lead naturally to the acceptance of a smaller\nclass of operators which we refer to as admissible. Admissible revision ensures\nthat the penultimate input is not ignored completely, thereby eliminating\nnatural revision, but includes the Darwiche-Pearl operator, Nayaks\nlexicographic revision operator, and a newly introduced operator called\nrestrained revision. We demonstrate that restrained revision is the most\nconservative of admissible revision operators, effecting as few changes as\npossible, while lexicographic revision is the least conservative, and point out\nthat restrained revision can also be viewed as a composite operator, consisting\nof natural revision preceded by an application of a \"backwards revision\"\noperator previously studied by Papini. Finally, we propose the establishment of\na principled approach for choosing an appropriate revision operator in\ndifferent contexts and discuss future work.\n"
  },
  {
    "id": "1109.6345",
    "title": "On Graphical Modeling of Preference and Importance",
    "abstract": "  In recent years, CP-nets have emerged as a useful tool for supporting\npreference elicitation, reasoning, and representation. CP-nets capture and\nsupport reasoning with qualitative conditional preference statements,\nstatements that are relatively natural for users to express. In this paper, we\nextend the CP-nets formalism to handle another class of very natural\nqualitative statements one often uses in expressing preferences in daily life -\nstatements of relative importance of attributes. The resulting formalism,\nTCP-nets, maintains the spirit of CP-nets, in that it remains focused on using\nonly simple and natural preference statements, uses the ceteris paribus\nsemantics, and utilizes a graphical representation of this information to\nreason about its consistency and to perform, possibly constrained, optimization\nusing it. The extra expressiveness it provides allows us to better model\ntradeoffs users would like to make, more faithfully representing their\npreferences.\n"
  },
  {
    "id": "1109.6346",
    "title": "The Planning Spectrum - One, Two, Three, Infinity",
    "abstract": "  Linear Temporal Logic (LTL) is widely used for defining conditions on the\nexecution paths of dynamic systems. In the case of dynamic systems that allow\nfor nondeterministic evolutions, one has to specify, along with an LTL formula\nf, which are the paths that are required to satisfy the formula. Two extreme\ncases are the universal interpretation A.f, which requires that the formula be\nsatisfied for all execution paths, and the existential interpretation E.f,\nwhich requires that the formula be satisfied for some execution path.\n  When LTL is applied to the definition of goals in planning problems on\nnondeterministic domains, these two extreme cases are too restrictive. It is\noften impossible to develop plans that achieve the goal in all the\nnondeterministic evolutions of a system, and it is too weak to require that the\ngoal is satisfied by some execution.\n  In this paper we explore alternative interpretations of an LTL formula that\nare between these extreme cases. We define a new language that permits an\narbitrary combination of the A and E quantifiers, thus allowing, for instance,\nto require that each finite execution can be extended to an execution\nsatisfying an LTL formula (AE.f), or that there is some finite execution whose\nextensions all satisfy an LTL formula (EA.f). We show that only eight of these\ncombinations of path quantifiers are relevant, corresponding to an alternation\nof the quantifiers of length one (A and E), two (AE and EA), three (AEA and\nEAE), and infinity ((AE)* and (EA)*). We also present a planning algorithm for\nthe new language that is based on an automata-theoretic approach, and study its\ncomplexity.\n"
  },
  {
    "id": "1109.6348",
    "title": "Fault Tolerant Boolean Satisfiability",
    "abstract": "  A delta-model is a satisfying assignment of a Boolean formula for which any\nsmall alteration, such as a single bit flip, can be repaired by flips to some\nsmall number of other bits, yielding a new satisfying assignment. These\nsatisfying assignments represent robust solutions to optimization problems\n(e.g., scheduling) where it is possible to recover from unforeseen events\n(e.g., a resource becoming unavailable). The concept of delta-models was\nintroduced by Ginsberg, Parkes and Roy (AAAI 1998), where it was proved that\nfinding delta-models for general Boolean formulas is NP-complete. In this\npaper, we extend that result by studying the complexity of finding delta-models\nfor classes of Boolean formulas which are known to have polynomial time\nsatisfiability solvers. In particular, we examine 2-SAT, Horn-SAT, Affine-SAT,\ndual-Horn-SAT, 0-valid and 1-valid SAT. We see a wide variation in the\ncomplexity of finding delta-models, e.g., while 2-SAT and Affine-SAT have\npolynomial time tests for delta-models, testing whether a Horn-SAT formula has\none is NP-complete.\n"
  },
  {
    "id": "1109.6361",
    "title": "Cognitive Principles in Robust Multimodal Interpretation",
    "abstract": "  Multimodal conversational interfaces provide a natural means for users to\ncommunicate with computer systems through multiple modalities such as speech\nand gesture. To build effective multimodal interfaces, automated interpretation\nof user multimodal inputs is important. Inspired by the previous investigation\non cognitive status in multimodal human machine interaction, we have developed\na greedy algorithm for interpreting user referring expressions (i.e.,\nmultimodal reference resolution). This algorithm incorporates the cognitive\nprinciples of Conversational Implicature and Givenness Hierarchy and applies\nconstraints from various sources (e.g., temporal, semantic, and contextual) to\nresolve references. Our empirical results have shown the advantage of this\nalgorithm in efficiently resolving a variety of user references. Because of its\nsimplicity and generality, this approach has the potential to improve the\nrobustness of multimodal input interpretation.\n"
  },
  {
    "id": "1109.6618",
    "title": "Multiple-Goal Heuristic Search",
    "abstract": "  This paper presents a new framework for anytime heuristic search where the\ntask is to achieve as many goals as possible within the allocated resources. We\nshow the inadequacy of traditional distance-estimation heuristics for tasks of\nthis type and present alternative heuristics that are more appropriate for\nmultiple-goal search. In particular, we introduce the marginal-utility\nheuristic, which estimates the cost and the benefit of exploring a subtree\nbelow a search node. We developed two methods for online learning of the\nmarginal-utility heuristic. One is based on local similarity of the partial\nmarginal utility of sibling nodes, and the other generalizes marginal-utility\nover the state feature space. We apply our adaptive and non-adaptive\nmultiple-goal search algorithms to several problems, including focused\ncrawling, and show their superiority over existing methods.\n"
  },
  {
    "id": "1109.6621",
    "title": "FluCaP: A Heuristic Search Planner for First-Order MDPs",
    "abstract": "  We present a heuristic search algorithm for solving first-order Markov\nDecision Processes (FOMDPs). Our approach combines first-order state\nabstraction that avoids evaluating states individually, and heuristic search\nthat avoids evaluating all states. Firstly, in contrast to existing systems,\nwhich start with propositionalizing the FOMDP and then perform state\nabstraction on its propositionalized version we apply state abstraction\ndirectly on the FOMDP avoiding propositionalization. This kind of abstraction\nis referred to as first-order state abstraction. Secondly, guided by an\nadmissible heuristic, the search is restricted to those states that are\nreachable from the initial state. We demonstrate the usefulness of the above\ntechniques for solving FOMDPs with a system, referred to as FluCaP (formerly,\nFCPlanner), that entered the probabilistic track of the 2004 International\nPlanning Competition (IPC2004) and demonstrated an advantage over other\nplanners on the problems represented in first-order terms.\n"
  },
  {
    "id": "1109.6841",
    "title": "Learning Dependency-Based Compositional Semantics",
    "abstract": "  Suppose we want to build a system that answers a natural language question by\nrepresenting its semantics as a logical form and computing the answer given a\nstructured database of facts. The core part of such a system is the semantic\nparser that maps questions to logical forms. Semantic parsers are typically\ntrained from examples of questions annotated with their target logical forms,\nbut this type of annotation is expensive.\n  Our goal is to learn a semantic parser from question-answer pairs instead,\nwhere the logical form is modeled as a latent variable. Motivated by this\nchallenging learning problem, we develop a new semantic formalism,\ndependency-based compositional semantics (DCS), which has favorable linguistic,\nstatistical, and computational properties. We define a log-linear distribution\nover DCS logical forms and estimate the parameters using a simple procedure\nthat alternates between beam search and numerical optimization. On two standard\nsemantic parsing benchmarks, our system outperforms all existing\nstate-of-the-art systems, despite using no annotated logical forms.\n"
  },
  {
    "id": "1110.0020",
    "title": "Causes of Ineradicable Spurious Predictions in Qualitative Simulation",
    "abstract": "  It was recently proved that a sound and complete qualitative simulator does\nnot exist, that is, as long as the input-output vocabulary of the\nstate-of-the-art QSIM algorithm is used, there will always be input models\nwhich cause any simulator with a coverage guarantee to make spurious\npredictions in its output. In this paper, we examine whether a meaningfully\nexpressive restriction of this vocabulary is possible so that one can build a\nsimulator with both the soundness and completeness properties. We prove several\nnegative results: All sound qualitative simulators, employing subsets of the\nQSIM representation which retain the operating region transition feature, and\nsupport at least the addition and constancy constraints, are shown to be\ninherently incomplete. Even when the simulations are restricted to run in a\nsingle operating region, a constraint vocabulary containing just the addition,\nconstancy, derivative, and multiplication relations makes the construction of\nsound and complete qualitative simulators impossible.\n"
  },
  {
    "id": "1110.0023",
    "title": "Properties and Applications of Programs with Monotone and Convex\n  Constraints",
    "abstract": "  We study properties of programs with monotone and convex constraints. We\nextend to these formalisms concepts and results from normal logic programming.\nThey include the notions of strong and uniform equivalence with their\ncharacterizations, tight programs and Fages Lemma, program completion and loop\nformulas. Our results provide an abstract account of properties of some recent\nextensions of logic programming with aggregates, especially the formalism of\nlparse programs. They imply a method to compute stable models of lparse\nprograms by means of off-the-shelf solvers of pseudo-boolean constraints, which\nis often much faster than the smodels system.\n"
  },
  {
    "id": "1110.0024",
    "title": "How the Landscape of Random Job Shop Scheduling Instances Depends on the\n  Ratio of Jobs to Machines",
    "abstract": "  We characterize the search landscape of random instances of the job shop\nscheduling problem (JSP). Specifically, we investigate how the expected values\nof (1) backbone size, (2) distance between near-optimal schedules, and (3)\nmakespan of random schedules vary as a function of the job to machine ratio\n(N/M). For the limiting cases N/M approaches 0 and N/M approaches infinity we\nprovide analytical results, while for intermediate values of N/M we perform\nexperiments. We prove that as N/M approaches 0, backbone size approaches 100%,\nwhile as N/M approaches infinity the backbone vanishes. In the process we show\nthat as N/M approaches 0 (resp. N/M approaches infinity), simple priority rules\nalmost surely generate an optimal schedule, providing theoretical evidence of\nan \"easy-hard-easy\" pattern of typical-case instance difficulty in job shop\nscheduling. We also draw connections between our theoretical results and the\n\"big valley\" picture of JSP landscapes.\n"
  },
  {
    "id": "1110.0026",
    "title": "Preference-based Search using Example-Critiquing with Suggestions",
    "abstract": "  We consider interactive tools that help users search for their most preferred\nitem in a large collection of options. In particular, we examine\nexample-critiquing, a technique for enabling users to incrementally construct\npreference models by critiquing example options that are presented to them. We\npresent novel techniques for improving the example-critiquing technology by\nadding suggestions to its displayed options. Such suggestions are calculated\nbased on an analysis of users current preference model and their potential\nhidden preferences. We evaluate the performance of our model-based suggestion\ntechniques with both synthetic and real users. Results show that such\nsuggestions are highly attractive to users and can stimulate them to express\nmore preferences to improve the chance of identifying their most preferred item\nby up to 78%.\n"
  },
  {
    "id": "1110.0027",
    "title": "Anytime Point-Based Approximations for Large POMDPs",
    "abstract": "  The Partially Observable Markov Decision Process has long been recognized as\na rich framework for real-world planning and control problems, especially in\nrobotics. However exact solutions in this framework are typically\ncomputationally intractable for all but the smallest problems. A well-known\ntechnique for speeding up POMDP solving involves performing value backups at\nspecific belief points, rather than over the entire belief simplex. The\nefficiency of this approach, however, depends greatly on the selection of\npoints. This paper presents a set of novel techniques for selecting informative\nbelief points which work well in practice. The point selection procedure is\ncombined with point-based value backups to form an effective anytime POMDP\nalgorithm called Point-Based Value Iteration (PBVI). The first aim of this\npaper is to introduce this algorithm and present a theoretical analysis\njustifying the choice of belief selection technique. The second aim of this\npaper is to provide a thorough empirical comparison between PBVI and other\nstate-of-the-art POMDP methods, in particular the Perseus algorithm, in an\neffort to highlight their similarities and differences. Evaluation is performed\nusing both standard POMDP domains and realistic robotic tasks.\n"
  },
  {
    "id": "1110.0028",
    "title": "Solving Factored MDPs with Hybrid State and Action Variables",
    "abstract": "  Efficient representations and solutions for large decision problems with\ncontinuous and discrete variables are among the most important challenges faced\nby the designers of automated decision support systems. In this paper, we\ndescribe a novel hybrid factored Markov decision process (MDP) model that\nallows for a compact representation of these problems, and a new hybrid\napproximate linear programming (HALP) framework that permits their efficient\nsolutions. The central idea of HALP is to approximate the optimal value\nfunction by a linear combination of basis functions and optimize its weights by\nlinear programming. We analyze both theoretical and computational aspects of\nthis approach, and demonstrate its scale-up potential on several hybrid\noptimization problems.\n"
  },
  {
    "id": "1110.0029",
    "title": "Combination Strategies for Semantic Role Labeling",
    "abstract": "  This paper introduces and analyzes a battery of inference models for the\nproblem of semantic role labeling: one based on constraint satisfaction, and\nseveral strategies that model the inference as a meta-learning problem using\ndiscriminative classifiers. These classifiers are developed with a rich set of\nnovel features that encode proposition and sentence-level information. To our\nknowledge, this is the first work that: (a) performs a thorough analysis of\nlearning-based inference models for semantic role labeling, and (b) compares\nseveral inference strategies in this context. We evaluate the proposed\ninference strategies in the framework of the CoNLL-2005 shared task using only\nautomatically-generated syntactic information. The extensive experimental\nevaluation and analysis indicates that all the proposed inference strategies\nare successful -they all outperform the current best results reported in the\nCoNLL-2005 evaluation exercise- but each of the proposed approaches has its\nadvantages and disadvantages. Several important traits of a state-of-the-art\nSRL combination strategy emerge from this analysis: (i) individual models\nshould be combined at the granularity of candidate arguments rather than at the\ngranularity of complete solutions; (ii) the best combination strategy uses an\ninference model based in learning; and (iii) the learning-based inference\nbenefits from max-margin classifiers and global feedback.\n"
  },
  {
    "id": "1110.0248",
    "title": "A Behavioral Distance for Fuzzy-Transition Systems",
    "abstract": "  In contrast to the existing approaches to bisimulation for fuzzy systems, we\nintroduce a behavioral distance to measure the behavioral similarity of states\nin a nondeterministic fuzzy-transition system. This behavioral distance is\ndefined as the greatest fixed point of a suitable monotonic function and\nprovides a quantitative analogue of bisimilarity. The behavioral distance has\nthe important property that two states are at zero distance if and only if they\nare bisimilar. Moreover, for any given threshold, we find that states with\nbehavioral distances bounded by the threshold are equivalent. In addition, we\nshow that two system combinators---parallel composition and product---are\nnon-expansive with respect to our behavioral distance, which makes\ncompositional verification possible.\n"
  },
  {
    "id": "1110.1016",
    "title": "Engineering Benchmarks for Planning: the Domains Used in the\n  Deterministic Part of IPC-4",
    "abstract": "  In a field of research about general reasoning mechanisms, it is essential to\nhave appropriate benchmarks. Ideally, the benchmarks should reflect possible\napplications of the developed technology. In AI Planning, researchers more and\nmore tend to draw their testing examples from the benchmark collections used in\nthe International Planning Competition (IPC). In the organization of (the\ndeterministic part of) the fourth IPC, IPC-4, the authors therefore invested\nsignificant effort to create a useful set of benchmarks. They come from five\ndifferent (potential) real-world applications of planning: airport ground\ntraffic control, oil derivative transportation in pipeline networks,\nmodel-checking safety properties, power supply restoration, and UMTS call\nsetup. Adapting and preparing such an application for use as a benchmark in the\nIPC involves, at the time, inevitable (often drastic) simplifications, as well\nas careful choice between, and engineering of, domain encodings. For the first\ntime in the IPC, we used compilations to formulate complex domain features in\nsimple languages such as STRIPS, rather than just dropping the more interesting\nproblem constraints in the simpler language subsets. The article explains and\ndiscusses the five application domains and their adaptation to form the PDDL\ntest suites used in IPC-4. We summarize known theoretical results on structural\nproperties of the domains, regarding their computational complexity and\nprovable properties of their topology under the h+ function (an idealized\nversion of the relaxed plan heuristic). We present new (empirical) results\nilluminating properties such as the quality of the most wide-spread heuristic\nfunctions (planning graph, serial planning graph, and relaxed plan), the growth\nof propositional representations over instance size, and the number of actions\navailable to achieve each fact; we discuss these data in conjunction with the\nbest results achieved by the different kinds of planners participating in\nIPC-4.\n"
  },
  {
    "id": "1110.2200",
    "title": "Modelling Mixed Discrete-Continuous Domains for Planning",
    "abstract": "  In this paper we present pddl+, a planning domain description language for\nmodelling mixed discrete-continuous planning domains. We describe the syntax\nand modelling style of pddl+, showing that the language makes convenient the\nmodelling of complex time-dependent effects. We provide a formal semantics for\npddl+ by mapping planning instances into constructs of hybrid automata. Using\nthe syntax of HAs as our semantic model we construct a semantic mapping to\nlabelled transition systems to complete the formal interpretation of pddl+\nplanning instances. An advantage of building a mapping from pddl+ to HA theory\nis that it forms a bridge between the Planning and Real Time Systems research\ncommunities. One consequence is that we can expect to make use of some of the\ntheoretical properties of HAs. For example, for a restricted class of HAs the\nReachability problem (which is equivalent to Plan Existence) is decidable.\npddl+ provides an alternative to the continuous durative action model of\npddl2.1, adding a more flexible and robust model of time-dependent behaviour.\n"
  },
  {
    "id": "1110.2203",
    "title": "Set Intersection and Consistency in Constraint Networks",
    "abstract": "  In this paper, we show that there is a close relation between consistency in\na constraint network and set intersection. A proof schema is provided as a\ngeneric way to obtain consistency properties from properties on set\nintersection. This approach not only simplifies the understanding of and\nunifies many existing consistency results, but also directs the study of\nconsistency to that of set intersection properties in many situations, as\ndemonstrated by the results on the convexity and tightness of constraints in\nthis paper. Specifically, we identify a new class of tree convex constraints\nwhere local consistency ensures global consistency. This generalizes row convex\nconstraints. Various consistency results are also obtained on constraint\nnetworks where only some, in contrast to all in the existing work,constraints\nare tight.\n"
  },
  {
    "id": "1110.2204",
    "title": "Consistency and Random Constraint Satisfaction Models",
    "abstract": "  In this paper, we study the possibility of designing non-trivial random CSP\nmodels by exploiting the intrinsic connection between structures and\ntypical-case hardness. We show that constraint consistency, a notion that has\nbeen developed to improve the efficiency of CSP algorithms, is in fact the key\nto the design of random CSP models that have interesting phase transition\nbehavior and guaranteed exponential resolution complexity without putting much\nrestriction on the parameter of constraint tightness or the domain size of the\nproblem. We propose a very flexible framework for constructing problem\ninstances withinteresting behavior and develop a variety of concrete methods to\nconstruct specific random CSP models that enforce different levels of\nconstraint consistency. A series of experimental studies with interesting\nobservations are carried out to illustrate the effectiveness of introducing\nstructural elements in random instances, to verify the robustness of our\nproposal, and to investigate features of some specific models based on our\nframework that are highly related to the behavior of backtracking search\nalgorithms.\n"
  },
  {
    "id": "1110.2205",
    "title": "Answer Sets for Logic Programs with Arbitrary Abstract Constraint Atoms",
    "abstract": "  In this paper, we present two alternative approaches to defining answer sets\nfor logic programs with arbitrary types of abstract constraint atoms (c-atoms).\nThese approaches generalize the fixpoint-based and the level mapping based\nanswer set semantics of normal logic programs to the case of logic programs\nwith arbitrary types of c-atoms. The results are four different answer set\ndefinitions which are equivalent when applied to normal logic programs. The\nstandard fixpoint-based semantics of logic programs is generalized in two\ndirections, called answer set by reduct and answer set by complement. These\ndefinitions, which differ from each other in the treatment of\nnegation-as-failure (naf) atoms, make use of an immediate consequence operator\nto perform answer set checking, whose definition relies on the notion of\nconditional satisfaction of c-atoms w.r.t. a pair of interpretations. The other\ntwo definitions, called strongly and weakly well-supported models, are\ngeneralizations of the notion of well-supported models of normal logic programs\nto the case of programs with c-atoms. As for the case of fixpoint-based\nsemantics, the difference between these two definitions is rooted in the\ntreatment of naf atoms. We prove that answer sets by reduct (resp. by\ncomplement) are equivalent to weakly (resp. strongly) well-supported models of\na program, thus generalizing the theorem on the correspondence between stable\nmodels and well-supported models of a normal logic program to the class of\nprograms with c-atoms. We show that the newly defined semantics coincide with\npreviously introduced semantics for logic programs with monotone c-atoms, and\nthey extend the original answer set semantics of normal logic programs. We also\nstudy some properties of answer sets of programs with c-atoms, and relate our\ndefinitions to several semantics for logic programs with aggregates presented\nin the literature.\n"
  },
  {
    "id": "1110.2209",
    "title": "Bin Completion Algorithms for Multicontainer Packing, Knapsack, and\n  Covering Problems",
    "abstract": "  Many combinatorial optimization problems such as the bin packing and multiple\nknapsack problems involve assigning a set of discrete objects to multiple\ncontainers. These problems can be used to model task and resource allocation\nproblems in multi-agent systems and distributed systms, and can also be found\nas subproblems of scheduling problems. We propose bin completion, a\nbranch-and-bound strategy for one-dimensional, multicontainer packing problems.\nBin completion combines a bin-oriented search space with a powerful dominance\ncriterion that enables us to prune much of the space. The performance of the\nbasic bin completion framework can be enhanced by using a number of extensions,\nincluding nogood-based pruning techniques that allow further exploitation of\nthe dominance criterion. Bin completion is applied to four problems: multiple\nknapsack, bin covering, min-cost covering, and bin packing. We show that our\nbin completion algorithms yield new, state-of-the-art results for the multiple\nknapsack, bin covering, and min-cost covering problems, outperforming previous\nalgorithms by several orders of magnitude with respect to runtime on some\nclasses of hard, random problem instances. For the bin packing problem, we\ndemonstrate significant improvements compared to most previous results, but\nshow that bin completion is not competitive with current state-of-the-art\ncutting-stock based approaches.\n"
  },
  {
    "id": "1110.2212",
    "title": "Uncertainty in Soft Temporal Constraint Problems:A General Framework and\n  Controllability Algorithms for the Fuzzy Case",
    "abstract": "  In real-life temporal scenarios, uncertainty and preferences are often\nessential and coexisting aspects. We present a formalism where quantitative\ntemporal constraints with both preferences and uncertainty can be defined. We\nshow how three classical notions of controllability (that is, strong, weak, and\ndynamic), which have been developed for uncertain temporal problems, can be\ngeneralized to handle preferences as well. After defining this general\nframework, we focus on problems where preferences follow the fuzzy approach,\nand with properties that assure tractability. For such problems, we propose\nalgorithms to check the presence of the controllability properties. In\nparticular, we show that in such a setting dealing simultaneously with\npreferences and uncertainty does not increase the complexity of controllability\ntesting. We also develop a dynamic execution algorithm, of polynomial\ncomplexity, that produces temporal plans under uncertainty that are optimal\nwith respect to fuzzy preferences.\n"
  },
  {
    "id": "1110.2213",
    "title": "Supporting Temporal Reasoning by Mapping Calendar Expressions to Minimal\n  Periodic Sets",
    "abstract": "  In the recent years several research efforts have focused on the concept of\ntime granularity and its applications. A first stream of research investigated\nthe mathematical models behind the notion of granularity and the algorithms to\nmanage temporal data based on those models. A second stream of research\ninvestigated symbolic formalisms providing a set of algebraic operators to\ndefine granularities in a compact and compositional way. However, only very\nlimited manipulation algorithms have been proposed to operate directly on the\nalgebraic representation making it unsuitable to use the symbolic formalisms in\napplications that need manipulation of granularities.\n  This paper aims at filling the gap between the results from these two streams\nof research, by providing an efficient conversion from the algebraic\nrepresentation to the equivalent low-level representation based on the\nmathematical models. In addition, the conversion returns a minimal\nrepresentation in terms of period length. Our results have a major practical\nimpact: users can more easily define arbitrary granularities in terms of\nalgebraic operators, and then access granularity reasoning and other services\noperating efficiently on the equivalent, minimal low-level representation. As\nan example, we illustrate the application to temporal constraint reasoning with\nmultiple granularities.\n  From a technical point of view, we propose an hybrid algorithm that\ninterleaves the conversion of calendar subexpressions into periodical sets with\nthe minimization of the period length. The algorithm returns set-based\ngranularity representations having minimal period length, which is the most\nrelevant parameter for the performance of the considered reasoning services.\nExtensive experimental work supports the techniques used in the algorithm, and\nshows the efficiency and effectiveness of the algorithm.\n"
  },
  {
    "id": "1110.2216",
    "title": "The Generalized A* Architecture",
    "abstract": "  We consider the problem of computing a lightest derivation of a global\nstructure using a set of weighted rules. A large variety of inference problems\nin AI can be formulated in this framework. We generalize A* search and\nheuristics derived from abstractions to a broad class of lightest derivation\nproblems. We also describe a new algorithm that searches for lightest\nderivations using a hierarchy of abstractions. Our generalization of A* gives a\nnew algorithm for searching AND/OR graphs in a bottom-up fashion. We discuss\nhow the algorithms described here provide a general architecture for addressing\nthe pipeline problem --- the problem of passing information back and forth\nbetween various stages of processing in a perceptual system. We consider\nexamples in computer vision and natural language processing. We apply the\nhierarchical search algorithm to the problem of estimating the boundaries of\nconvex objects in grayscale images and compare it to other search methods. A\nsecond set of experiments demonstrate the use of a new compositional model for\nfinding salient curves in images.\n"
  },
  {
    "id": "1110.2726",
    "title": "Combining Spatial and Temporal Logics: Expressiveness vs. Complexity",
    "abstract": "  In this paper, we construct and investigate a hierarchy of spatio-temporal\nformalisms that result from various combinations of propositional spatial and\ntemporal logics such as the propositional temporal logic PTL, the spatial\nlogics RCC-8, BRCC-8, S4u and their fragments. The obtained results give a\nclear picture of the trade-off between expressiveness and computational\nrealisability within the hierarchy. We demonstrate how different combining\nprinciples as well as spatial and temporal primitives can produce NP-, PSPACE-,\nEXPSPACE-, 2EXPSPACE-complete, and even undecidable spatio-temporal logics out\nof components that are at most NP- or PSPACE-complete.\n"
  },
  {
    "id": "1110.2728",
    "title": "An Approach to Temporal Planning and Scheduling in Domains with\n  Predictable Exogenous Events",
    "abstract": "  The treatment of exogenous events in planning is practically important in\nmany real-world domains where the preconditions of certain plan actions are\naffected by such events. In this paper we focus on planning in temporal domains\nwith exogenous events that happen at known times, imposing the constraint that\ncertain actions in the plan must be executed during some predefined time\nwindows. When actions have durations, handling such temporal constraints adds\nan extra difficulty to planning. We propose an approach to planning in these\ndomains which integrates constraint-based temporal reasoning into a graph-based\nplanning framework using local search. Our techniques are implemented in a\nplanner that took part in the 4th International Planning Competition (IPC-4). A\nstatistical analysis of the results of IPC-4 demonstrates the effectiveness of\nour approach in terms of both CPU-time and plan quality. Additional experiments\nshow the good performance of the temporal reasoning techniques integrated into\nour planner.\n"
  },
  {
    "id": "1110.2729",
    "title": "The Power of Modeling - a Response to PDDL2.1",
    "abstract": "  In this commentary I argue that although PDDL is a very useful standard for\nthe planning competition, its design does not properly consider the issue of\ndomain modeling. Hence, I would not advocate its use in specifying planning\ndomains outside of the context of the planning competition. Rather, the field\nneeds to explore different approaches and grapple more directly with the\nproblem of effectively modeling and utilizing all of the diverse pieces of\nknowledge we typically have about planning domains.\n"
  },
  {
    "id": "1110.2730",
    "title": "Imperfect Match: PDDL 2.1 and Real Applications",
    "abstract": "  PDDL was originally conceived and constructed as a lingua franca for the\nInternational Planning Competition. PDDL2.1 embodies a set of extensions\nintended to support the expression of something closer to real planning\nproblems. This objective has only been partially achieved, due in large part to\na deliberate focus on not moving too far from classical planning models and\nsolution methods.\n"
  },
  {
    "id": "1110.2731",
    "title": "PDDL 2.1: Representation vs. Computation",
    "abstract": "  I comment on the PDDL 2.1 language and its use in the planning competition,\nfocusing on the choices made for accommodating time and concurrency. I also\ndiscuss some methodological issues that have to do with the move toward more\nexpressive planning languages and the balance needed in planning research\nbetween semantics and computation.\n"
  },
  {
    "id": "1110.2732",
    "title": "Proactive Algorithms for Job Shop Scheduling with Probabilistic\n  Durations",
    "abstract": "  Most classical scheduling formulations assume a fixed and known duration for\neach activity. In this paper, we weaken this assumption, requiring instead that\neach duration can be represented by an independent random variable with a known\nmean and variance. The best solutions are ones which have a high probability of\nachieving a good makespan. We first create a theoretical framework, formally\nshowing how Monte Carlo simulation can be combined with deterministic\nscheduling algorithms to solve this problem. We propose an associated\ndeterministic scheduling problem whose solution is proved, under certain\nconditions, to be a lower bound for the probabilistic problem. We then propose\nand investigate a number of techniques for solving such problems based on\ncombinations of Monte Carlo simulation, solutions to the associated\ndeterministic problem, and either constraint programming or tabu search. Our\nempirical results demonstrate that a combination of the use of the associated\ndeterministic problem and Monte Carlo simulation results in algorithms that\nscale best both in terms of problem size and uncertainty. Further experiments\npoint to the correlation between the quality of the deterministic solution and\nthe quality of the probabilistic solution as a major factor responsible for\nthis success.\n"
  },
  {
    "id": "1110.2734",
    "title": "The Language of Search",
    "abstract": "  This paper is concerned with a class of algorithms that perform exhaustive\nsearch on propositional knowledge bases. We show that each of these algorithms\ndefines and generates a propositional language. Specifically, we show that the\ntrace of a search can be interpreted as a combinational circuit, and a search\nalgorithm then defines a propositional language consisting of circuits that are\ngenerated across all possible executions of the algorithm. In particular, we\nshow that several versions of exhaustive DPLL search correspond to such\nwell-known languages as FBDD, OBDD, and a precisely-defined subset of d-DNNF.\nBy thus mapping search algorithms to propositional languages, we provide a\nuniform and practical framework in which successful search techniques can be\nharnessed for compilation of knowledge into various languages of interest, and\na new methodology whereby the power and limitations of search algorithms can be\nunderstood by looking up the tractability and succinctness of the corresponding\npropositional languages.\n"
  },
  {
    "id": "1110.2735",
    "title": "Understanding Algorithm Performance on an Oversubscribed Scheduling\n  Application",
    "abstract": "  The best performing algorithms for a particular oversubscribed scheduling\napplication, Air Force Satellite Control Network (AFSCN) scheduling, appear to\nhave little in common. Yet, through careful experimentation and modeling of\nperformance in real problem instances, we can relate characteristics of the\nbest algorithms to characteristics of the application. In particular, we find\nthat plateaus dominate the search spaces (thus favoring algorithms that make\nlarger changes to solutions) and that some randomization in exploration is\ncritical to good performance (due to the lack of gradient information on the\nplateaus). Based on our explanations of algorithm performance, we develop a new\nalgorithm that combines characteristics of the best performers; the new\nalgorithms performance is better than the previous best. We show how hypothesis\ndriven experimentation and search modeling can both explain algorithm\nperformance and motivate the design of a new algorithm.\n"
  },
  {
    "id": "1110.2736",
    "title": "Marvin: A Heuristic Search Planner with Online Macro-Action Learning",
    "abstract": "  This paper describes Marvin, a planner that competed in the Fourth\nInternational Planning Competition (IPC 4). Marvin uses\naction-sequence-memoisation techniques to generate macro-actions, which are\nthen used during search for a solution plan. We provide an overview of its\narchitecture and search behaviour, detailing the algorithms used. We also\nempirically demonstrate the effectiveness of its features in various planning\ndomains; in particular, the effects on performance due to the use of\nmacro-actions, the novel features of its search behaviour, and the native\nsupport of ADL and Derived Predicates.\n"
  },
  {
    "id": "1110.2737",
    "title": "Anytime Heuristic Search",
    "abstract": "  We describe how to convert the heuristic search algorithm A* into an anytime\nalgorithm that finds a sequence of improved solutions and eventually converges\nto an optimal solution. The approach we adopt uses weighted heuristic search to\nfind an approximate solution quickly, and then continues the weighted search to\nfind improved solutions as well as to improve a bound on the suboptimality of\nthe current solution. When the time available to solve a search problem is\nlimited or uncertain, this creates an anytime heuristic search algorithm that\nallows a flexible tradeoff between search time and solution quality. We analyze\nthe properties of the resulting Anytime A* algorithm, and consider its\nperformance in three domains; sliding-tile puzzles, STRIPS planning, and\nmultiple sequence alignment. To illustrate the generality of this approach, we\nalso describe how to transform the memory-efficient search algorithm Recursive\nBest-First Search (RBFS) into an anytime algorithm.\n"
  },
  {
    "id": "1110.2738",
    "title": "Discovering Classes of Strongly Equivalent Logic Programs",
    "abstract": "  In this paper we apply computer-aided theorem discovery technique to discover\ntheorems about strongly equivalent logic programs under the answer set\nsemantics. Our discovered theorems capture new classes of strongly equivalent\nlogic programs that can lead to new program simplification rules that preserve\nstrong equivalence. Specifically, with the help of computers, we discovered\nexact conditions that capture the strong equivalence between a rule and the\nempty set, between two rules, between two rules and one of the two rules,\nbetween two rules and another rule, and between three rules and two of the\nthree rules.\n"
  },
  {
    "id": "1110.2739",
    "title": "Phase Transition for Random Quantified XOR-Formulas",
    "abstract": "  The QXORSAT problem is the quantified version of the satisfiability problem\nXORSAT in which the connective exclusive-or is used instead of the usual or. We\nstudy the phase transition associated with random QXORSAT instances. We give a\ndescription of this phase transition in the case of one alternation of\nquantifiers, thus performing an advanced practical and theoretical study on the\nphase transition of a quantified roblem.\n"
  },
  {
    "id": "1110.2740",
    "title": "Cutset Sampling for Bayesian Networks",
    "abstract": "  The paper presents a new sampling methodology for Bayesian networks that\nsamples only a subset of variables and applies exact inference to the rest.\nCutset sampling is a network structure-exploiting application of the\nRao-Blackwellisation principle to sampling in Bayesian networks. It improves\nconvergence by exploiting memory-based inference algorithms. It can also be\nviewed as an anytime approximation of the exact cutset-conditioning algorithm\ndeveloped by Pearl. Cutset sampling can be implemented efficiently when the\nsampled variables constitute a loop-cutset of the Bayesian network and, more\ngenerally, when the induced width of the networks graph conditioned on the\nobserved sampled variables is bounded by a constant w. We demonstrate\nempirically the benefit of this scheme on a range of benchmarks.\n"
  },
  {
    "id": "1110.2741",
    "title": "An Algebraic Graphical Model for Decision with Uncertainties,\n  Feasibilities, and Utilities",
    "abstract": "  Numerous formalisms and dedicated algorithms have been designed in the last\ndecades to model and solve decision making problems. Some formalisms, such as\nconstraint networks, can express \"simple\" decision problems, while others are\ndesigned to take into account uncertainties, unfeasible decisions, and\nutilities. Even in a single formalism, several variants are often proposed to\nmodel different types of uncertainty (probability, possibility...) or utility\n(additive or not). In this article, we introduce an algebraic graphical model\nthat encompasses a large number of such formalisms: (1) we first adapt previous\nstructures from Friedman, Chu and Halpern for representing uncertainty,\nutility, and expected utility in order to deal with generic forms of sequential\ndecision making; (2) on these structures, we then introduce composite graphical\nmodels that express information via variables linked by \"local\" functions,\nthanks to conditional independence; (3) on these graphical models, we finally\ndefine a simple class of queries which can represent various scenarios in terms\nof observabilities and controllabilities. A natural decision-tree semantics for\nsuch queries is completed by an equivalent operational semantics, which induces\ngeneric algorithms. The proposed framework, called the\nPlausibility-Feasibility-Utility (PFU) framework, not only provides a better\nunderstanding of the links between existing formalisms, but it also covers yet\nunpublished frameworks (such as possibilistic influence diagrams) and unifies\nformalisms such as quantified boolean formulas and influence diagrams. Our\nbacktrack and variable elimination generic algorithms are a first step towards\nunified algorithms.\n"
  },
  {
    "id": "1110.2742",
    "title": "Semantic Matchmaking as Non-Monotonic Reasoning: A Description Logic\n  Approach",
    "abstract": "  Matchmaking arises when supply and demand meet in an electronic marketplace,\nor when agents search for a web service to perform some task, or even when\nrecruiting agencies match curricula and job profiles. In such open\nenvironments, the objective of a matchmaking process is to discover best\navailable offers to a given request. We address the problem of matchmaking from\na knowledge representation perspective, with a formalization based on\nDescription Logics. We devise Concept Abduction and Concept Contraction as\nnon-monotonic inferences in Description Logics suitable for modeling\nmatchmaking in a logical framework, and prove some related complexity results.\nWe also present reasonable algorithms for semantic matchmaking based on the\ndevised inferences, and prove that they obey to some commonsense properties.\nFinally, we report on the implementation of the proposed matchmaking framework,\nwhich has been used both as a mediator in e-marketplaces and for semantic web\nservices discovery.\n"
  },
  {
    "id": "1110.2743",
    "title": "Solution-Guided Multi-Point Constructive Search for Job Shop Scheduling",
    "abstract": "  Solution-Guided Multi-Point Constructive Search (SGMPCS) is a novel\nconstructive search technique that performs a series of resource-limited tree\nsearches where each search begins either from an empty solution (as in\nrandomized restart) or from a solution that has been encountered during the\nsearch. A small number of these \"elite solutions is maintained during the\nsearch. We introduce the technique and perform three sets of experiments on the\njob shop scheduling problem. First, a systematic, fully crossed study of SGMPCS\nis carried out to evaluate the performance impact of various parameter\nsettings. Second, we inquire into the diversity of the elite solution set,\nshowing, contrary to expectations, that a less diverse set leads to stronger\nperformance. Finally, we compare the best parameter setting of SGMPCS from the\nfirst two experiments to chronological backtracking, limited discrepancy\nsearch, randomized restart, and a sophisticated tabu search algorithm on a set\nof well-known benchmark problems. Results demonstrate that SGMPCS is\nsignificantly better than the other constructive techniques tested, though lags\nbehind the tabu search.\n"
  },
  {
    "id": "1110.3002",
    "title": "Are Minds Computable?",
    "abstract": "  This essay explores the limits of Turing machines concerning the modeling of\nminds and suggests alternatives to go beyond those limits.\n"
  },
  {
    "id": "1110.3385",
    "title": "Fuzzy Inference Systems Optimization",
    "abstract": "  This paper compares various optimization methods for fuzzy inference system\noptimization. The optimization methods compared are genetic algorithm, particle\nswarm optimization and simulated annealing. When these techniques were\nimplemented it was observed that the performance of each technique within the\nfuzzy inference system classification was context dependent.\n"
  },
  {
    "id": "1110.3888",
    "title": "Handling controversial arguments by matrix",
    "abstract": "  We introduce matrix and its block to the Dung's theory of argumentation\nframework. It is showed that each argumentation framework has a matrix\nrepresentation, and the indirect attack relation and indirect defence relation\ncan be characterized by computing the matrix. This provide a powerful\nmathematics way to determine the \"controversial arguments\" in an argumentation\nframework. Also, we introduce several kinds of blocks based on the matrix, and\nvarious prudent semantics of argumentation frameworks can all be determined by\ncomputing and comparing the matrices and their blocks which we have defined. In\ncontrast with traditional method of directed graph, the matrix method has an\nexcellent advantage: computability(even can be realized on computer easily).\nSo, there is an intensive perspective to import the theory of matrix to the\nresearch of argumentation frameworks and its related areas.\n"
  },
  {
    "id": "1110.4076",
    "title": "Learning in Real-Time Search: A Unifying Framework",
    "abstract": "  Real-time search methods are suited for tasks in which the agent is\ninteracting with an initially unknown environment in real time. In such\nsimultaneous planning and learning problems, the agent has to select its\nactions in a limited amount of time, while sensing only a local part of the\nenvironment centered at the agents current location. Real-time heuristic search\nagents select actions using a limited lookahead search and evaluating the\nfrontier states with a heuristic function. Over repeated experiences, they\nrefine heuristic values of states to avoid infinite loops and to converge to\nbetter solutions. The wide spread of such settings in autonomous software and\nhardware agents has led to an explosion of real-time search algorithms over the\nlast two decades. Not only is a potential user confronted with a hodgepodge of\nalgorithms, but he also faces the choice of control parameters they use. In\nthis paper we address both problems. The first contribution is an introduction\nof a simple three-parameter framework (named LRTS) which extracts the core\nideas behind many existing algorithms. We then prove that LRTA*, epsilon-LRTA*,\nSLA*, and gamma-Trap algorithms are special cases of our framework. Thus, they\nare unified and extended with additional features. Second, we prove\ncompleteness and convergence of any algorithm covered by the LRTS framework.\nThird, we prove several upper-bounds relating the control parameters and\nsolution quality. Finally, we analyze the influence of the three control\nparameters empirically in the realistic scalable domains of real-time\nnavigation on initially unknown maps from a commercial role-playing game as\nwell as routing in ad hoc sensor networks.\n"
  },
  {
    "id": "1110.4719",
    "title": "A Generalized Arc-Consistency Algorithm for a Class of Counting\n  Constraints: Revised Edition that Incorporates One Correction",
    "abstract": "  This paper introduces the SEQ BIN meta-constraint with a polytime algorithm\nachieving general- ized arc-consistency according to some properties. SEQ BIN\ncan be used for encoding counting con- straints such as CHANGE, SMOOTH or\nINCREAS- ING NVALUE. For some of these constraints and some of their variants\nGAC can be enforced with a time and space complexity linear in the sum of\ndomain sizes, which improves or equals the best known results of the\nliterature.\n"
  },
  {
    "id": "1110.5172",
    "title": "Quels formalismes temporels pour repr\\'esenter des connaissances\n  extraites de textes de recettes de cuisine ?",
    "abstract": "  The Taaable projet goal is to create a case-based reasoning system for\nretrieval and adaptation of cooking recipes. Within this framework, we are\ndiscussing the temporal aspects of recipes and the means of representing those\nin order to adapt their text.\n"
  },
  {
    "id": "1110.6290",
    "title": "Modelling Constraint Solver Architecture Design as a Constraint Problem",
    "abstract": "  Designing component-based constraint solvers is a complex problem. Some\ncomponents are required, some are optional and there are interdependencies\nbetween the components. Because of this, previous approaches to solver design\nand modification have been ad-hoc and limited. We present a system that\ntransforms a description of the components and the characteristics of the\ntarget constraint solver into a constraint problem. Solving this problem yields\nthe description of a valid solver. Our approach represents a significant step\ntowards the automated design and synthesis of constraint solvers that are\nspecialised for individual constraint problem classes or instances.\n"
  },
  {
    "id": "1110.6589",
    "title": "A cognitive diversity framework for radar target classification",
    "abstract": "  Classification of targets by radar has proved to be notoriously difficult\nwith the best systems still yet to attain sufficiently high levels of\nperformance and reliability. In the current contribution we explore a new\ndesign of radar based target recognition, where angular diversity is used in a\ncognitive manner to attain better performance. Performance is bench- marked\nagainst conventional classification schemes. The proposed scheme can easily be\nextended to cognitive target recognition based on multiple diversity\nstrategies.\n"
  },
  {
    "id": "1111.0039",
    "title": "Reasoning with Very Expressive Fuzzy Description Logics",
    "abstract": "  It is widely recognized today that the management of imprecision and\nvagueness will yield more intelligent and realistic knowledge-based\napplications. Description Logics (DLs) are a family of knowledge representation\nlanguages that have gained considerable attention the last decade, mainly due\nto their decidability and the existence of empirically high performance of\nreasoning algorithms. In this paper, we extend the well known fuzzy ALC DL to\nthe fuzzy SHIN DL, which extends the fuzzy ALC DL with transitive role axioms\n(S), inverse roles (I), role hierarchies (H) and number restrictions (N). We\nillustrate why transitive role axioms are difficult to handle in the presence\nof fuzzy interpretations and how to handle them properly. Then we extend these\nresults by adding role hierarchies and finally number restrictions. The main\ncontributions of the paper are the decidability proof of the fuzzy DL languages\nfuzzy-SI and fuzzy-SHIN, as well as decision procedures for the knowledge base\nsatisfiability problem of the fuzzy-SI and fuzzy-SHIN.\n"
  },
  {
    "id": "1111.0040",
    "title": "New Inference Rules for Max-SAT",
    "abstract": "  Exact Max-SAT solvers, compared with SAT solvers, apply little inference at\neach node of the proof tree. Commonly used SAT inference rules like unit\npropagation produce a simplified formula that preserves satisfiability but,\nunfortunately, solving the Max-SAT problem for the simplified formula is not\nequivalent to solving it for the original formula. In this paper, we define a\nnumber of original inference rules that, besides being applied efficiently,\ntransform Max-SAT instances into equivalent Max-SAT instances which are easier\nto solve. The soundness of the rules, that can be seen as refinements of unit\nresolution adapted to Max-SAT, are proved in a novel and simple way via an\ninteger programming transformation. With the aim of finding out how powerful\nthe inference rules are in practice, we have developed a new Max-SAT solver,\ncalled MaxSatz, which incorporates those rules, and performed an experimental\ninvestigation. The results provide empirical evidence that MaxSatz is very\ncompetitive, at least, on random Max-2SAT, random Max-3SAT, Max-Cut, and Graph\n3-coloring instances, as well as on the benchmarks from the Max-SAT Evaluation\n2006.\n"
  },
  {
    "id": "1111.0043",
    "title": "Obtaining Reliable Feedback for Sanctioning Reputation Mechanisms",
    "abstract": "  Reputation mechanisms offer an effective alternative to verification\nauthorities for building trust in electronic markets with moral hazard. Future\nclients guide their business decisions by considering the feedback from past\ntransactions; if truthfully exposed, cheating behavior is sanctioned and thus\nbecomes irrational.\n  It therefore becomes important to ensure that rational clients have the right\nincentives to report honestly. As an alternative to side-payment schemes that\nexplicitly reward truthful reports, we show that honesty can emerge as a\nrational behavior when clients have a repeated presence in the market. To this\nend we describe a mechanism that supports an equilibrium where truthful\nfeedback is obtained. Then we characterize the set of pareto-optimal equilibria\nof the mechanism, and derive an upper bound on the percentage of false reports\nthat can be recorded by the mechanism. An important role in the existence of\nthis bound is played by the fact that rational clients can establish a\nreputation for reporting honestly.\n"
  },
  {
    "id": "1111.0044",
    "title": "Probabilistic Planning via Heuristic Forward Search and Weighted Model\n  Counting",
    "abstract": "  We present a new algorithm for probabilistic planning with no observability.\nOur algorithm, called Probabilistic-FF, extends the heuristic forward-search\nmachinery of Conformant-FF to problems with probabilistic uncertainty about\nboth the initial state and action effects. Specifically, Probabilistic-FF\ncombines Conformant-FFs techniques with a powerful machinery for weighted model\ncounting in (weighted) CNFs, serving to elegantly define both the search space\nand the heuristic function. Our evaluation of Probabilistic-FF shows its fine\nscalability in a range of probabilistic domains, constituting a several orders\nof magnitude improvement over previous results in this area. We use a\nproblematic case to point out the main open issue to be addressed by further\nresearch.\n"
  },
  {
    "id": "1111.0049",
    "title": "Conjunctive Query Answering for the Description Logic SHIQ",
    "abstract": "  Conjunctive queries play an important role as an expressive query language\nfor Description Logics (DLs). Although modern DLs usually provide for\ntransitive roles, conjunctive query answering over DL knowledge bases is only\npoorly understood if transitive roles are admitted in the query. In this paper,\nwe consider unions of conjunctive queries over knowledge bases formulated in\nthe prominent DL SHIQ and allow transitive roles in both the query and the\nknowledge base. We show decidability of query answering in this setting and\nestablish two tight complexity bounds: regarding combined complexity, we prove\nthat there is a deterministic algorithm for query answering that needs time\nsingle exponential in the size of the KB and double exponential in the size of\nthe query, which is optimal. Regarding data complexity, we prove containment in\nco-NP.\n"
  },
  {
    "id": "1111.0051",
    "title": "Qualitative System Identification from Imperfect Data",
    "abstract": "  Experience in the physical sciences suggests that the only realistic means of\nunderstanding complex systems is through the use of mathematical models.\nTypically, this has come to mean the identification of quantitative models\nexpressed as differential equations. Quantitative modelling works best when the\nstructure of the model (i.e., the form of the equations) is known; and the\nprimary concern is one of estimating the values of the parameters in the model.\nFor complex biological systems, the model-structure is rarely known and the\nmodeler has to deal with both model-identification and parameter-estimation. In\nthis paper we are concerned with providing automated assistance to the first of\nthese problems. Specifically, we examine the identification by machine of the\nstructural relationships between experimentally observed variables. These\nrelationship will be expressed in the form of qualitative abstractions of a\nquantitative model. Such qualitative models may not only provide clues to the\nprecise quantitative model, but also assist in understanding the essence of\nthat model. Our position in this paper is that background knowledge\nincorporating system modelling principles can be used to constrain effectively\nthe set of good qualitative models. Utilising the model-identification\nframework provided by Inductive Logic Programming (ILP) we present empirical\nsupport for this position using a series of increasingly complex artificial\ndatasets. The results are obtained with qualitative and quantitative data\nsubject to varying amounts of noise and different degrees of sparsity. The\nresults also point to the presence of a set of qualitative states, which we\nterm kernel subsets, that may be necessary for a qualitative model-learner to\nlearn correct models. We demonstrate scalability of the method to biological\nsystem modelling by identification of the glycolysis metabolic pathway from\ndata.\n"
  },
  {
    "id": "1111.0053",
    "title": "Exploiting Subgraph Structure in Multi-Robot Path Planning",
    "abstract": "  Multi-robot path planning is difficult due to the combinatorial explosion of\nthe search space with every new robot added. Complete search of the combined\nstate-space soon becomes intractable. In this paper we present a novel form of\nabstraction that allows us to plan much more efficiently. The key to this\nabstraction is the partitioning of the map into subgraphs of known structure\nwith entry and exit restrictions which we can represent compactly. Planning\nthen becomes a search in the much smaller space of subgraph configurations.\nOnce an abstract plan is found, it can be quickly resolved into a correct (but\npossibly sub-optimal) concrete plan without the need for further search. We\nprove that this technique is sound and complete and demonstrate its practical\neffectiveness on a real map.\n  A contending solution, prioritised planning, is also evaluated and shown to\nhave similar performance albeit at the cost of completeness. The two approaches\nare not necessarily conflicting; we demonstrate how they can be combined into a\nsingle algorithm which outperforms either approach alone.\n"
  },
  {
    "id": "1111.0055",
    "title": "Extended RDF as a Semantic Foundation of Rule Markup Languages",
    "abstract": "  Ontologies and automated reasoning are the building blocks of the Semantic\nWeb initiative. Derivation rules can be included in an ontology to define\nderived concepts, based on base concepts. For example, rules allow to define\nthe extension of a class or property, based on a complex relation between the\nextensions of the same or other classes and properties. On the other hand, the\ninclusion of negative information both in the form of negation-as-failure and\nexplicit negative information is also needed to enable various forms of\nreasoning. In this paper, we extend RDF graphs with weak and strong negation,\nas well as derivation rules. The ERDF stable model semantics of the extended\nframework (Extended RDF) is defined, extending RDF(S) semantics. A distinctive\nfeature of our theory, which is based on Partial Logic, is that both truth and\nfalsity extensions of properties and classes are considered, allowing for truth\nvalue gaps. Our framework supports both closed-world and open-world reasoning\nthrough the explicit representation of the particular closed-world assumptions\nand the ERDF ontological categories of total properties and total classes.\n"
  },
  {
    "id": "1111.0056",
    "title": "The Complexity of Planning Problems With Simple Causal Graphs",
    "abstract": "  We present three new complexity results for classes of planning problems with\nsimple causal graphs. First, we describe a polynomial-time algorithm that uses\nmacros to generate plans for the class 3S of planning problems with binary\nstate variables and acyclic causal graphs. This implies that plan generation\nmay be tractable even when a planning problem has an exponentially long minimal\nsolution. We also prove that the problem of plan existence for planning\nproblems with multi-valued variables and chain causal graphs is NP-hard.\nFinally, we show that plan existence for planning problems with binary state\nvariables and polytree causal graphs is NP-complete.\n"
  },
  {
    "id": "1111.0059",
    "title": "Loosely Coupled Formulations for Automated Planning: An Integer\n  Programming Perspective",
    "abstract": "  We represent planning as a set of loosely coupled network flow problems,\nwhere each network corresponds to one of the state variables in the planning\ndomain. The network nodes correspond to the state variable values and the\nnetwork arcs correspond to the value transitions. The planning problem is to\nfind a path (a sequence of actions) in each network such that, when merged,\nthey constitute a feasible plan. In this paper we present a number of integer\nprogramming formulations that model these loosely coupled networks with varying\ndegrees of flexibility. Since merging may introduce exponentially many ordering\nconstraints we implement a so-called branch-and-cut algorithm, in which these\nconstraints are dynamically generated and added to the formulation when needed.\nOur results are very promising, they improve upon previous planning as integer\nprogramming approaches and lay the foundation for integer programming\napproaches for cost optimal planning.\n"
  },
  {
    "id": "1111.0060",
    "title": "A Constraint Programming Approach for Solving a Queueing Control Problem",
    "abstract": "  In a facility with front room and back room operations, it is useful to\nswitch workers between the rooms in order to cope with changing customer\ndemand. Assuming stochastic customer arrival and service times, we seek a\npolicy for switching workers such that the expected customer waiting time is\nminimized while the expected back room staffing is sufficient to perform all\nwork. Three novel constraint programming models and several shaving procedures\nfor these models are presented. Experimental results show that a model based on\nclosed-form expressions together with a combination of shaving procedures is\nthe most efficient. This model is able to find and prove optimal solutions for\nmany problem instances within a reasonable run-time. Previously, the only\navailable approach was a heuristic algorithm. Furthermore, a hybrid method\ncombining the heuristic and the best constraint programming method is shown to\nperform as well as the heuristic in terms of solution quality over time, while\nachieving the same performance in terms of proving optimality as the pure\nconstraint programming model. This is the first work of which we are aware that\nsolves such queueing-based problems with constraint programming.\n"
  },
  {
    "id": "1111.0062",
    "title": "Optimal and Approximate Q-value Functions for Decentralized POMDPs",
    "abstract": "  Decision-theoretic planning is a popular approach to sequential decision\nmaking problems, because it treats uncertainty in sensing and acting in a\nprincipled way. In single-agent frameworks like MDPs and POMDPs, planning can\nbe carried out by resorting to Q-value functions: an optimal Q-value function\nQ* is computed in a recursive manner by dynamic programming, and then an\noptimal policy is extracted from Q*. In this paper we study whether similar\nQ-value functions can be defined for decentralized POMDP models (Dec-POMDPs),\nand how policies can be extracted from such value functions. We define two\nforms of the optimal Q-value function for Dec-POMDPs: one that gives a\nnormative description as the Q-value function of an optimal pure joint policy\nand another one that is sequentially rational and thus gives a recipe for\ncomputation. This computation, however, is infeasible for all but the smallest\nproblems. Therefore, we analyze various approximate Q-value functions that\nallow for efficient computation. We describe how they relate, and we prove that\nthey all provide an upper bound to the optimal Q-value function Q*. Finally,\nunifying some previous approaches for solving Dec-POMDPs, we describe a family\nof algorithms for extracting policies from such Q-value functions, and perform\nan experimental evaluation on existing test problems, including a new\nfirefighting benchmark problem.\n"
  },
  {
    "id": "1111.0065",
    "title": "Communication-Based Decomposition Mechanisms for Decentralized MDPs",
    "abstract": "  Multi-agent planning in stochastic environments can be framed formally as a\ndecentralized Markov decision problem. Many real-life distributed problems that\narise in manufacturing, multi-robot coordination and information gathering\nscenarios can be formalized using this framework. However, finding the optimal\nsolution in the general case is hard, limiting the applicability of recently\ndeveloped algorithms. This paper provides a practical approach for solving\ndecentralized control problems when communication among the decision makers is\npossible, but costly. We develop the notion of communication-based mechanism\nthat allows us to decompose a decentralized MDP into multiple single-agent\nproblems. In this framework, referred to as decentralized semi-Markov decision\nprocess with direct communication (Dec-SMDP-Com), agents operate separately\nbetween communications. We show that finding an optimal mechanism is equivalent\nto solving optimally a Dec-SMDP-Com. We also provide a heuristic search\nalgorithm that converges on the optimal decomposition. Restricting the\ndecomposition to some specific types of local behaviors reduces significantly\nthe complexity of planning. In particular, we present a polynomial-time\nalgorithm for the case in which individual agents perform goal-oriented\nbehaviors between communications. The paper concludes with an additional\ntractable algorithm that enables the introduction of human knowledge, thereby\nreducing the overall problem to finding the best time to communicate. Empirical\nresults show that these approaches provide good approximate solutions.\n"
  },
  {
    "id": "1111.0067",
    "title": "A General Theory of Additive State Space Abstractions",
    "abstract": "  Informally, a set of abstractions of a state space S is additive if the\ndistance between any two states in S is always greater than or equal to the sum\nof the corresponding distances in the abstract spaces. The first known additive\nabstractions, called disjoint pattern databases, were experimentally\ndemonstrated to produce state of the art performance on certain state spaces.\nHowever, previous applications were restricted to state spaces with special\nproperties, which precludes disjoint pattern databases from being defined for\nseveral commonly used testbeds, such as Rubiks Cube, TopSpin and the Pancake\npuzzle. In this paper we give a general definition of additive abstractions\nthat can be applied to any state space and prove that heuristics based on\nadditive abstractions are consistent as well as admissible. We use this new\ndefinition to create additive abstractions for these testbeds and show\nexperimentally that well chosen additive abstractions can reduce search time\nsubstantially for the (18,4)-TopSpin puzzle and by three orders of magnitude\nover state of the art methods for the 17-Pancake puzzle. We also derive a way\nof testing if the heuristic value returned by additive abstractions is provably\ntoo low and show that the use of this test can reduce search time for the\n15-puzzle and TopSpin by roughly a factor of two.\n"
  },
  {
    "id": "1111.0068",
    "title": "First Order Decision Diagrams for Relational MDPs",
    "abstract": "  Markov decision processes capture sequential decision making under\nuncertainty, where an agent must choose actions so as to optimize long term\nreward. The paper studies efficient reasoning mechanisms for Relational Markov\nDecision Processes (RMDP) where world states have an internal relational\nstructure that can be naturally described in terms of objects and relations\namong them. Two contributions are presented. First, the paper develops First\nOrder Decision Diagrams (FODD), a new compact representation for functions over\nrelational structures, together with a set of operators to combine FODDs, and\nnovel reduction techniques to keep the representation small. Second, the paper\nshows how FODDs can be used to develop solutions for RMDPs, where reasoning is\nperformed at the abstract level and the resulting optimal policy is independent\nof domain size (number of objects) or instantiation. In particular, a variant\nof the value iteration algorithm is developed by using special operations over\nFODDs, and the algorithm is shown to converge to the optimal policy.\n"
  },
  {
    "id": "1111.0860",
    "title": "Clause/Term Resolution and Learning in the Evaluation of Quantified\n  Boolean Formulas",
    "abstract": "  Resolution is the rule of inference at the basis of most procedures for\nautomated reasoning. In these procedures, the input formula is first translated\ninto an equisatisfiable formula in conjunctive normal form (CNF) and then\nrepresented as a set of clauses. Deduction starts by inferring new clauses by\nresolution, and goes on until the empty clause is generated or satisfiability\nof the set of clauses is proven, e.g., because no new clauses can be generated.\n  In this paper, we restrict our attention to the problem of evaluating\nQuantified Boolean Formulas (QBFs). In this setting, the above outlined\ndeduction process is known to be sound and complete if given a formula in CNF\nand if a form of resolution, called Q-resolution, is used. We introduce\nQ-resolution on terms, to be used for formulas in disjunctive normal form. We\nshow that the computation performed by most of the available procedures for\nQBFs --based on the Davis-Logemann-Loveland procedure (DLL) for propositional\nsatisfiability-- corresponds to a tree in which Q-resolution on terms and\nclauses alternate. This poses the theoretical bases for the introduction of\nlearning, corresponding to recording Q-resolution formulas associated with the\nnodes of the tree. We discuss the problems related to the introduction of\nlearning in DLL based procedures, and present solutions extending\nstate-of-the-art proposals coming from the literature on propositional\nsatisfiability. Finally, we show that our DLL based solver extended with\nlearning, performs significantly better on benchmarks used in the 2003 QBF\nsolvers comparative evaluation.\n"
  },
  {
    "id": "1111.1321",
    "title": "MIVAR: Transition from Productions to Bipartite Graphs MIVAR Nets and\n  Practical Realization of Automated Constructor of Algorithms Handling More\n  than Three Million Production Rules",
    "abstract": "  The theoretical transition from the graphs of production systems to the\nbipartite graphs of the MIVAR nets is shown. Examples of the implementation of\nthe MIVAR nets in the formalisms of matrixes and graphs are given. The linear\ncomputational complexity of algorithms for automated building of objects and\nrules of the MIVAR nets is theoretically proved. On the basis of the MIVAR nets\nthe UDAV software complex is developed, handling more than 1.17 million objects\nand more than 3.5 million rules on ordinary computers. The results of\nexperiments that confirm a linear computational complexity of the MIVAR method\nof information processing are given.\n  Keywords: MIVAR, MIVAR net, logical inference, computational complexity,\nartificial intelligence, intelligent systems, expert systems, General Problem\nSolver.\n"
  },
  {
    "id": "1111.1486",
    "title": "Embedding Description Logic Programs into Default Logic",
    "abstract": "  Description logic programs (dl-programs) under the answer set semantics\nformulated by Eiter {\\em et al.} have been considered as a prominent formalism\nfor integrating rules and ontology knowledge bases. A question of interest has\nbeen whether dl-programs can be captured in a general formalism of nonmonotonic\nlogic. In this paper, we study the possibility of embedding dl-programs into\ndefault logic. We show that dl-programs under the strong and weak answer set\nsemantics can be embedded in default logic by combining two translations, one\nof which eliminates the constraint operator from nonmonotonic dl-atoms and the\nother translates a dl-program into a default theory. For dl-programs without\nnonmonotonic dl-atoms but with the negation-as-failure operator, our embedding\nis polynomial, faithful, and modular. In addition, our default logic encoding\ncan be extended in a simple way to capture recently proposed weakly\nwell-supported answer set semantics, for arbitrary dl-programs. These results\nreinforce the argument that default logic can serve as a fruitful foundation\nfor query-based approaches to integrating ontology and rules. With its simple\nsyntax and intuitive semantics, plus available computational results, default\nlogic can be considered an attractive approach to integration of ontology and\nrules.\n"
  },
  {
    "id": "1111.1941",
    "title": "Semantic-Driven e-Government: Application of Uschold and King Ontology\n  Building Methodology for Semantic Ontology Models Development",
    "abstract": "  Electronic government (e-government) has been one of the most active areas of\nontology development during the past six years. In e-government, ontologies are\nbeing used to describe and specify e-government services (e-services) because\nthey enable easy composition, matching, mapping and merging of various\ne-government services. More importantly, they also facilitate the semantic\nintegration and interoperability of e-government services. However, it is still\nunclear in the current literature how an existing ontology building methodology\ncan be applied to develop semantic ontology models in a government service\ndomain. In this paper the Uschold and King ontology building methodology is\napplied to develop semantic ontology models in a government service domain.\nFirstly, the Uschold and King methodology is presented, discussed and applied\nto build a government domain ontology. Secondly, the domain ontology is\nevaluated for semantic consistency using its semi-formal representation in\nDescription Logic. Thirdly, an alignment of the domain ontology with the\nDescriptive Ontology for Linguistic and Cognitive Engineering (DOLCE) upper\nlevel ontology is drawn to allow its wider visibility and facilitate its\nintegration with existing metadata standard. Finally, the domain ontology is\nformally written in Web Ontology Language (OWL) to enable its automatic\nprocessing by computers. The study aims to provide direction for the\napplication of existing ontology building methodologies in the Semantic Web\ndevelopment processes of e-government domain specific ontology models; which\nwould enable their repeatability in other e-government projects and strengthen\nthe adoption of semantic technologies in e-government.\n"
  },
  {
    "id": "1111.2249",
    "title": "SATzilla: Portfolio-based Algorithm Selection for SAT",
    "abstract": "  It has been widely observed that there is no single \"dominant\" SAT solver;\ninstead, different solvers perform best on different instances. Rather than\nfollowing the traditional approach of choosing the best solver for a given\nclass of instances, we advocate making this decision online on a per-instance\nbasis. Building on previous work, we describe SATzilla, an automated approach\nfor constructing per-instance algorithm portfolios for SAT that use so-called\nempirical hardness models to choose among their constituent solvers. This\napproach takes as input a distribution of problem instances and a set of\ncomponent solvers, and constructs a portfolio optimizing a given objective\nfunction (such as mean runtime, percent of instances solved, or score in a\ncompetition). The excellent performance of SATzilla was independently verified\nin the 2007 SAT Competition, where our SATzilla07 solvers won three gold, one\nsilver and one bronze medal. In this article, we go well beyond SATzilla07 by\nmaking the portfolio construction scalable and completely automated, and\nimproving it by integrating local search solvers as candidate solvers, by\npredicting performance score instead of runtime, and by using hierarchical\nhardness models that take into account different types of SAT instances. We\ndemonstrate the effectiveness of these new techniques in extensive experimental\nresults on data sets including instances from the most recent SAT competition.\n"
  },
  {
    "id": "1111.2763",
    "title": "8-Valent Fuzzy Logic for Iris Recognition and Biometry",
    "abstract": "  This paper shows that maintaining logical consistency of an iris recognition\nsystem is a matter of finding a suitable partitioning of the input space in\nenrollable and unenrollable pairs by negotiating the user comfort and the\nsafety of the biometric system. In other words, consistent enrollment is\nmandatory in order to preserve system consistency. A fuzzy 3-valued\ndisambiguated model of iris recognition is proposed and analyzed in terms of\ncompleteness, consistency, user comfort and biometric safety. It is also shown\nhere that the fuzzy 3-valued model of iris recognition is hosted by an 8-valued\nBoolean algebra of modulo 8 integers that represents the computational\nformalization in which a biometric system (a software agent) can achieve the\nartificial understanding of iris recognition in a logically consistent manner.\n"
  },
  {
    "id": "1111.3690",
    "title": "New Candidates Welcome! Possible Winners with respect to the Addition of\n  New Candidates",
    "abstract": "  In voting contexts, some new candidates may show up in the course of the\nprocess. In this case, we may want to determine which of the initial candidates\nare possible winners, given that a fixed number $k$ of new candidates will be\nadded. We give a computational study of this problem, focusing on scoring\nrules, and we provide a formal comparison with related problems such as control\nvia adding candidates or cloning.\n"
  },
  {
    "id": "1111.3934",
    "title": "Model-based Utility Functions",
    "abstract": "  Orseau and Ring, as well as Dewey, have recently described problems,\nincluding self-delusion, with the behavior of agents using various definitions\nof utility functions. An agent's utility function is defined in terms of the\nagent's history of interactions with its environment. This paper argues, via\ntwo examples, that the behavior problems can be avoided by formulating the\nutility function in two steps: 1) inferring a model of the environment from\ninteractions, and 2) computing utility as a function of the environment model.\nBasing a utility function on a model that the agent must learn implies that the\nutility function must initially be expressed in terms of specifications to be\nmatched to structures in the learned model. These specifications constitute\nprior assumptions about the environment so this approach will not work with\narbitrary environments. But the approach should work for agents designed by\nhumans to act in the physical world. The paper also addresses the issue of\nself-modifying agents and shows that if provided with the possibility to modify\ntheir utility functions agents will not choose to do so, under some usual\nassumptions.\n"
  },
  {
    "id": "1111.4083",
    "title": "Unbiased Statistics of a CSP - A Controlled-Bias Generator",
    "abstract": "  We show that estimating the complexity (mean and distribution) of the\ninstances of a fixed size Constraint Satisfaction Problem (CSP) can be very\nhard. We deal with the main two aspects of the problem: defining a measure of\ncomplexity and generating random unbiased instances. For the first problem, we\nrely on a general framework and a measure of complexity we presented at\nCISSE08. For the generation problem, we restrict our analysis to the Sudoku\nexample and we provide a solution that also explains why it is so difficult.\n"
  },
  {
    "id": "1111.4232",
    "title": "A Model of Spatial Thinking for Computational Intelligence",
    "abstract": "  Trying to be effective (no matter who exactly and in what field) a person\nface the problem which inevitably destroys all our attempts to easily get to a\ndesired goal. The problem is the existence of some insuperable barriers for our\nmind, anotherwords barriers for principles of thinking. They are our clue and\nmain reason for research. Here we investigate these barriers and their features\nexposing the nature of mental process. We start from special structures which\nreflect the ways to define relations between objects. Then we came to realizing\nabout what is the material our mind uses to build thoughts, to make\nconclusions, to understand, to form reasoning, etc. This can be called a mental\ndynamics. After this the nature of mental barriers on the required level of\nabstraction as well as the ways to pass through them became clear. We begin to\nunderstand why thinking flows in such a way, with such specifics and with such\nlimitations we can observe in reality. This can help us to be more optimal. At\nthe final step we start to understand, what ma-thematical models can be applied\nto such a picture. We start to express our thoughts in a language of\nmathematics, developing an apparatus for our Spatial Theory of Mind, suitable\nto represent processes and infrastructure of thinking. We use abstract algebra\nand stay invariant in relation to the nature of objects.\n"
  },
  {
    "id": "1111.5689",
    "title": "Revisiting Numerical Pattern Mining with Formal Concept Analysis",
    "abstract": "  In this paper, we investigate the problem of mining numerical data in the\nframework of Formal Concept Analysis. The usual way is to use a scaling\nprocedure --transforming numerical attributes into binary ones-- leading either\nto a loss of information or of efficiency, in particular w.r.t. the volume of\nextracted patterns. By contrast, we propose to directly work on numerical data\nin a more precise and efficient way, and we prove it. For that, the notions of\nclosed patterns, generators and equivalent classes are revisited in the\nnumerical context. Moreover, two original algorithms are proposed and used in\nan evaluation involving real-world data, showing the predominance of the\npresent approach.\n"
  },
  {
    "id": "1111.6117",
    "title": "Principles of Solomonoff Induction and AIXI",
    "abstract": "  We identify principles characterizing Solomonoff Induction by demands on an\nagent's external behaviour. Key concepts are rationality, computability,\nindifference and time consistency. Furthermore, we discuss extensions to the\nfull AI case to derive AIXI.\n"
  },
  {
    "id": "1111.6191",
    "title": "Pattern-Based Classification: A Unifying Perspective",
    "abstract": "  The use of patterns in predictive models is a topic that has received a lot\nof attention in recent years. Pattern mining can help to obtain models for\nstructured domains, such as graphs and sequences, and has been proposed as a\nmeans to obtain more accurate and more interpretable models. Despite the large\namount of publications devoted to this topic, we believe however that an\noverview of what has been accomplished in this area is missing. This paper\npresents our perspective on this evolving area. We identify the principles of\npattern mining that are important when mining patterns for models and provide\nan overview of pattern-based classification methods. We categorize these\nmethods along the following dimensions: (1) whether they post-process a\npre-computed set of patterns or iteratively execute pattern mining algorithms;\n(2) whether they select patterns model-independently or whether the pattern\nselection is guided by a model. We summarize the results that have been\nobtained for each of these methods.\n"
  },
  {
    "id": "1111.6401",
    "title": "Graph based E-Government web service composition",
    "abstract": "  Nowadays, e-government has emerged as a government policy to improve the\nquality and efficiency of public administrations. By exploiting the potential\nof new information and communication technologies, government agencies are\nproviding a wide spectrum of online services. These services are composed of\nseveral web services that comply with well defined processes. One of the big\nchallenges is the need to optimize the composition of the elementary web\nservices. In this paper, we present a solution for optimizing the computation\neffort in web service composition. Our method is based on Graph Theory. We\nmodel the semantic relationship between the involved web services through a\ndirected graph. Then, we compute all shortest paths using for the first time,\nan extended version of the Floyd-Warshall algorithm.\n"
  },
  {
    "id": "1111.6713",
    "title": "An Enhanced Indexing And Ranking Technique On The Semantic Web",
    "abstract": "  With the fast growth of the Internet, more and more information is available\non the Web. The Semantic Web has many features which cannot be handled by using\nthe traditional search engines. It extracts metadata for each discovered Web\ndocuments in RDF or OWL formats, and computes relations between documents. We\nproposed a hybrid indexing and ranking technique for the Semantic Web which\nfinds relevant documents and computes the similarity among a set of documents.\nFirst, it returns with the most related document from the repository of\nSemantic Web Documents (SWDs) by using a modified version of the ObjectRank\ntechnique. Then, it creates a sub-graph for the most related SWDs. Finally, It\nreturns the hubs and authorities of these document by using the HITS algorithm.\nOur technique increases the quality of the results and decreases the execution\ntime of processing the user's query.\n"
  },
  {
    "id": "1111.6790",
    "title": "Constraining the Size Growth of the Task Space with Socially Guided\n  Intrinsic Motivation using Demonstrations",
    "abstract": "  This paper presents an algorithm for learning a highly redundant inverse\nmodel in continuous and non-preset environments. Our Socially Guided Intrinsic\nMotivation by Demonstrations (SGIM-D) algorithm combines the advantages of both\nsocial learning and intrinsic motivation, to specialise in a wide range of\nskills, while lessening its dependence on the teacher. SGIM-D is evaluated on a\nfishing skill learning experiment.\n"
  },
  {
    "id": "1112.0508",
    "title": "Label Ranking with Abstention: Predicting Partial Orders by Thresholding\n  Probability Distributions (Extended Abstract)",
    "abstract": "  We consider an extension of the setting of label ranking, in which the\nlearner is allowed to make predictions in the form of partial instead of total\norders. Predictions of that kind are interpreted as a partial abstention: If\nthe learner is not sufficiently certain regarding the relative order of two\nalternatives, it may abstain from this decision and instead declare these\nalternatives as being incomparable. We propose a new method for learning to\npredict partial orders that improves on an existing approach, both\ntheoretically and empirically. Our method is based on the idea of thresholding\nthe probabilities of pairwise preferences between labels as induced by a\npredicted (parameterized) probability distribution on the set of all rankings.\n"
  },
  {
    "id": "1112.1489",
    "title": "Multi-granular Perspectives on Covering",
    "abstract": "  Covering model provides a general framework for granular computing in that\noverlapping among granules are almost indispensable. For any given covering,\nboth intersection and union of covering blocks containing an element are\nexploited as granules to form granular worlds at different abstraction levels,\nrespectively, and transformations among these different granular worlds are\nalso discussed. As an application of the presented multi-granular perspective\non covering, relational interpretation and axiomization of four types of\ncovering based rough upper approximation operators are investigated, which can\nbe dually applied to lower ones.\n"
  },
  {
    "id": "1112.2113",
    "title": "Incremental Slow Feature Analysis: Adaptive and Episodic Learning from\n  High-Dimensional Input Streams",
    "abstract": "  Slow Feature Analysis (SFA) extracts features representing the underlying\ncauses of changes within a temporally coherent high-dimensional raw sensory\ninput signal. Our novel incremental version of SFA (IncSFA) combines\nincremental Principal Components Analysis and Minor Components Analysis. Unlike\nstandard batch-based SFA, IncSFA adapts along with non-stationary environments,\nis amenable to episodic training, is not corrupted by outliers, and is\ncovariance-free. These properties make IncSFA a generally useful unsupervised\npreprocessor for autonomous learning agents and robots. In IncSFA, the CCIPCA\nand MCA updates take the form of Hebbian and anti-Hebbian updating, extending\nthe biological plausibility of SFA. In both single node and deep network\nversions, IncSFA learns to encode its input streams (such as high-dimensional\nvideo) by informative slow features representing meaningful abstract\nenvironmental properties. It can handle cases where batch SFA fails.\n"
  },
  {
    "id": "1112.2640",
    "title": "Threshold Choice Methods: the Missing Link",
    "abstract": "  Many performance metrics have been introduced for the evaluation of\nclassification performance, with different origins and niches of application:\naccuracy, macro-accuracy, area under the ROC curve, the ROC convex hull, the\nabsolute error, and the Brier score (with its decomposition into refinement and\ncalibration). One way of understanding the relation among some of these metrics\nis the use of variable operating conditions (either in the form of\nmisclassification costs or class proportions). Thus, a metric may correspond to\nsome expected loss over a range of operating conditions. One dimension for the\nanalysis has been precisely the distribution we take for this range of\noperating conditions, leading to some important connections in the area of\nproper scoring rules. However, we show that there is another dimension which\nhas not received attention in the analysis of performance metrics. This new\ndimension is given by the decision rule, which is typically implemented as a\nthreshold choice method when using scoring models. In this paper, we explore\nmany old and new threshold choice methods: fixed, score-uniform, score-driven,\nrate-driven and optimal, among others. By calculating the loss of these methods\nfor a uniform range of operating conditions we get the 0-1 loss, the absolute\nerror, the Brier score (mean squared error), the AUC and the refinement loss\nrespectively. This provides a comprehensive view of performance metrics as well\nas a systematic approach to loss minimisation, namely: take a model, apply\nseveral threshold choice methods consistent with the information which is (and\nwill be) available about the operating condition, and compare their expected\nlosses. In order to assist in this procedure we also derive several connections\nbetween the aforementioned performance metrics, and we highlight the role of\ncalibration in choosing the threshold choice method.\n"
  },
  {
    "id": "1112.2681",
    "title": "Inference in Probabilistic Logic Programs with Continuous Random\n  Variables",
    "abstract": "  Probabilistic Logic Programming (PLP), exemplified by Sato and Kameya's\nPRISM, Poole's ICL, Raedt et al's ProbLog and Vennekens et al's LPAD, is aimed\nat combining statistical and logical knowledge representation and inference. A\nkey characteristic of PLP frameworks is that they are conservative extensions\nto non-probabilistic logic programs which have been widely used for knowledge\nrepresentation. PLP frameworks extend traditional logic programming semantics\nto a distribution semantics, where the semantics of a probabilistic logic\nprogram is given in terms of a distribution over possible models of the\nprogram. However, the inference techniques used in these works rely on\nenumerating sets of explanations for a query answer. Consequently, these\nlanguages permit very limited use of random variables with continuous\ndistributions. In this paper, we present a symbolic inference procedure that\nuses constraints and represents sets of explanations without enumeration. This\npermits us to reason over PLPs with Gaussian or Gamma-distributed random\nvariables (in addition to discrete-valued random variables) and linear equality\nconstraints over reals. We develop the inference procedure in the context of\nPRISM; however the procedure's core ideas can be easily applied to other PLP\nlanguages as well. An interesting aspect of our inference procedure is that\nPRISM's query evaluation process becomes a special case in the absence of any\ncontinuous random variables in the program. The symbolic inference procedure\nenables us to reason over complex probabilistic models such as Kalman filters\nand a large subclass of Hybrid Bayesian networks that were hitherto not\npossible in PLP frameworks. (To appear in Theory and Practice of Logic\nProgramming).\n"
  },
  {
    "id": "1112.5381",
    "title": "Improving the Efficiency of Approximate Inference for Probabilistic\n  Logical Models by means of Program Specialization",
    "abstract": "  We consider the task of performing probabilistic inference with probabilistic\nlogical models. Many algorithms for approximate inference with such models are\nbased on sampling. From a logic programming perspective, sampling boils down to\nrepeatedly calling the same queries on a knowledge base composed of a static\npart and a dynamic part. The larger the static part, the more redundancy there\nis in these repeated calls. This is problematic since inefficient sampling\nyields poor approximations.\n  We show how to apply logic program specialization to make sampling-based\ninference more efficient. We develop an algorithm that specializes the\ndefinitions of the query predicates with respect to the static part of the\nknowledge base. In experiments on real-world data we obtain speedups of up to\nan order of magnitude, and these speedups grow with the data-size.\n"
  },
  {
    "id": "1201.0414",
    "title": "Continuity in Information Algebras",
    "abstract": "  In this paper, the continuity and strong continuity in domain-free\ninformation algebras and labeled information algebras are introduced\nrespectively. A more general concept of continuous function which is defined\nbetween two domain-free continuous information algebras is presented. It is\nshown that, with the operations combination and focusing, the set of all\ncontinuous functions between two domain-free s-continuous information algebras\nforms a new s-continuous information algebra. By studying the relationship\nbetween domain-free information algebras and labeled information algebras, it\nis demonstrated that they do correspond to each other on s-compactness.\n"
  },
  {
    "id": "1201.0564",
    "title": "The RegularGcc Matrix Constraint",
    "abstract": "  We study propagation of the RegularGcc global constraint. This ensures that\neach row of a matrix of decision variables satisfies a Regular constraint, and\neach column satisfies a Gcc constraint. On the negative side, we prove that\npropagation is NP-hard even under some strong restrictions (e.g. just 3 values,\njust 4 states in the automaton, or just 5 columns to the matrix). On the\npositive side, we identify two cases where propagation is fixed parameter\ntractable. In addition, we show how to improve propagation over a simple\ndecomposition into separate Regular and Gcc constraints by identifying some\nnecessary but insufficient conditions for a solution. We enforce these\nconditions with some additional weighted row automata. Experimental results\ndemonstrate the potential of these methods on some standard benchmark problems.\n"
  },
  {
    "id": "1201.2004",
    "title": "Optimal Fuzzy Model Construction with Statistical Information using\n  Genetic Algorithm",
    "abstract": "  Fuzzy rule based models have a capability to approximate any continuous\nfunction to any degree of accuracy on a compact domain. The majority of FLC\ndesign process relies on heuristic knowledge of experience operators. In order\nto make the design process automatic we present a genetic approach to learn\nfuzzy rules as well as membership function parameters. Moreover, several\nstatistical information criteria such as the Akaike information criterion\n(AIC), the Bhansali-Downham information criterion (BDIC), and the\nSchwarz-Rissanen information criterion (SRIC) are used to construct optimal\nfuzzy models by reducing fuzzy rules. A genetic scheme is used to design\nTakagi-Sugeno-Kang (TSK) model for identification of the antecedent rule\nparameters and the identification of the consequent parameters. Computer\nsimulations are presented confirming the performance of the constructed fuzzy\nlogic controller.\n"
  },
  {
    "id": "1201.2711",
    "title": "Ultrametric Model of Mind, I: Review",
    "abstract": "  We mathematically model Ignacio Matte Blanco's principles of symmetric and\nasymmetric being through use of an ultrametric topology. We use for this the\nhighly regarded 1975 book of this Chilean psychiatrist and pyschoanalyst (born\n1908, died 1995). Such an ultrametric model corresponds to hierarchical\nclustering in the empirical data, e.g. text. We show how an ultrametric\ntopology can be used as a mathematical model for the structure of the logic\nthat reflects or expresses Matte Blanco's symmetric being, and hence of the\nreasoning and thought processes involved in conscious reasoning or in reasoning\nthat is lacking, perhaps entirely, in consciousness or awareness of itself. In\na companion paper we study how symmetric (in the sense of Matte Blanco's)\nreasoning can be demarcated in a context of symmetric and asymmetric reasoning\nprovided by narrative text.\n"
  },
  {
    "id": "1201.3107",
    "title": "Tacit knowledge mining algorithm based on linguistic truth-valued\n  concept lattice",
    "abstract": "  This paper is the continuation of our research work about linguistic\ntruth-valued concept lattice. In order to provide a mathematical tool for\nmining tacit knowledge, we establish a concrete model of 6-ary linguistic\ntruth-valued concept lattice and introduce a mining algorithm through the\nstructure consistency. Specifically, we utilize the attributes to depict\nknowledge, propose the 6-ary linguistic truth-valued attribute extended context\nand congener context to characterize tacit knowledge, and research the\nnecessary and sufficient conditions of forming tacit knowledge. We respectively\ngive the algorithms of generating the linguistic truth-valued congener context\nand constructing the linguistic truth-valued concept lattice.\n"
  },
  {
    "id": "1201.3204",
    "title": "Evaluation of a Simple, Scalable, Parallel Best-First Search Strategy",
    "abstract": "  Large-scale, parallel clusters composed of commodity processors are\nincreasingly available, enabling the use of vast processing capabilities and\ndistributed RAM to solve hard search problems. We investigate Hash-Distributed\nA* (HDA*), a simple approach to parallel best-first search that asynchronously\ndistributes and schedules work among processors based on a hash function of the\nsearch state. We use this approach to parallelize the A* algorithm in an\noptimal sequential version of the Fast Downward planner, as well as a 24-puzzle\nsolver. The scaling behavior of HDA* is evaluated experimentally on a shared\nmemory, multicore machine with 8 cores, a cluster of commodity machines using\nup to 64 cores, and large-scale high-performance clusters, using up to 2400\nprocessors. We show that this approach scales well, allowing the effective\nutilization of large amounts of distributed memory to optimally solve problems\nwhich require terabytes of RAM. We also compare HDA* to Transposition-table\nDriven Scheduling (TDS), a hash-based parallelization of IDA*, and show that,\nin planning, HDA* significantly outperforms TDS. A simple hybrid which combines\nHDA* and TDS to exploit strengths of both algorithms is proposed and evaluated.\n"
  },
  {
    "id": "1201.3408",
    "title": "The computation of first order moments on junction trees",
    "abstract": "  We review some existing methods for the computation of first order moments on\njunction trees using Shafer-Shenoy algorithm. First, we consider the problem of\nfirst order moments computation as vertices problem in junction trees. In this\nway, the problem is solved using the memory space of an order of the junction\ntree edge-set cardinality. After that, we consider two algorithms,\nLauritzen-Nilsson algorithm, and Mau\\'a et al. algorithm, which computes the\nfirst order moments as the normalization problem in junction tree, using the\nmemory space of an order of the junction tree leaf-set cardinality.\n"
  },
  {
    "id": "1201.4080",
    "title": "Progress in animation of an EMA-controlled tongue model for\n  acoustic-visual speech synthesis",
    "abstract": "  We present a technique for the animation of a 3D kinematic tongue model, one\ncomponent of the talking head of an acoustic-visual (AV) speech synthesizer.\nThe skeletal animation approach is adapted to make use of a deformable rig\ncontrolled by tongue motion capture data obtained with electromagnetic\narticulography (EMA), while the tongue surface is extracted from volumetric\nmagnetic resonance imaging (MRI) data. Initial results are shown and future\nwork outlined.\n"
  },
  {
    "id": "1201.5426",
    "title": "Constraint Propagation as Information Maximization",
    "abstract": "  This paper draws on diverse areas of computer science to develop a unified\nview of computation:\n  (1) Optimization in operations research, where a numerical objective function\nis maximized under constraints, is generalized from the numerical total order\nto a non-numerical partial order that can be interpreted in terms of\ninformation. (2) Relations are generalized so that there are relations of which\nthe constituent tuples have numerical indexes, whereas in other relations these\nindexes are variables. The distinction is essential in our definition of\nconstraint satisfaction problems. (3) Constraint satisfaction problems are\nformulated in terms of semantics of conjunctions of atomic formulas of\npredicate logic. (4) Approximation structures, which are available for several\nimportant domains, are applied to solutions of constraint satisfaction\nproblems.\n  As application we treat constraint satisfaction problems over reals. These\ncover a large part of numerical analysis, most significantly nonlinear\nequations and inequalities. The chaotic algorithm analyzed in the paper\ncombines the efficiency of floating-point computation with the correctness\nguarantees of arising from our logico-mathematical model of\nconstraint-satisfaction problems.\n"
  },
  {
    "id": "1201.5472",
    "title": "A multiagent urban traffic simulation",
    "abstract": "  We built a multiagent simulation of urban traffic to model both ordinary\ntraffic and emergency or crisis mode traffic. This simulation first builds a\nmodeled road network based on detailed geographical information. On this\nnetwork, the simulation creates two populations of agents: the Transporters and\nthe Mobiles. Transporters embody the roads themselves; they are utilitarian and\nmeant to handle the low level realism of the simulation. Mobile agents embody\nthe vehicles that circulate on the network. They have one or several\ndestinations they try to reach using initially their beliefs of the structure\nof the network (length of the edges, speed limits, number of lanes etc.).\nNonetheless, when confronted to a dynamic, emergent prone environment (other\nvehicles, unexpectedly closed ways or lanes, traffic jams etc.), the rather\nreactive agent will activate more cognitive modules to adapt its beliefs,\ndesires and intentions. It may change its destination(s), change the tactics\nused to reach the destination (favoring less used roads, following other\nagents, using general headings), etc. We describe our current validation of our\nmodel and the next planned improvements, both in validation and in\nfunctionalities.\n"
  },
  {
    "id": "1201.5841",
    "title": "The thermodynamic cost of fast thought",
    "abstract": "  After more than sixty years, Shannon's research [1-3] continues to raise\nfundamental questions, such as the one formulated by Luce [4,5], which is still\nunanswered: \"Why is information theory not very applicable to psychological\nproblems, despite apparent similarities of concepts?\" On this topic, Pinker\n[6], one of the foremost defenders of the computational theory of mind [6], has\nargued that thought is simply a type of computation, and that the gap between\nhuman cognition and computational models may be illusory. In this context, in\nhis latest book, titled Thinking Fast and Slow [8], Kahneman [7,8] provides\nfurther theoretical interpretation by differentiating the two assumed systems\nof the cognitive functioning of the human mind. He calls them intuition (system\n1) determined to be an associative (automatic, fast and perceptual) machine,\nand reasoning (system 2) required to be voluntary and to operate logical-\ndeductively. In this paper, we propose an ansatz inspired by Ausubel's learning\ntheory for investigating, from the constructivist perspective [9-12],\ninformation processing in the working memory of cognizers. Specifically, a\nthought experiment is performed utilizing the mind of a dual-natured creature\nknown as Maxwell's demon: a tiny \"man-machine\" solely equipped with the\ncharacteristics of system 1, which prevents it from reasoning. The calculation\npresented here shows that [...]. This result indicates that when the system 2\nis shut down, both an intelligent being, as well as a binary machine, incur the\nsame energy cost per unit of information processed, which mathematically proves\nthe computational attribute of the system 1, as Kahneman [7,8] theorized. This\nfinding links information theory to human psychological features and opens a\nnew path toward the conception of a multi-bit reasoning machine.\n"
  },
  {
    "id": "1201.6511",
    "title": "Ontologies for the Integration of Air Quality Models and 3D City Models",
    "abstract": "  The holistic approach to sustainable urban planning implies using different\nmodels in an integrated way that is capable of simulating the urban system. As\nthe interconnection of such models is not a trivial task, one of the key\nelements that may be applied is the description of the urban geometric\nproperties in an \"interoperable\" way. Focusing on air quality as one of the\nmost pronounced urban problems, the geometric aspects of a city may be\ndescribed by objects such as those defined in CityGML, so that an appropriate\nair quality model can be applied for estimating the quality of the urban air on\nthe basis of atmospheric flow and chemistry equations.\n  In this paper we first present theoretical background and motivations for the\ninterconnection of 3D city models and other models related to sustainable\ndevelopment and urban planning. Then we present a practical experiment based on\nthe interconnection of CityGML with an air quality model. Our approach is based\non the creation of an ontology of air quality models and on the extension of an\nontology of urban planning process (OUPP) that acts as an ontology mediator.\n"
  },
  {
    "id": "1202.0440",
    "title": "The implications of embodiment for behavior and cognition: animal and\n  robotic case studies",
    "abstract": "  In this paper, we will argue that if we want to understand the function of\nthe brain (or the control in the case of robots), we must understand how the\nbrain is embedded into the physical system, and how the organism interacts with\nthe real world. While embodiment has often been used in its trivial meaning,\ni.e. 'intelligence requires a body', the concept has deeper and more important\nimplications, concerned with the relation between physical and information\n(neural, control) processes. A number of case studies are presented to\nillustrate the concept. These involve animals and robots and are concentrated\naround locomotion, grasping, and visual perception. A theoretical scheme that\ncan be used to embed the diverse case studies will be presented. Finally, we\nwill establish a link between the low-level sensory-motor processes and\ncognition. We will present an embodied view on categorization, and propose the\nconcepts of 'body schema' and 'forward models' as a natural extension of the\nembodied approach toward first representations.\n"
  },
  {
    "id": "1202.0837",
    "title": "On the influence of intelligence in (social) intelligence testing\n  environments",
    "abstract": "  This paper analyses the influence of including agents of different degrees of\nintelligence in a multiagent system. The goal is to better understand how we\ncan develop intelligence tests that can evaluate social intelligence. We\nanalyse several reinforcement algorithms in several contexts of cooperation and\ncompetition. Our experimental setting is inspired by the recently developed\nDarwin-Wallace distribution.\n"
  },
  {
    "id": "1202.1886",
    "title": "Classification of artificial intelligence ids for smurf attack",
    "abstract": "  Many methods have been developed to secure the network infrastructure and\ncommunication over the Internet. Intrusion detection is a relatively new\naddition to such techniques. Intrusion detection systems (IDS) are used to find\nout if someone has intrusion into or is trying to get it the network. One big\nproblem is amount of Intrusion which is increasing day by day. We need to know\nabout network attack information using IDS, then analysing the effect. Due to\nthe nature of IDSs which are solely signature based, every new intrusion cannot\nbe detected; so it is important to introduce artificial intelligence (AI)\nmethods / techniques in IDS. Introduction of AI necessitates the importance of\nnormalization in intrusions. This work is focused on classification of AI based\nIDS techniques which will help better design intrusion detection systems in the\nfuture. We have also proposed a support vector machine for IDS to detect Smurf\nattack with much reliable accuracy.\n"
  },
  {
    "id": "1202.1891",
    "title": "Hyper heuristic based on great deluge and its variants for exam\n  timetabling problem",
    "abstract": "  Today, University Timetabling problems are occurred annually and they are\noften hard and time consuming to solve. This paper describes Hyper Heuristics\n(HH) method based on Great Deluge (GD) and its variants for solving large,\nhighly constrained timetabling problems from different domains. Generally, in\nhyper heuristic framework, there are two main stages: heuristic selection and\nmove acceptance. This paper emphasizes on the latter stage to develop Hyper\nHeuristic (HH) framework. The main contribution of this paper is that Great\nDeluge (GD) and its variants: Flex Deluge(FD), Non-linear(NLGD), Extended Great\nDeluge(EGD) are used as move acceptance method in HH by combining Reinforcement\nlearning (RL).These HH methods are tested on exam benchmark timetabling problem\nand best results and comparison analysis are reported.\n"
  },
  {
    "id": "1202.1945",
    "title": "A framework: Cluster detection and multidimensional visualization of\n  automated data mining using intelligent agents",
    "abstract": "  Data Mining techniques plays a vital role like extraction of required\nknowledge, finding unsuspected information to make strategic decision in a\nnovel way which in term understandable by domain experts. A generalized frame\nwork is proposed by considering non - domain experts during mining process for\nbetter understanding, making better decision and better finding new patters in\ncase of selecting suitable data mining techniques based on the user profile by\nmeans of intelligent agents. KEYWORDS: Data Mining Techniques, Intelligent\nAgents, User Profile, Multidimensional Visualization, Knowledge Discovery.\n"
  },
  {
    "id": "1202.3698",
    "title": "Extended Lifted Inference with Joint Formulas",
    "abstract": "  The First-Order Variable Elimination (FOVE) algorithm allows exact inference\nto be applied directly to probabilistic relational models, and has proven to be\nvastly superior to the application of standard inference methods on a grounded\npropositional model. Still, FOVE operators can be applied under restricted\nconditions, often forcing one to resort to propositional inference. This paper\naims to extend the applicability of FOVE by providing two new model conversion\noperators: the first and the primary is joint formula conversion and the second\nis just-different counting conversion. These new operations allow efficient\ninference methods to be applied directly on relational models, where no\nexisting efficient method could be applied hitherto. In addition, aided by\nthese capabilities, we show how to adapt FOVE to provide exact solutions to\nMaximum Expected Utility (MEU) queries over relational models for decision\nunder uncertainty. Experimental evaluations show our algorithms to provide\nsignificant speedup over the alternatives.\n"
  },
  {
    "id": "1202.3699",
    "title": "Learning is planning: near Bayes-optimal reinforcement learning via\n  Monte-Carlo tree search",
    "abstract": "  Bayes-optimal behavior, while well-defined, is often difficult to achieve.\nRecent advances in the use of Monte-Carlo tree search (MCTS) have shown that it\nis possible to act near-optimally in Markov Decision Processes (MDPs) with very\nlarge or infinite state spaces. Bayes-optimal behavior in an unknown MDP is\nequivalent to optimal behavior in the known belief-space MDP, although the size\nof this belief-space MDP grows exponentially with the amount of history\nretained, and is potentially infinite. We show how an agent can use one\nparticular MCTS algorithm, Forward Search Sparse Sampling (FSSS), in an\nefficient way to act nearly Bayes-optimally for all but a polynomial number of\nsteps, assuming that FSSS can be used to act efficiently in any possible\nunderlying MDP.\n"
  },
  {
    "id": "1202.3707",
    "title": "A temporally abstracted Viterbi algorithm",
    "abstract": "  Hierarchical problem abstraction, when applicable, may offer exponential\nreductions in computational complexity. Previous work on coarse-to-fine dynamic\nprogramming (CFDP) has demonstrated this possibility using state abstraction to\nspeed up the Viterbi algorithm. In this paper, we show how to apply temporal\nabstraction to the Viterbi problem. Our algorithm uses bounds derived from\nanalysis of coarse timescales to prune large parts of the state trellis at\nfiner timescales. We demonstrate improvements of several orders of magnitude\nover the standard Viterbi algorithm, as well as significant speedups over CFDP,\nfor problems whose state variables evolve at widely differing rates.\n"
  },
  {
    "id": "1202.3709",
    "title": "EDML: A Method for Learning Parameters in Bayesian Networks",
    "abstract": "  We propose a method called EDML for learning MAP parameters in binary\nBayesian networks under incomplete data. The method assumes Beta priors and can\nbe used to learn maximum likelihood parameters when the priors are\nuninformative. EDML exhibits interesting behaviors, especially when compared to\nEM. We introduce EDML, explain its origin, and study some of its properties\nboth analytically and empirically.\n"
  },
  {
    "id": "1202.3711",
    "title": "A Logical Characterization of Constraint-Based Causal Discovery",
    "abstract": "  We present a novel approach to constraint-based causal discovery, that takes\nthe form of straightforward logical inference, applied to a list of simple,\nlogical statements about causal relations that are derived directly from\nobserved (in)dependencies. It is both sound and complete, in the sense that all\ninvariant features of the corresponding partial ancestral graph (PAG) are\nidentified, even in the presence of latent variables and selection bias. The\napproach shows that every identifiable causal relation corresponds to one of\njust two fundamental forms. More importantly, as the basic building blocks of\nthe method do not rely on the detailed (graphical) structure of the\ncorresponding PAG, it opens up a range of new opportunities, including more\nrobust inference, detailed accountability, and application to large models.\n"
  },
  {
    "id": "1202.3713",
    "title": "Bayesian network learning with cutting planes",
    "abstract": "  The problem of learning the structure of Bayesian networks from complete\ndiscrete data with a limit on parent set size is considered. Learning is cast\nexplicitly as an optimisation problem where the goal is to find a BN structure\nwhich maximises log marginal likelihood (BDe score). Integer programming,\nspecifically the SCIP framework, is used to solve this optimisation problem.\nAcyclicity constraints are added to the integer program (IP) during solving in\nthe form of cutting planes. Finding good cutting planes is the key to the\nsuccess of the approach -the search for such cutting planes is effected using a\nsub-IP. Results show that this is a particularly fast method for exact BN\nlearning.\n"
  },
  {
    "id": "1202.3718",
    "title": "On the Complexity of Decision Making in Possibilistic Decision Trees",
    "abstract": "  When the information about uncertainty cannot be quantified in a simple,\nprobabilistic way, the topic of possibilistic decision theory is often a\nnatural one to consider. The development of possibilistic decision theory has\nlead to a series of possibilistic criteria, e.g pessimistic possibilistic\nqualitative utility, possibilistic likely dominance, binary possibilistic\nutility and possibilistic Choquet integrals. This paper focuses on sequential\ndecision making in possibilistic decision trees. It proposes a complexity study\nof the problem of finding an optimal strategy depending on the monotonicity\nproperty of the optimization criteria which allows the application of dynamic\nprogramming that offers a polytime reduction of the decision problem. It also\nshows that possibilistic Choquet integrals do not satisfy this property, and\nthat in this case the optimization problem is NP - hard.\n"
  },
  {
    "id": "1202.3719",
    "title": "Inference in Probabilistic Logic Programs using Weighted CNF's",
    "abstract": "  Probabilistic logic programs are logic programs in which some of the facts\nare annotated with probabilities. Several classical probabilistic inference\ntasks (such as MAP and computing marginals) have not yet received a lot of\nattention for this formalism. The contribution of this paper is that we develop\nefficient inference algorithms for these tasks. This is based on a conversion\nof the probabilistic logic program and the query and evidence to a weighted CNF\nformula. This allows us to reduce the inference tasks to well-studied tasks\nsuch as weighted model counting. To solve such tasks, we employ\nstate-of-the-art methods. We consider multiple methods for the conversion of\nthe programs as well as for inference on the weighted CNF. The resulting\napproach is evaluated experimentally and shown to improve upon the\nstate-of-the-art in probabilistic logic programming.\n"
  },
  {
    "id": "1202.3721",
    "title": "Dynamic consistency and decision making under vacuous belief",
    "abstract": "  The ideas about decision making under ignorance in economics are combined\nwith the ideas about uncertainty representation in computer science. The\ncombination sheds new light on the question of how artificial agents can act in\na dynamically consistent manner. The notion of sequential consistency is\nformalized by adapting the law of iterated expectation for plausibility\nmeasures. The necessary and sufficient condition for a certainty equivalence\noperator for Nehring-Puppe's preference to be sequentially consistent is given.\nThis result sheds light on the models of decision making under uncertainty.\n"
  },
  {
    "id": "1202.3723",
    "title": "Approximation by Quantization",
    "abstract": "  Inference in graphical models consists of repeatedly multiplying and summing\nout potentials. It is generally intractable because the derived potentials\nobtained in this way can be exponentially large. Approximate inference\ntechniques such as belief propagation and variational methods combat this by\nsimplifying the derived potentials, typically by dropping variables from them.\nWe propose an alternate method for simplifying potentials: quantizing their\nvalues. Quantization causes different states of a potential to have the same\nvalue, and therefore introduces context-specific independencies that can be\nexploited to represent the potential more compactly. We use algebraic decision\ndiagrams (ADDs) to do this efficiently. We apply quantization and ADD reduction\nto variable elimination and junction tree propagation, yielding a family of\nbounded approximate inference schemes. Our experimental tests show that our new\nschemes significantly outperform state-of-the-art approaches on many benchmark\ninstances.\n"
  },
  {
    "id": "1202.3724",
    "title": "Probabilistic Theorem Proving",
    "abstract": "  Many representation schemes combining first-order logic and probability have\nbeen proposed in recent years. Progress in unifying logical and probabilistic\ninference has been slower. Existing methods are mainly variants of lifted\nvariable elimination and belief propagation, neither of which take logical\nstructure into account. We propose the first method that has the full power of\nboth graphical model inference and first-order theorem proving (in finite\ndomains with Herbrand interpretations). We first define probabilistic theorem\nproving, their generalization, as the problem of computing the probability of a\nlogical formula given the probabilities or weights of a set of formulas. We\nthen show how this can be reduced to the problem of lifted weighted model\ncounting, and develop an efficient algorithm for the latter. We prove the\ncorrectness of this algorithm, investigate its properties, and show how it\ngeneralizes previous approaches. Experiments show that it greatly outperforms\nlifted variable elimination when logical structure is present. Finally, we\npropose an algorithm for approximate probabilistic theorem proving, and show\nthat it can greatly outperform lifted belief propagation.\n"
  },
  {
    "id": "1202.3728",
    "title": "Reasoning about RoboCup Soccer Narratives",
    "abstract": "  This paper presents an approach for learning to translate simple narratives,\ni.e., texts (sequences of sentences) describing dynamic systems, into coherent\nsequences of events without the need for labeled training data. Our approach\nincorporates domain knowledge in the form of preconditions and effects of\nevents, and we show that it outperforms state-of-the-art supervised learning\nsystems on the task of reconstructing RoboCup soccer games from their\ncommentaries.\n"
  },
  {
    "id": "1202.3729",
    "title": "Suboptimality Bounds for Stochastic Shortest Path Problems",
    "abstract": "  We consider how to use the Bellman residual of the dynamic programming\noperator to compute suboptimality bounds for solutions to stochastic shortest\npath problems. Such bounds have been previously established only in the special\ncase that \"all policies are proper,\" in which case the dynamic programming\noperator is known to be a contraction, and have been shown to be easily\ncomputable only in the more limited special case of discounting. Under the\ncondition that transition costs are positive, we show that suboptimality bounds\ncan be easily computed even when not all policies are proper. In the general\ncase when there are no restrictions on transition costs, the analysis is more\ncomplex. But we present preliminary results that show such bounds are possible.\n"
  },
  {
    "id": "1202.3740",
    "title": "An Efficient Protocol for Negotiation over Combinatorial Domains with\n  Incomplete Information",
    "abstract": "  We study the problem of agent-based negotiation in combinatorial domains. It\nis difficult to reach optimal agreements in bilateral or multi-lateral\nnegotiations when the agents' preferences for the possible alternatives are not\ncommon knowledge. Self-interested agents often end up negotiating inefficient\nagreements in such situations. In this paper, we present a protocol for\nnegotiation in combinatorial domains which can lead rational agents to reach\noptimal agreements under incomplete information setting. Our proposed protocol\nenables the negotiating agents to identify efficient solutions using\ndistributed search that visits only a small subspace of the whole outcome\nspace. Moreover, the proposed protocol is sufficiently general that it is\napplicable to most preference representation models in combinatorial domains.\nWe also present results of experiments that demonstrate the feasibility and\ncomputational efficiency of our approach.\n"
  },
  {
    "id": "1202.3741",
    "title": "Noisy Search with Comparative Feedback",
    "abstract": "  We present theoretical results in terms of lower and upper bounds on the\nquery complexity of noisy search with comparative feedback. In this search\nmodel, the noise in the feedback depends on the distance between query points\nand the search target. Consequently, the error probability in the feedback is\nnot fixed but varies for the queries posed by the search algorithm. Our results\nshow that a target out of n items can be found in O(log n) queries. We also\nshow the surprising result that for k possible answers per query, the speedup\nis not log k (as for k-ary search) but only log log k in some cases.\n"
  },
  {
    "id": "1202.3743",
    "title": "Belief change with noisy sensing in the situation calculus",
    "abstract": "  Situation calculus has been applied widely in artificial intelligence to\nmodel and reason about actions and changes in dynamic systems. Since actions\ncarried out by agents will cause constant changes of the agents' beliefs, how\nto manage these changes is a very important issue. Shapiro et al. [22] is one\nof the studies that considered this issue. However, in this framework, the\nproblem of noisy sensing, which often presents in real-world applications, is\nnot considered. As a consequence, noisy sensing actions in this framework will\nlead to an agent facing inconsistent situation and subsequently the agent\ncannot proceed further. In this paper, we investigate how noisy sensing actions\ncan be handled in iterated belief change within the situation calculus\nformalism. We extend the framework proposed in [22] with the capability of\nmanaging noisy sensings. We demonstrate that an agent can still detect the\nactual situation when the ratio of noisy sensing actions vs. accurate sensing\nactions is limited. We prove that our framework subsumes the iterated belief\nchange strategy in [22] when all sensing actions are accurate. Furthermore, we\nprove that our framework can adequately handle belief introspection, mistaken\nbeliefs, belief revision and belief update even with noisy sensing, as done in\n[22] with accurate sensing actions only.\n"
  },
  {
    "id": "1202.3744",
    "title": "Improving the Scalability of Optimal Bayesian Network Learning with\n  External-Memory Frontier Breadth-First Branch and Bound Search",
    "abstract": "  Previous work has shown that the problem of learning the optimal structure of\na Bayesian network can be formulated as a shortest path finding problem in a\ngraph and solved using A* search. In this paper, we improve the scalability of\nthis approach by developing a memory-efficient heuristic search algorithm for\nlearning the structure of a Bayesian network. Instead of using A*, we propose a\nfrontier breadth-first branch and bound search that leverages the layered\nstructure of the search graph of this problem so that no more than two layers\nof the graph, plus solution reconstruction information, need to be stored in\nmemory at a time. To further improve scalability, the algorithm stores most of\nthe graph in external memory, such as hard disk, when it does not fit in RAM.\nExperimental results show that the resulting algorithm solves significantly\nlarger problems than the current state of the art.\n"
  },
  {
    "id": "1202.3745",
    "title": "Order-of-Magnitude Influence Diagrams",
    "abstract": "  In this paper, we develop a qualitative theory of influence diagrams that can\nbe used to model and solve sequential decision making tasks when only\nqualitative (or imprecise) information is available. Our approach is based on\nan order-of-magnitude approximation of both probabilities and utilities and\nallows for specifying partially ordered preferences via sets of utility values.\nWe also propose a dedicated variable elimination algorithm that can be applied\nfor solving order-of-magnitude influence diagrams.\n"
  },
  {
    "id": "1202.3749",
    "title": "Compact Mathematical Programs For DEC-MDPs With Structured Agent\n  Interactions",
    "abstract": "  To deal with the prohibitive complexity of calculating policies in\nDecentralized MDPs, researchers have proposed models that exploit structured\nagent interactions. Settings where most agent actions are independent except\nfor few actions that affect the transitions and/or rewards of other agents can\nbe modeled using Event-Driven Interactions with Complex Rewards (EDI-CR).\nFinding the optimal joint policy can be formulated as an optimization problem.\nHowever, existing formulations are too verbose and/or lack optimality\nguarantees. We propose a compact Mixed Integer Linear Program formulation of\nEDI-CR instances. The key insight is that most action sequences of a group of\nagents have the same effect on a given agent. This allows us to treat these\nsequences similarly and use fewer variables. Experiments show that our\nformulation is more compact and leads to faster solution times and better\nsolutions than existing formulations.\n"
  },
  {
    "id": "1202.3754",
    "title": "A Geometric Traversal Algorithm for Reward-Uncertain MDPs",
    "abstract": "  Markov decision processes (MDPs) are widely used in modeling decision making\nproblems in stochastic environments. However, precise specification of the\nreward functions in MDPs is often very difficult. Recent approaches have\nfocused on computing an optimal policy based on the minimax regret criterion\nfor obtaining a robust policy under uncertainty in the reward function. One of\nthe core tasks in computing the minimax regret policy is to obtain the set of\nall policies that can be optimal for some candidate reward function. In this\npaper, we propose an efficient algorithm that exploits the geometric properties\nof the reward function associated with the policies. We also present an\napproximate version of the method for further speed up. We experimentally\ndemonstrate that our algorithm improves the performance by orders of magnitude.\n"
  },
  {
    "id": "1202.3759",
    "title": "Compressed Inference for Probabilistic Sequential Models",
    "abstract": "  Hidden Markov models (HMMs) and conditional random fields (CRFs) are two\npopular techniques for modeling sequential data. Inference algorithms designed\nover CRFs and HMMs allow estimation of the state sequence given the\nobservations. In several applications, estimation of the state sequence is not\nthe end goal; instead the goal is to compute some function of it. In such\nscenarios, estimating the state sequence by conventional inference techniques,\nfollowed by computing the functional mapping from the estimate is not\nnecessarily optimal. A more formal approach is to directly infer the final\noutcome from the observations. In particular, we consider the specific\ninstantiation of the problem where the goal is to find the state trajectories\nwithout exact transition points and derive a novel polynomial time inference\nalgorithm that outperforms vanilla inference techniques. We show that this\nparticular problem arises commonly in many disparate applications and present\nexperiments on three of them: (1) Toy robot tracking; (2) Single stroke\ncharacter recognition; (3) Handwritten word recognition.\n"
  },
  {
    "id": "1202.3762",
    "title": "Symbolic Dynamic Programming for Discrete and Continuous State MDPs",
    "abstract": "  Many real-world decision-theoretic planning problems can be naturally modeled\nwith discrete and continuous state Markov decision processes (DC-MDPs). While\nprevious work has addressed automated decision-theoretic planning for DCMDPs,\noptimal solutions have only been defined so far for limited settings, e.g.,\nDC-MDPs having hyper-rectangular piecewise linear value functions. In this\nwork, we extend symbolic dynamic programming (SDP) techniques to provide\noptimal solutions for a vastly expanded class of DCMDPs. To address the\ninherent combinatorial aspects of SDP, we introduce the XADD - a continuous\nvariable extension of the algebraic decision diagram (ADD) - that maintains\ncompact representations of the exact value function. Empirically, we\ndemonstrate an implementation of SDP with XADDs on various DC-MDPs, showing the\nfirst optimal automated solutions to DCMDPs with linear and nonlinear piecewise\npartitioned value functions and showing the advantages of constraint-based\npruning for XADDs.\n"
  },
  {
    "id": "1202.3764",
    "title": "Adjustment Criteria in Causal Diagrams: An Algorithmic Perspective",
    "abstract": "  Identifying and controlling bias is a key problem in empirical sciences.\nCausal diagram theory provides graphical criteria for deciding whether and how\ncausal effects can be identified from observed (nonexperimental) data by\ncovariate adjustment. Here we prove equivalences between existing as well as\nnew criteria for adjustment and we provide a new simplified but still\nequivalent notion of d-separation. These lead to efficient algorithms for two\nimportant tasks in causal diagram analysis: (1) listing minimal covariate\nadjustments (with polynomial delay); and (2) identifying the subdiagram\ninvolved in biasing paths (in linear time). Our results improve upon existing\nexponential-time solutions for these problems, enabling users to assess the\neffects of covariate adjustment on diagrams with tens to hundreds of variables\ninteractively in real time.\n"
  },
  {
    "id": "1202.3767",
    "title": "Distributed Anytime MAP Inference",
    "abstract": "  We present a distributed anytime algorithm for performing MAP inference in\ngraphical models. The problem is formulated as a linear programming relaxation\nover the edges of a graph. The resulting program has a constraint structure\nthat allows application of the Dantzig-Wolfe decomposition principle.\nSubprograms are defined over individual edges and can be computed in a\ndistributed manner. This accommodates solutions to graphs whose state space\ndoes not fit in memory. The decomposition master program is guaranteed to\ncompute the optimal solution in a finite number of iterations, while the\nsolution converges monotonically with each iteration. Formulating the MAP\ninference problem as a linear program allows additional (global) constraints to\nbe defined; something not possible with message passing algorithms.\nExperimental results show that our algorithm's solution quality outperforms\nmost current algorithms and it scales well to large problems.\n"
  },
  {
    "id": "1202.3773",
    "title": "Measuring the Hardness of Stochastic Sampling on Bayesian Networks with\n  Deterministic Causalities: the k-Test",
    "abstract": "  Approximate Bayesian inference is NP-hard. Dagum and Luby defined the Local\nVariance Bound (LVB) to measure the approximation hardness of Bayesian\ninference on Bayesian networks, assuming the networks model strictly positive\njoint probability distributions, i.e. zero probabilities are not permitted.\nThis paper introduces the k-test to measure the approximation hardness of\ninference on Bayesian networks with deterministic causalities in the\nprobability distribution, i.e. when zero conditional probabilities are\npermitted. Approximation by stochastic sampling is a widely-used inference\nmethod that is known to suffer from inefficiencies due to sample rejection. The\nk-test predicts when rejection rates of stochastic sampling a Bayesian network\nwill be low, modest, high, or when sampling is intractable.\n"
  },
  {
    "id": "1202.3887",
    "title": "Extended Mixture of MLP Experts by Hybrid of Conjugate Gradient Method\n  and Modified Cuckoo Search",
    "abstract": "  This paper investigates a new method for improving the learning algorithm of\nMixture of Experts (ME) model using a hybrid of Modified Cuckoo Search (MCS)\nand Conjugate Gradient (CG) as a second order optimization technique. The CG\ntechnique is combined with Back-Propagation (BP) algorithm to yield a much more\nefficient learning algorithm for ME structure. In addition, the experts and\ngating networks in enhanced model are replaced by CG based Multi-Layer\nPerceptrons (MLPs) to provide faster and more accurate learning. The CG is\nconsiderably depends on initial weights of connections of Artificial Neural\nNetwork (ANN), so, a metaheuristic algorithm, the so-called Modified Cuckoo\nSearch is applied in order to select the optimal weights. The performance of\nproposed method is compared with Gradient Decent Based ME (GDME) and Conjugate\nGradient Based ME (CGME) in classification and regression problems. The\nexperimental results show that hybrid MSC and CG based ME (MCS-CGME) has faster\nconvergence and better performance in utilized benchmark data sets.\n"
  },
  {
    "id": "1202.4190",
    "title": "Generalized FMD Detection for Spectrum Sensing Under Low Signal-to-Noise\n  Ratio",
    "abstract": "  Spectrum sensing is a fundamental problem in cognitive radio. We propose a\nfunction of covariance matrix based detection algorithm for spectrum sensing in\ncognitive radio network. Monotonically increasing property of function of\nmatrix involving trace operation is utilized as the cornerstone for this\nalgorithm. The advantage of proposed algorithm is it works under extremely low\nsignal-to-noise ratio, like lower than -30 dB with limited sample data.\nTheoretical analysis of threshold setting for the algorithm is discussed. A\nperformance comparison between the proposed algorithm and other\nstate-of-the-art methods is provided, by the simulation on captured digital\ntelevision (DTV) signal.\n"
  },
  {
    "id": "1202.6009",
    "title": "Marginality: a numerical mapping for enhanced treatment of nominal and\n  hierarchical attributes",
    "abstract": "  The purpose of statistical disclosure control (SDC) of microdata, a.k.a. data\nanonymization or privacy-preserving data mining, is to publish data sets\ncontaining the answers of individual respondents in such a way that the\nrespondents corresponding to the released records cannot be re-identified and\nthe released data are analytically useful. SDC methods are either based on\nmasking the original data, generating synthetic versions of them or creating\nhybrid versions by combining original and synthetic data. The choice of SDC\nmethods for categorical data, especially nominal data, is much smaller than the\nchoice of methods for numerical data. We mitigate this problem by introducing a\nnumerical mapping for hierarchical nominal data which allows computing means,\nvariances and covariances on them.\n"
  },
  {
    "id": "1202.6153",
    "title": "One Decade of Universal Artificial Intelligence",
    "abstract": "  The first decade of this century has seen the nascency of the first\nmathematical theory of general artificial intelligence. This theory of\nUniversal Artificial Intelligence (UAI) has made significant contributions to\nmany theoretical, philosophical, and practical AI questions. In a series of\npapers culminating in book (Hutter, 2005), an exciting sound and complete\nmathematical model for a super intelligent agent (AIXI) has been developed and\nrigorously analyzed. While nowadays most AI researchers avoid discussing\nintelligence, the award-winning PhD thesis (Legg, 2008) provided the\nphilosophical embedding and investigated the UAI-based universal measure of\nrational intelligence, which is formal, objective and non-anthropocentric.\nRecently, effective approximations of AIXI have been derived and experimentally\ninvestigated in JAIR paper (Veness et al. 2011). This practical breakthrough\nhas resulted in some impressive applications, finally muting earlier critique\nthat UAI is only a theory. For the first time, without providing any domain\nknowledge, the same agent is able to self-adapt to a diverse range of\ninteractive environments. For instance, AIXI is able to learn from scratch to\nplay TicTacToe, Pacman, Kuhn Poker, and other games by trial and error, without\neven providing the rules of the games.\n  These achievements give new hope that the grand goal of Artificial General\nIntelligence is not elusive.\n  This article provides an informal overview of UAI in context. It attempts to\ngently introduce a very theoretical, formal, and mathematical subject, and\ndiscusses philosophical and technical ingredients, traits of intelligence, some\nsocial questions, and the past and future of UAI.\n"
  },
  {
    "id": "1202.6386",
    "title": "Relational Reinforcement Learning in Infinite Mario",
    "abstract": "  Relational representations in reinforcement learning allow for the use of\nstructural information like the presence of objects and relationships between\nthem in the description of value functions. Through this paper, we show that\nsuch representations allow for the inclusion of background knowledge that\nqualitatively describes a state and can be used to design agents that\ndemonstrate learning behavior in domains with large state and actions spaces\nsuch as computer games.\n"
  },
  {
    "id": "1203.1021",
    "title": "Development of an Ontology to Assist the Modeling of Accident Scenarii\n  \"Application on Railroad Transport \"",
    "abstract": "  In a world where communication and information sharing are at the heart of\nour business, the terminology needs are most pressing. It has become imperative\nto identify the terms used and defined in a consensual and coherent way while\npreserving linguistic diversity. To streamline and strengthen the process of\nacquisition, representation and exploitation of scenarii of train accidents, it\nis necessary to harmonize and standardize the terminology used by players in\nthe security field. The research aims to significantly improve analytical\nactivities and operations of the various safety studies, by tracking the error\nin system, hardware, software and human. This paper presents the contribution\nof ontology to modeling scenarii for rail accidents through a knowledge model\nbased on a generic ontology and domain ontology. After a detailed presentation\nof the state of the art material, this article presents the first results of\nthe developed model.\n"
  },
  {
    "id": "1203.1095",
    "title": "Search Combinators",
    "abstract": "  The ability to model search in a constraint solver can be an essential asset\nfor solving combinatorial problems. However, existing infrastructure for\ndefining search heuristics is often inadequate. Either modeling capabilities\nare extremely limited or users are faced with a general-purpose programming\nlanguage whose features are not tailored towards writing search heuristics. As\na result, major improvements in performance may remain unexplored.\n  This article introduces search combinators, a lightweight and\nsolver-independent method that bridges the gap between a conceptually simple\nmodeling language for search (high-level, functional and naturally\ncompositional) and an efficient implementation (low-level, imperative and\nhighly non-modular). By allowing the user to define application-tailored search\nstrategies from a small set of primitives, search combinators effectively\nprovide a rich domain-specific language (DSL) for modeling search to the user.\nRemarkably, this DSL comes at a low implementation cost to the developer of a\nconstraint solver.\n  The article discusses two modular implementation approaches and shows, by\nempirical evaluation, that search combinators can be implemented without\noverhead compared to a native, direct implementation in a constraint solver.\n"
  },
  {
    "id": "1203.1882",
    "title": "Multi source feedback based performance appraisal system using Fuzzy\n  logic decision support system",
    "abstract": "  In Multi-Source Feedback or 360 Degree Feedback, data on the performance of\nan individual are collected systematically from a number of stakeholders and\nare used for improving performance. The 360-Degree Feedback approach provides a\nconsistent management philosophy meeting the criterion outlined previously. The\n360-degree feedback appraisal process describes a human resource methodology\nthat is frequently used for both employee appraisal and employee development.\nUsed in employee performance appraisals, the 360-degree feedback methodology is\ndifferentiated from traditional, top-down appraisal methods in which the\nsupervisor responsible for the appraisal provides the majority of the data.\nInstead it seeks to use information gained from other sources to provide a\nfuller picture of employees' performances. Similarly, when this technique used\nin employee development it augments employees' perceptions of training needs\nwith those of the people with whom they interact. The 360-degree feedback based\nappraisal is a comprehensive method where in the feedback about the employee\ncomes from all the sources that come into contact with the employee on his/her\njob. The respondents for an employee can be her/his peers, managers,\nsubordinates team members, customers, suppliers and vendors. Hence anyone who\ncomes into contact with the employee, the 360 degree appraisal has four\ncomponents that include self-appraisal, superior's appraisal, subordinate's\nappraisal student's appraisal and peer's appraisal .The proposed system is an\nattempt to implement the 360 degree feedback based appraisal system in\nacademics especially engineering colleges.\n"
  },
  {
    "id": "1203.3051",
    "title": "Combining Voting Rules Together",
    "abstract": "  We propose a simple method for combining together voting rules that performs\na run-off between the different winners of each voting rule. We prove that this\ncombinator has several good properties. For instance, even if just one of the\nbase voting rules has a desirable property like Condorcet consistency, the\ncombination inherits this property. In addition, we prove that combining voting\nrules together in this way can make finding a manipulation more computationally\ndifficult. Finally, we study the impact of this combinator on approximation\nmethods that find close to optimal manipulations.\n"
  },
  {
    "id": "1203.3464",
    "title": "Gibbs Sampling in Open-Universe Stochastic Languages",
    "abstract": "  Languages for open-universe probabilistic models (OUPMs) can represent\nsituations with an unknown number of objects and iden- tity uncertainty. While\nsuch cases arise in a wide range of important real-world appli- cations,\nexisting general purpose inference methods for OUPMs are far less efficient\nthan those available for more restricted lan- guages and model classes. This\npaper goes some way to remedying this deficit by in- troducing, and proving\ncorrect, a generaliza- tion of Gibbs sampling to partial worlds with possibly\nvarying model structure. Our ap- proach draws on and extends previous generic\nOUPM inference methods, as well as aux- iliary variable samplers for\nnonparametric mixture models. It has been implemented for BLOG, a well-known\nOUPM language. Combined with compile-time optimizations, the resulting\nalgorithm yields very substan- tial speedups over existing methods on sev- eral\ntest cases, and substantially improves the practicality of OUPM languages\ngenerally.\n"
  },
  {
    "id": "1203.3465",
    "title": "Compiling Possibilistic Networks: Alternative Approaches to\n  Possibilistic Inference",
    "abstract": "  Qualitative possibilistic networks, also known as min-based possibilistic\nnetworks, are important tools for handling uncertain information in the\npossibility theory frame- work. Despite their importance, only the junction\ntree adaptation has been proposed for exact reasoning with such networks. This\npaper explores alternative algorithms using compilation techniques. We first\npropose possibilistic adaptations of standard compilation-based probabilistic\nmethods. Then, we develop a new, purely possibilistic, method based on the\ntransformation of the initial network into a possibilistic base. A comparative\nstudy shows that this latter performs better than the possibilistic adap-\ntations of probabilistic methods. This result is also confirmed by experimental\nresults.\n"
  },
  {
    "id": "1203.3466",
    "title": "Possibilistic Answer Set Programming Revisited",
    "abstract": "  Possibilistic answer set programming (PASP) extends answer set programming\n(ASP) by attaching to each rule a degree of certainty. While such an extension\nis important from an application point of view, existing semantics are not\nwell-motivated, and do not always yield intuitive results. To develop a more\nsuitable semantics, we first introduce a characterization of answer sets of\nclassical ASP programs in terms of possibilistic logic where an ASP program\nspecifies a set of constraints on possibility distributions. This\ncharacterization is then naturally generalized to define answer sets of PASP\nprograms. We furthermore provide a syntactic counterpart, leading to a\npossibilistic generalization of the well-known Gelfond-Lifschitz reduct, and we\nshow how our framework can readily be implemented using standard ASP solvers.\n"
  },
  {
    "id": "1203.3467",
    "title": "Three new sensitivity analysis methods for influence diagrams",
    "abstract": "  Performing sensitivity analysis for influence diagrams using the decision\ncircuit framework is particularly convenient, since the partial derivatives\nwith respect to every parameter are readily available [Bhattacharjya and\nShachter, 2007; 2008]. In this paper we present three non-linear sensitivity\nanalysis methods that utilize this partial derivative information and therefore\ndo not require re-evaluating the decision situation multiple times.\nSpecifically, we show how to efficiently compare strategies in decision\nsituations, perform sensitivity to risk aversion and compute the value of\nperfect hedging [Seyller, 2008].\n"
  },
  {
    "id": "1203.3469",
    "title": "Probabilistic Similarity Logic",
    "abstract": "  Many machine learning applications require the ability to learn from and\nreason about noisy multi-relational data. To address this, several effective\nrepresentations have been developed that provide both a language for expressing\nthe structural regularities of a domain, and principled support for\nprobabilistic inference. In addition to these two aspects, however, many\napplications also involve a third aspect-the need to reason about\nsimilarities-which has not been directly supported in existing frameworks. This\npaper introduces probabilistic similarity logic (PSL), a general-purpose\nframework for joint reasoning about similarity in relational domains that\nincorporates probabilistic reasoning about similarities and relational\nstructure in a principled way. PSL can integrate any existing domain-specific\nsimilarity measures and also supports reasoning about similarities between sets\nof entities. We provide efficient inference and learning techniques for PSL and\ndemonstrate its effectiveness both in common relational tasks and in settings\nthat require reasoning about similarity.\n"
  },
  {
    "id": "1203.3470",
    "title": "ALARMS: Alerting and Reasoning Management System for Next Generation\n  Aircraft Hazards",
    "abstract": "  The Next Generation Air Transportation System will introduce new, advanced\nsensor technologies into the cockpit. With the introduction of such systems,\nthe responsibilities of the pilot are expected to dramatically increase. In the\nALARMS (ALerting And Reasoning Management System) project for NASA, we focus on\na key challenge of this environment, the quick and efficient handling of\naircraft sensor alerts. It is infeasible to alert the pilot on the state of all\nsubsystems at all times. Furthermore, there is uncertainty as to the true\nhazard state despite the evidence of the alerts, and there is uncertainty as to\nthe effect and duration of actions taken to address these alerts. This paper\nreports on the first steps in the construction of an application designed to\nhandle Next Generation alerts. In ALARMS, we have identified 60 different\naircraft subsystems and 20 different underlying hazards. In this paper, we show\nhow a Bayesian network can be used to derive the state of the underlying\nhazards, based on the sensor input. Then, we propose a framework whereby an\nautomated system can plan to address these hazards in cooperation with the\npilot, using a Time-Dependent Markov Process (TMDP). Different hazards and\npilot states will call for different alerting automation plans. We demonstrate\nthis emerging application of Bayesian networks and TMDPs to cockpit automation,\nfor a use case where a small number of hazards are present, and analyze the\nresulting alerting automation policies.\n"
  },
  {
    "id": "1203.3473",
    "title": "Lifted Inference for Relational Continuous Models",
    "abstract": "  Relational Continuous Models (RCMs) represent joint probability densities\nover attributes of objects, when the attributes have continuous domains. With\nrelational representations, they can model joint probability distributions over\nlarge numbers of variables compactly in a natural way. This paper presents a\nnew exact lifted inference algorithm for RCMs, thus it scales up to large\nmodels of real world applications. The algorithm applies to Relational Pairwise\nModels which are (relational) products of potentials of arity 2. Our algorithm\nis unique in two ways. First, it substantially improves the efficiency of\nlifted inference with variables of continuous domains. When a relational model\nhas Gaussian potentials, it takes only linear-time compared to cubic time of\nprevious methods. Second, it is the first exact inference algorithm which\nhandles RCMs in a lifted way. The algorithm is illustrated over an example from\neconometrics. Experimental results show that our algorithm outperforms both a\ngroundlevel inference algorithm and an algorithm built with previously-known\nlifted methods.\n"
  },
  {
    "id": "1203.3474",
    "title": "Distribution over Beliefs for Memory Bounded Dec-POMDP Planning",
    "abstract": "  We propose a new point-based method for approximate planning in Dec-POMDP\nwhich outperforms the state-of-the-art approaches in terms of solution quality.\nIt uses a heuristic estimation of the prior probability of beliefs to choose a\nbounded number of policy trees: this choice is formulated as a combinatorial\noptimisation problem minimising the error induced by pruning.\n"
  },
  {
    "id": "1203.3477",
    "title": "A Scalable Method for Solving High-Dimensional Continuous POMDPs Using\n  Local Approximation",
    "abstract": "  Partially-Observable Markov Decision Processes (POMDPs) are typically solved\nby finding an approximate global solution to a corresponding belief-MDP. In\nthis paper, we offer a new planning algorithm for POMDPs with continuous state,\naction and observation spaces. Since such domains have an inherent notion of\nlocality, we can find an approximate solution using local optimization methods.\nWe parameterize the belief distribution as a Gaussian mixture, and use the\nExtended Kalman Filter (EKF) to approximate the belief update. Since the EKF is\na first-order filter, we can marginalize over the observations analytically. By\nusing feedback control and state estimation during policy execution, we recover\na behavior that is effectively conditioned on incoming observations despite the\nunconditioned planning. Local optimization provides no guarantees of global\noptimality, but it allows us to tackle domains that are at least an order of\nmagnitude larger than the current state-of-the-art. We demonstrate the\nscalability of our algorithm by considering a simulated hand-eye coordination\ndomain with 16 continuous state dimensions and 6 continuous action dimensions.\n"
  },
  {
    "id": "1203.3482",
    "title": "Formula-Based Probabilistic Inference",
    "abstract": "  Computing the probability of a formula given the probabilities or weights\nassociated with other formulas is a natural extension of logical inference to\nthe probabilistic setting. Surprisingly, this problem has received little\nattention in the literature to date, particularly considering that it includes\nmany standard inference problems as special cases. In this paper, we propose\ntwo algorithms for this problem: formula decomposition and conditioning, which\nis an exact method, and formula importance sampling, which is an approximate\nmethod. The latter is, to our knowledge, the first application of model\ncounting to approximate probabilistic inference. Unlike conventional\nvariable-based algorithms, our algorithms work in the dual realm of logical\nformulas. Theoretically, we show that our algorithms can greatly improve\nefficiency by exploiting the structural information in the formulas.\nEmpirically, we show that they are indeed quite powerful, often achieving\nsubstantial performance gains over state-of-the-art schemes.\n"
  },
  {
    "id": "1203.3490",
    "title": "Anytime Planning for Decentralized POMDPs using Expectation Maximization",
    "abstract": "  Decentralized POMDPs provide an expressive framework for multi-agent\nsequential decision making. While fnite-horizon DECPOMDPs have enjoyed\nsignifcant success, progress remains slow for the infnite-horizon case mainly\ndue to the inherent complexity of optimizing stochastic controllers\nrepresenting agent policies. We present a promising new class of algorithms for\nthe infnite-horizon case, which recasts the optimization problem as inference\nin a mixture of DBNs. An attractive feature of this approach is the\nstraightforward adoption of existing inference techniques in DBNs for solving\nDEC-POMDPs and supporting richer representations such as factored or continuous\nstates and actions. We also derive the Expectation Maximization (EM) algorithm\nto optimize the joint policy represented as DBNs. Experiments on benchmark\ndomains show that EM compares favorably against the state-of-the-art solvers.\n"
  },
  {
    "id": "1203.3493",
    "title": "Solving Hybrid Influence Diagrams with Deterministic Variables",
    "abstract": "  We describe a framework and an algorithm for solving hybrid influence\ndiagrams with discrete, continuous, and deterministic chance variables, and\ndiscrete and continuous decision variables. A continuous chance variable in an\ninfluence diagram is said to be deterministic if its conditional distributions\nhave zero variances. The solution algorithm is an extension of Shenoy's fusion\nalgorithm for discrete influence diagrams. We describe an extended\nShenoy-Shafer architecture for propagation of discrete, continuous, and utility\npotentials in hybrid influence diagrams that include deterministic chance\nvariables. The algorithm and framework are illustrated by solving two small\nexamples.\n"
  },
  {
    "id": "1203.3499",
    "title": "A Delayed Column Generation Strategy for Exact k-Bounded MAP Inference\n  in Markov Logic Networks",
    "abstract": "  The paper introduces k-bounded MAP inference, a parameterization of MAP\ninference in Markov logic networks. k-Bounded MAP states are MAP states with at\nmost k active ground atoms of hidden (non-evidence) predicates. We present a\nnovel delayed column generation algorithm and provide empirical evidence that\nthe algorithm efficiently computes k-bounded MAP states for meaningful\nreal-world graph matching problems. The underlying idea is that, instead of\nsolving one large optimization problem, it is often more efficient to tackle\nseveral small ones.\n"
  },
  {
    "id": "1203.3500",
    "title": "Comparative Analysis of Probabilistic Models for Activity Recognition\n  with an Instrumented Walker",
    "abstract": "  Rollating walkers are popular mobility aids used by older adults to improve\nbalance control. There is a need to automatically recognize the activities\nperformed by walker users to better understand activity patterns, mobility\nissues and the context in which falls are more likely to happen. We design and\ncompare several techniques to recognize walker related activities. A\ncomprehensive evaluation with control subjects and walker users from a\nretirement community is presented.\n"
  },
  {
    "id": "1203.3508",
    "title": "Merging Knowledge Bases in Possibilistic Logic by Lexicographic\n  Aggregation",
    "abstract": "  Belief merging is an important but difficult problem in Artificial\nIntelligence, especially when sources of information are pervaded with\nuncertainty. Many merging operators have been proposed to deal with this\nproblem in possibilistic logic, a weighted logic which is powerful for handling\ninconsistency and deal- ing with uncertainty. They often result in a\npossibilistic knowledge base which is a set of weighted formulas. Although\npossibilistic logic is inconsistency tolerant, it suers from the well-known\n\"drowning effect\". Therefore, we may still want to obtain a consistent possi-\nbilistic knowledge base as the result of merg- ing. In such a case, we argue\nthat it is not always necessary to keep weighted informa- tion after merging.\nIn this paper, we define a merging operator that maps a set of pos- sibilistic\nknowledge bases and a formula rep- resenting the integrity constraints to a\nclas- sical knowledge base by using lexicographic ordering. We show that it\nsatisfies nine pos- tulates that generalize basic postulates for propositional\nmerging given in [11]. These postulates capture the principle of minimal change\nin some sense. We then provide an algorithm for generating the resulting knowl-\nedge base of our merging operator. Finally, we discuss the compatibility of our\nmerging operator with propositional merging and es- tablish the advantage of\nour merging opera- tor over existing semantic merging operators in the\npropositional case.\n"
  },
  {
    "id": "1203.3509",
    "title": "Characterizing the Set of Coherent Lower Previsions with a Finite Number\n  of Constraints or Vertices",
    "abstract": "  The standard coherence criterion for lower previsions is expressed using an\ninfinite number of linear constraints. For lower previsions that are\nessentially defined on some finite set of gambles on a finite possibility\nspace, we present a reformulation of this criterion that only uses a finite\nnumber of constraints. Any such lower prevision is coherent if it lies within\nthe convex polytope defined by these constraints. The vertices of this polytope\nare the extreme coherent lower previsions for the given set of gambles. Our\nreformulation makes it possible to compute them. We show how this is done and\nillustrate the procedure and its results.\n"
  },
  {
    "id": "1203.3513",
    "title": "Dynamic programming in in uence diagrams with decision circuits",
    "abstract": "  Decision circuits perform efficient evaluation of influence diagrams,\nbuilding on the ad- vances in arithmetic circuits for belief net- work\ninference [Darwiche, 2003; Bhattachar- jya and Shachter, 2007]. We show how\neven more compact decision circuits can be con- structed for dynamic\nprogramming in influ- ence diagrams with separable value functions and\nconditionally independent subproblems. Once a decision circuit has been\nconstructed based on the diagram's \"global\" graphical structure, it can be\ncompiled to exploit \"lo- cal\" structure for efficient evaluation and sen-\nsitivity analysis.\n"
  },
  {
    "id": "1203.3525",
    "title": "Learning Why Things Change: The Difference-Based Causality Learner",
    "abstract": "  In this paper, we present the Difference- Based Causality Learner (DBCL), an\nalgorithm for learning a class of discrete-time dynamic models that represents\nall causation across time by means of difference equations driving change in a\nsystem. We motivate this representation with real-world mechanical systems and\nprove DBCL's correctness for learning structure from time series data, an\nendeavour that is complicated by the existence of latent derivatives that have\nto be detected. We also prove that, under common assumptions for causal\ndiscovery, DBCL will identify the presence or absence of feedback loops, making\nthe model more useful for predicting the effects of manipulating variables when\nthe system is in equilibrium. We argue analytically and show empirically the\nadvantages of DBCL over vector autoregression (VAR) and Granger causality\nmodels as well as modified forms of Bayesian and constraintbased structure\ndiscovery algorithms. Finally, we show that our algorithm can discover causal\ndirections of alpha rhythms in human brains from EEG data.\n"
  },
  {
    "id": "1203.3528",
    "title": "Rollout Sampling Policy Iteration for Decentralized POMDPs",
    "abstract": "  We present decentralized rollout sampling policy iteration (DecRSPI) - a new\nalgorithm for multi-agent decision problems formalized as DEC-POMDPs. DecRSPI\nis designed to improve scalability and tackle problems that lack an explicit\nmodel. The algorithm uses Monte- Carlo methods to generate a sample of\nreachable belief states. Then it computes a joint policy for each belief state\nbased on the rollout estimations. A new policy representation allows us to\nrepresent solutions compactly. The key benefits of the algorithm are its linear\ntime complexity over the number of agents, its bounded memory usage and good\nsolution quality. It can solve larger problems that are intractable for\nexisting planning algorithms. Experimental results confirm the effectiveness\nand scalability of the approach.\n"
  },
  {
    "id": "1203.3531",
    "title": "Solving Multistage Influence Diagrams using Branch-and-Bound Search",
    "abstract": "  A branch-and-bound approach to solving influ- ence diagrams has been\npreviously proposed in the literature, but appears to have never been\nimplemented and evaluated - apparently due to the difficulties of computing\neffective bounds for the branch-and-bound search. In this paper, we describe\nhow to efficiently compute effective bounds, and we develop a practical\nimplementa- tion of depth-first branch-and-bound search for influence diagram\nevaluation that outperforms existing methods for solving influence diagrams\nwith multiple stages.\n"
  },
  {
    "id": "1203.3538",
    "title": "RAPID: A Reachable Anytime Planner for Imprecisely-sensed Domains",
    "abstract": "  Despite the intractability of generic optimal partially observable Markov\ndecision process planning, there exist important problems that have highly\nstructured models. Previous researchers have used this insight to construct\nmore efficient algorithms for factored domains, and for domains with\ntopological structure in the flat state dynamics model. In our work, motivated\nby findings from the education community relevant to automated tutoring, we\nconsider problems that exhibit a form of topological structure in the factored\ndynamics model. Our Reachable Anytime Planner for Imprecisely-sensed Domains\n(RAPID) leverages this structure to efficiently compute a good initial envelope\nof reachable states under the optimal MDP policy in time linear in the number\nof state variables. RAPID performs partially-observable planning over the\nlimited envelope of states, and slowly expands the state space considered as\ntime allows. RAPID performs well on a large tutoring-inspired problem\nsimulation with 122 state variables, corresponding to a flat state space of\nover 10^30 states.\n"
  },
  {
    "id": "1203.4011",
    "title": "Understanding Sampling Style Adversarial Search Methods",
    "abstract": "  UCT has recently emerged as an exciting new adversarial reasoning technique\nbased on cleverly balancing exploration and exploitation in a Monte-Carlo\nsampling setting. It has been particularly successful in the game of Go but the\nreasons for its success are not well understood and attempts to replicate its\nsuccess in other domains such as Chess have failed. We provide an in-depth\nanalysis of the potential of UCT in domain-independent settings, in cases where\nheuristic values are available, and the effect of enhancing random playouts to\nmore informed playouts between two weak minimax players. To provide further\ninsights, we develop synthetic game tree instances and discuss interesting\nproperties of UCT, both empirically and analytically.\n"
  },
  {
    "id": "1203.4287",
    "title": "Parameter Learning in PRISM Programs with Continuous Random Variables",
    "abstract": "  Probabilistic Logic Programming (PLP), exemplified by Sato and Kameya's\nPRISM, Poole's ICL, De Raedt et al's ProbLog and Vennekens et al's LPAD,\ncombines statistical and logical knowledge representation and inference.\nInference in these languages is based on enumerative construction of proofs\nover logic programs. Consequently, these languages permit very limited use of\nrandom variables with continuous distributions. In this paper, we extend PRISM\nwith Gaussian random variables and linear equality constraints, and consider\nthe problem of parameter learning in the extended language. Many statistical\nmodels such as finite mixture models and Kalman filter can be encoded in\nextended PRISM. Our EM-based learning algorithm uses a symbolic inference\nprocedure that represents sets of derivations without enumeration. This permits\nus to learn the distribution parameters of extended PRISM programs with\ndiscrete as well as Gaussian variables. The learning algorithm naturally\ngeneralizes the ones used for PRISM and Hybrid Bayesian Networks.\n"
  },
  {
    "id": "1203.5452",
    "title": "Modeling of Mixed Decision Making Process",
    "abstract": "  Decision making whenever and wherever it is happened is key to organizations\nsuccess. In order to make correct decision, individuals, teams and\norganizations need both knowledge management (to manage content) and\ncollaboration (to manage group processes) to make that more effective and\nefficient. In this paper, we explain the knowledge management and collaboration\nconvergence. Then, we propose a formal description of mixed and multimodal\ndecision making (MDM) process where decision may be made by three possible\nmodes: individual, collective or hybrid. Finally, we explicit the MDM process\nbased on UML-G profile.\n"
  },
  {
    "id": "1203.5532",
    "title": "On the Use of Non-Stationary Policies for Infinite-Horizon Discounted\n  Markov Decision Processes",
    "abstract": "  We consider infinite-horizon $\\gamma$-discounted Markov Decision Processes,\nfor which it is known that there exists a stationary optimal policy. We\nconsider the algorithm Value Iteration and the sequence of policies\n$\\pi_1,...,\\pi_k$ it implicitely generates until some iteration $k$. We provide\nperformance bounds for non-stationary policies involving the last $m$ generated\npolicies that reduce the state-of-the-art bound for the last stationary policy\n$\\pi_k$ by a factor $\\frac{1-\\gamma}{1-\\gamma^m}$. In particular, the use of\nnon-stationary policies allows to reduce the usual asymptotic performance\nbounds of Value Iteration with errors bounded by $\\epsilon$ at each iteration\nfrom $\\frac{\\gamma}{(1-\\gamma)^2}\\epsilon$ to\n$\\frac{\\gamma}{1-\\gamma}\\epsilon$, which is significant in the usual situation\nwhen $\\gamma$ is close to 1. Given Bellman operators that can only be computed\nwith some error $\\epsilon$, a surprising consequence of this result is that the\nproblem of \"computing an approximately optimal non-stationary policy\" is much\nsimpler than that of \"computing an approximately optimal stationary policy\",\nand even slightly simpler than that of \"approximately computing the value of\nsome fixed policy\", since this last problem only has a guarantee of\n$\\frac{1}{1-\\gamma}\\epsilon$.\n"
  },
  {
    "id": "1203.6716",
    "title": "Creating Intelligent Linking for Information Threading in Knowledge\n  Networks",
    "abstract": "  Informledge System (ILS) is a knowledge network with autonomous nodes and\nintelligent links that integrate and structure the pieces of knowledge. In this\npaper, we aim to put forward the link dynamics involved in intelligent\nprocessing of information in ILS. There has been advancement in knowledge\nmanagement field which involve managing information in databases from a single\ndomain. ILS works with information from multiple domains stored in distributed\nway in the autonomous nodes termed as Knowledge Network Node (KNN). Along with\nthe concept under consideration, KNNs store the processed information linking\nconcepts and processors leading to the appropriate processing of information.\n"
  },
  {
    "id": "1204.0181",
    "title": "Expert PC Troubleshooter With Fuzzy-Logic And Self-Learning Support",
    "abstract": "  Expert systems use human knowledge often stored as rules within the computer\nto solve problems that generally would entail human intelligence. Today, with\ninformation systems turning out to be more pervasive and with the myriad\nadvances in information technologies, automating computer fault diagnosis is\nbecoming so fundamental that soon every enterprise has to endorse it. This\npaper proposes an expert system called Expert PC Troubleshooter for diagnosing\ncomputer problems. The system is composed of a user interface, a rule-base, an\ninference engine, and an expert interface. Additionally, the system features a\nfuzzy-logic module to troubleshoot POST beep errors, and an intelligent agent\nthat assists in the knowledge acquisition process. The proposed system is meant\nto automate the maintenance, repair, and operations (MRO) process, and free-up\nhuman technicians from manually performing routine, laborious, and\ntimeconsuming maintenance tasks. As future work, the proposed system is to be\nparallelized so as to boost its performance and speed-up its various\noperations.\n"
  },
  {
    "id": "1204.0731",
    "title": "Unit contradiction versus unit propagation",
    "abstract": "  Some aspects of the result of applying unit resolution on a CNF formula can\nbe formalized as functions with domain a set of partial truth assignments. We\nare interested in two ways for computing such functions, depending on whether\nthe result is the production of the empty clause or the assignment of a\nvariable with a given truth value. We show that these two models can compute\nthe same functions with formulae of polynomially related sizes, and we explain\nhow this result is related to the CNF encoding of Boolean constraints.\n"
  },
  {
    "id": "1204.1576",
    "title": "Development of knowledge Base Expert System for Natural treatment of\n  Diabetes disease",
    "abstract": "  The development of expert system for treatment of Diabetes disease by using\nnatural methods is new information technology derived from Artificial\nIntelligent research using ESTA (Expert System Text Animation) System. The\nproposed expert system contains knowledge about various methods of natural\ntreatment methods (Massage, Herbal/Proper Nutrition, Acupuncture, Gems) for\nDiabetes diseases of Human Beings. The system is developed in the ESTA (Expert\nSystem shell for Text Animation) which is Visual Prolog 7.3 Application. The\nknowledge for the said system will be acquired from domain experts, texts and\nother related sources.\n"
  },
  {
    "id": "1204.1637",
    "title": "Characterization of Dynamic Bayesian Network",
    "abstract": "  In this report, we will be interested at Dynamic Bayesian Network (DBNs) as a\nmodel that tries to incorporate temporal dimension with uncertainty. We start\nwith basics of DBN where we especially focus in Inference and Learning concepts\nand algorithms. Then we will present different levels and methods of creating\nDBNs as well as approaches of incorporating temporal dimension in static\nBayesian network.\n"
  },
  {
    "id": "1204.1653",
    "title": "Machine Cognition Models: EPAM and GPS",
    "abstract": "  Through history, the human being tried to relay its daily tasks to other\ncreatures, which was the main reason behind the rise of civilizations. It\nstarted with deploying animals to automate tasks in the field of\nagriculture(bulls), transportation (e.g. horses and donkeys), and even\ncommunication (pigeons). Millenniums after, come the Golden age with\n\"Al-jazari\" and other Muslim inventors, which were the pioneers of automation,\nthis has given birth to industrial revolution in Europe, centuries after. At\nthe end of the nineteenth century, a new era was to begin, the computational\nera, the most advanced technological and scientific development that is driving\nthe mankind and the reason behind all the evolutions of science; such as\nmedicine, communication, education, and physics. At this edge of technology\nengineers and scientists are trying to model a machine that behaves the same as\nthey do, which pushed us to think about designing and implementing \"Things\nthat-Thinks\", then artificial intelligence was. In this work we will cover each\nof the major discoveries and studies in the field of machine cognition, which\nare the \"Elementary Perceiver and Memorizer\"(EPAM) and \"The General Problem\nSolver\"(GPS). The First one focus mainly on implementing the human-verbal\nlearning behavior, while the second one tries to model an architecture that is\nable to solve problems generally (e.g. theorem proving, chess playing, and\narithmetic). We will cover the major goals and the main ideas of each model, as\nwell as comparing their strengths and weaknesses, and finally giving their\nfields of applications. And Finally, we will suggest a real life implementation\nof a cognitive machine.\n"
  },
  {
    "id": "1204.1851",
    "title": "A Probabilistic Logic Programming Event Calculus",
    "abstract": "  We present a system for recognising human activity given a symbolic\nrepresentation of video content. The input of our system is a set of\ntime-stamped short-term activities (STA) detected on video frames. The output\nis a set of recognised long-term activities (LTA), which are pre-defined\ntemporal combinations of STA. The constraints on the STA that, if satisfied,\nlead to the recognition of a LTA, have been expressed using a dialect of the\nEvent Calculus. In order to handle the uncertainty that naturally occurs in\nhuman activity recognition, we adapted this dialect to a state-of-the-art\nprobabilistic logic programming framework. We present a detailed evaluation and\ncomparison of the crisp and probabilistic approaches through experimentation on\na benchmark dataset of human surveillance videos.\n"
  },
  {
    "id": "1204.2018",
    "title": "Applications of fuzzy logic to Case-Based Reasoning",
    "abstract": "  The article discusses some applications of fuzzy logic ideas to formalizing\nof the Case-Based Reasoning (CBR) process and to measuring the effectiveness of\nCBR systems\n"
  },
  {
    "id": "1204.3255",
    "title": "Lower Complexity Bounds for Lifted Inference",
    "abstract": "  One of the big challenges in the development of probabilistic relational (or\nprobabilistic logical) modeling and learning frameworks is the design of\ninference techniques that operate on the level of the abstract model\nrepresentation language, rather than on the level of ground, propositional\ninstances of the model. Numerous approaches for such \"lifted inference\"\ntechniques have been proposed. While it has been demonstrated that these\ntechniques will lead to significantly more efficient inference on some specific\nmodels, there are only very recent and still quite restricted results that show\nthe feasibility of lifted inference on certain syntactically defined classes of\nmodels. Lower complexity bounds that imply some limitations for the feasibility\nof lifted inference on more expressive model classes were established early on\nin (Jaeger 2000). However, it is not immediate that these results also apply to\nthe type of modeling languages that currently receive the most attention, i.e.,\nweighted, quantifier-free formulas. In this paper we extend these earlier\nresults, and show that under the assumption that NETIME =/= ETIME, there is no\npolynomial lifted inference algorithm for knowledge bases of weighted,\nquantifier- and function-free formulas. Further strengthening earlier results,\nthis is also shown to hold for approximate inference, and for knowledge bases\nnot containing the equality predicate.\n"
  },
  {
    "id": "1204.3844",
    "title": "On how percolation threshold affects PSO performance",
    "abstract": "  Statistical evidence of the influence of neighborhood topology on the\nperformance of particle swarm optimization (PSO) algorithms has been shown in\nmany works. However, little has been done about the implications could have the\npercolation threshold in determining the topology of this neighborhood. This\nwork addresses this problem for individuals that, like robots, are able to\nsense in a limited neighborhood around them. Based on the concept of\npercolation threshold, and more precisely, the disk percolation model in 2D, we\nshow that better results are obtained for low values of radius, when\nindividuals occasionally ask others their best visited positions, with the\nconsequent decrease of computational complexity. On the other hand, since\npercolation threshold is a universal measure, it could have a great interest to\ncompare the performance of different hybrid PSO algorithms.\n"
  },
  {
    "id": "1204.4051",
    "title": "Solution Representations and Local Search for the bi-objective Inventory\n  Routing Problem",
    "abstract": "  The solution of the biobjective IRP is rather challenging, even for\nmetaheuristics. We are still lacking a profound understanding of appropriate\nsolution representations and effective neighborhood structures. Clearly, both\nthe delivery volumes and the routing aspects of the alternatives need to be\nreflected in an encoding, and must be modified when searching by means of local\nsearch. Our work contributes to the better understanding of such solution\nrepresentations. On the basis of an experimental investigation, the advantages\nand drawbacks of two encodings are studied and compared.\n"
  },
  {
    "id": "1204.4541",
    "title": "Automatic Sampling of Geographic objects",
    "abstract": "  Today, one's disposes of large datasets composed of thousands of geographic\nobjects. However, for many processes, which require the appraisal of an expert\nor much computational time, only a small part of these objects can be taken\ninto account. In this context, robust sampling methods become necessary. In\nthis paper, we propose a sampling method based on clustering techniques. Our\nmethod consists in dividing the objects in clusters, then in selecting in each\ncluster, the most representative objects. A case-study in the context of a\nprocess dedicated to knowledge revision for geographic data generalisation is\npresented. This case-study shows that our method allows to select relevant\nsamples of objects.\n"
  },
  {
    "id": "1204.4989",
    "title": "Using Belief Theory to Diagnose Control Knowledge Quality. Application\n  to cartographic generalisation",
    "abstract": "  Both humans and artificial systems frequently use trial and error methods to\nproblem solving. In order to be effective, this type of strategy implies having\nhigh quality control knowledge to guide the quest for the optimal solution.\nUnfortunately, this control knowledge is rarely perfect. Moreover, in\nartificial systems-as in humans-self-evaluation of one's own knowledge is often\ndifficult. Yet, this self-evaluation can be very useful to manage knowledge and\nto determine when to revise it. The objective of our work is to propose an\nautomated approach to evaluate the quality of control knowledge in artificial\nsystems based on a specific trial and error strategy, namely the informed tree\nsearch strategy. Our revision approach consists in analysing the system's\nexecution logs, and in using the belief theory to evaluate the global quality\nof the knowledge. We present a real-world industrial application in the form of\nan experiment using this approach in the domain of cartographic generalisation.\nThus far, the results of using our approach have been encouraging.\n"
  },
  {
    "id": "1204.6415",
    "title": "A Fuzzy Model for Analogical Problem Solving",
    "abstract": "  In this paper we develop a fuzzy model for the description of the process of\nAnalogical Reasoning by representing its main steps as fuzzy subsets of a set\nof linguistic labels characterizing the individuals' performance in each step\nand we use the Shannon- Wiener diversity index as a measure of the individuals'\nabilities in analogical problem solving. This model is compared with a\nstochastic model presented in author's earlier papers by introducing a finite\nMarkov chain on the steps of the process of Analogical Reasoning. A classroom\nexperiment is also presented to illustrate the use of our results in practice.\n"
  },
  {
    "id": "1205.1645",
    "title": "Publishing and linking transport data on the Web",
    "abstract": "  Without Linked Data, transport data is limited to applications exclusively\naround transport. In this paper, we present a workflow for publishing and\nlinking transport data on the Web. So we will be able to develop transport\napplications and to add other features which will be created from other\ndatasets. This will be possible because transport data will be linked to these\ndatasets. We apply this workflow to two datasets: NEPTUNE, a French standard\ndescribing a transport line, and Passim, a directory containing relevant\ninformation on transport services, in every French city.\n"
  },
  {
    "id": "1205.2541",
    "title": "An improved approach to attribute reduction with covering rough sets",
    "abstract": "  Attribute reduction is viewed as an important preprocessing step for pattern\nrecognition and data mining. Most of researches are focused on attribute\nreduction by using rough sets. Recently, Tsang et al. discussed attribute\nreduction with covering rough sets in the paper [E. C.C. Tsang, D. Chen, Daniel\nS. Yeung, Approximations and reducts with covering generalized rough sets,\nComputers and Mathematics with Applications 56 (2008) 279-289], where an\napproach based on discernibility matrix was presented to compute all attribute\nreducts. In this paper, we provide an improved approach by constructing simpler\ndiscernibility matrix with covering rough sets, and then proceed to improve\nsome characterizations of attribute reduction provided by Tsang et al. It is\nproved that the improved discernible matrix is equivalent to the old one, but\nthe computational complexity of discernible matrix is greatly reduced.\n"
  },
  {
    "id": "1205.2596",
    "title": "Proceedings of the Twenty-Seventh Conference on Uncertainty in\n  Artificial Intelligence (2011)",
    "abstract": "  This is the Proceedings of the Twenty-Seventh Conference on Uncertainty in\nArtificial Intelligence, which was held in Barcelona, Spain, July 14 - 17 2011.\n"
  },
  {
    "id": "1205.2597",
    "title": "Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial\n  Intelligence (2010)",
    "abstract": "  This is the Proceedings of the Twenty-Sixth Conference on Uncertainty in\nArtificial Intelligence, which was held on Catalina Island, CA, July 8 - 11\n2010.\n"
  },
  {
    "id": "1205.2601",
    "title": "Most Relevant Explanation: Properties, Algorithms, and Evaluations",
    "abstract": "  Most Relevant Explanation (MRE) is a method for finding multivariate\nexplanations for given evidence in Bayesian networks [12]. This paper studies\nthe theoretical properties of MRE and develops an algorithm for finding\nmultiple top MRE solutions. Our study shows that MRE relies on an implicit soft\nrelevance measure in automatically identifying the most relevant target\nvariables and pruning less relevant variables from an explanation. The soft\nmeasure also enables MRE to capture the intuitive phenomenon of explaining away\nencoded in Bayesian networks. Furthermore, our study shows that the solution\nspace of MRE has a special lattice structure which yields interesting dominance\nrelations among the solutions. A K-MRE algorithm based on these dominance\nrelations is developed for generating a set of top solutions that are more\nrepresentative. Our empirical results show that MRE methods are promising\napproaches for explanation in Bayesian networks.\n"
  },
  {
    "id": "1205.2613",
    "title": "Measuring Inconsistency in Probabilistic Knowledge Bases",
    "abstract": "  This paper develops an inconsistency measure on conditional probabilistic\nknowledge bases. The measure is based on fundamental principles for\ninconsistency measures and thus provides a solid theoretical framework for the\ntreatment of inconsistencies in probabilistic expert systems. We illustrate its\nusefulness and immediate application on several examples and present some\nformal results. Building on this measure we use the Shapley value-a well-known\nsolution for coalition games-to define a sophisticated indicator that is not\nonly able to measure inconsistencies but to reveal the causes of\ninconsistencies in the knowledge base. Altogether these tools guide the\nknowledge engineer in his aim to restore consistency and therefore enable him\nto build a consistent and usable knowledge base that can be employed in\nprobabilistic expert systems.\n"
  },
  {
    "id": "1205.2616",
    "title": "Bisimulation-based Approximate Lifted Inference",
    "abstract": "  There has been a great deal of recent interest in methods for performing\nlifted inference; however, most of this work assumes that the first-order model\nis given as input to the system. Here, we describe lifted inference algorithms\nthat determine symmetries and automatically lift the probabilistic model to\nspeedup inference. In particular, we describe approximate lifted inference\ntechniques that allow the user to trade off inference accuracy for\ncomputational efficiency by using a handful of tunable parameters, while\nkeeping the error bounded. Our algorithms are closely related to the\ngraph-theoretic concept of bisimulation. We report experiments on both\nsynthetic and real data to show that in the presence of symmetries, run-times\nfor inference can be improved significantly, with approximate lifted inference\nproviding orders of magnitude speedup over ground inference.\n"
  },
  {
    "id": "1205.2619",
    "title": "Regret-based Reward Elicitation for Markov Decision Processes",
    "abstract": "  The specification of aMarkov decision process (MDP) can be difficult. Reward\nfunction specification is especially problematic; in practice, it is often\ncognitively complex and time-consuming for users to precisely specify rewards.\nThis work casts the problem of specifying rewards as one of preference\nelicitation and aims to minimize the degree of precision with which a reward\nfunction must be specified while still allowing optimal or near-optimal\npolicies to be produced. We first discuss how robust policies can be computed\nfor MDPs given only partial reward information using the minimax regret\ncriterion. We then demonstrate how regret can be reduced by efficiently\neliciting reward information using bound queries, using regret-reduction as a\nmeans for choosing suitable queries. Empirical results demonstrate that\nregret-based reward elicitation offers an effective way to produce near-optimal\npolicies without resorting to the precise specification of the entire reward\nfunction.\n"
  },
  {
    "id": "1205.2621",
    "title": "Logical Inference Algorithms and Matrix Representations for\n  Probabilistic Conditional Independence",
    "abstract": "  Logical inference algorithms for conditional independence (CI) statements\nhave important applications from testing consistency during knowledge\nelicitation to constraintbased structure learning of graphical models. We prove\nthat the implication problem for CI statements is decidable, given that the\nsize of the domains of the random variables is known and fixed. We will present\nan approximate logical inference algorithm which combines a falsification and a\nnovel validation algorithm. The validation algorithm represents each set of CI\nstatements as a sparse 0-1 matrix A and validates instances of the implication\nproblem by solving specific linear programs with constraint matrix A. We will\nshow experimentally that the algorithm is both effective and efficient in\nvalidating and falsifying instances of the probabilistic CI implication\nproblem.\n"
  },
  {
    "id": "1205.2634",
    "title": "The Temporal Logic of Causal Structures",
    "abstract": "  Computational analysis of time-course data with an underlying causal\nstructure is needed in a variety of domains, including neural spike trains,\nstock price movements, and gene expression levels. However, it can be\nchallenging to determine from just the numerical time course data alone what is\ncoordinating the visible processes, to separate the underlying prima facie\ncauses into genuine and spurious causes and to do so with a feasible\ncomputational complexity. For this purpose, we have been developing a novel\nalgorithm based on a framework that combines notions of causality in philosophy\nwith algorithmic approaches built on model checking and statistical techniques\nfor multiple hypotheses testing. The causal relationships are described in\nterms of temporal logic formulae, reframing the inference problem in terms of\nmodel checking. The logic used, PCTL, allows description of both the time\nbetween cause and effect and the probability of this relationship being\nobserved. We show that equipped with these causal formulae with their\nassociated probabilities we may compute the average impact a cause makes to its\neffect and then discover statistically significant causes through the concepts\nof multiple hypothesis testing (treating each causal relationship as a\nhypothesis), and false discovery control. By exploring a well-chosen family of\npotentially all significant hypotheses with reasonably minimal description\nlength, it is possible to tame the algorithm's computational complexity while\nexploring the nearly complete search-space of all prima facie causes. We have\ntested these ideas in a number of domains and illustrate them here with two\nexamples.\n"
  },
  {
    "id": "1205.2635",
    "title": "Constraint Processing in Lifted Probabilistic Inference",
    "abstract": "  First-order probabilistic models combine representational power of\nfirst-order logic with graphical models. There is an ongoing effort to design\nlifted inference algorithms for first-order probabilistic models. We analyze\nlifted inference from the perspective of constraint processing and, through\nthis viewpoint, we analyze and compare existing approaches and expose their\nadvantages and limitations. Our theoretical results show that the wrong choice\nof constraint processing method can lead to exponential increase in\ncomputational complexity. Our empirical tests confirm the importance of\nconstraint processing in lifted inference. This is the first theoretical and\nempirical study of constraint processing in lifted inference.\n"
  },
  {
    "id": "1205.2637",
    "title": "Counting Belief Propagation",
    "abstract": "  A major benefit of graphical models is that most knowledge is captured in the\nmodel structure. Many models, however, produce inference problems with a lot of\nsymmetries not reflected in the graphical structure and hence not exploitable\nby efficient inference techniques such as belief propagation (BP). In this\npaper, we present a new and simple BP algorithm, called counting BP, that\nexploits such additional symmetries. Starting from a given factor graph,\ncounting BP first constructs a compressed factor graph of clusternodes and\nclusterfactors, corresponding to sets of nodes and factors that are\nindistinguishable given the evidence. Then it runs a modified BP algorithm on\nthe compressed graph that is equivalent to running BP on the original factor\ngraph. Our experiments show that counting BP is applicable to a variety of\nimportant AI tasks such as (dynamic) relational models and boolean model\ncounting, and that significant efficiency gains are obtainable, often by orders\nof magnitude.\n"
  },
  {
    "id": "1205.2642",
    "title": "Improved Mean and Variance Approximations for Belief Net Responses via\n  Network Doubling",
    "abstract": "  A Bayesian belief network models a joint distribution with an directed\nacyclic graph representing dependencies among variables and network parameters\ncharacterizing conditional distributions. The parameters are viewed as random\nvariables to quantify uncertainty about their values. Belief nets are used to\ncompute responses to queries; i.e., conditional probabilities of interest. A\nquery is a function of the parameters, hence a random variable. Van Allen et\nal. (2001, 2008) showed how to quantify uncertainty about a query via a delta\nmethod approximation of its variance. We develop more accurate approximations\nfor both query mean and variance. The key idea is to extend the query mean\napproximation to a \"doubled network\" involving two independent replicates. Our\nmethod assumes complete data and can be applied to discrete, continuous, and\nhybrid networks (provided discrete variables have only discrete parents). We\nanalyze several improvements, and provide empirical studies to demonstrate\ntheir effectiveness.\n"
  },
  {
    "id": "1205.2647",
    "title": "Generating Optimal Plans in Highly-Dynamic Domains",
    "abstract": "  Generating optimal plans in highly dynamic environments is challenging. Plans\nare predicated on an assumed initial state, but this state can change\nunexpectedly during plan generation, potentially invalidating the planning\neffort. In this paper we make three contributions: (1) We propose a novel\nalgorithm for generating optimal plans in settings where frequent, unexpected\nevents interfere with planning. It is able to quickly distinguish relevant from\nirrelevant state changes, and to update the existing planning search tree if\nnecessary. (2) We argue for a new criterion for evaluating plan adaptation\ntechniques: the relative running time compared to the \"size\" of changes. This\nis significant since during recovery more changes may occur that need to be\nrecovered from subsequently, and in order for this process of repeated recovery\nto terminate, recovery time has to converge. (3) We show empirically that our\napproach can converge and find optimal plans in environments that would\nordinarily defy planning due to their high dynamics.\n"
  },
  {
    "id": "1205.2651",
    "title": "Seeing the Forest Despite the Trees: Large Scale Spatial-Temporal\n  Decision Making",
    "abstract": "  We introduce a challenging real-world planning problem where actions must be\ntaken at each location in a spatial area at each point in time. We use forestry\nplanning as the motivating application. In Large Scale Spatial-Temporal (LSST)\nplanning problems, the state and action spaces are defined as the\ncross-products of many local state and action spaces spread over a large\nspatial area such as a city or forest. These problems possess state\nuncertainty, have complex utility functions involving spatial constraints and\nwe generally must rely on simulations rather than an explicit transition model.\nWe define LSST problems as reinforcement learning problems and present a\nsolution using policy gradients. We compare two different policy formulations:\nan explicit policy that identifies each location in space and the action to\ntake there; and an abstract policy that defines the proportion of actions to\ntake across all locations in space. We show that the abstract policy is more\nrobust and achieves higher rewards with far fewer parameters than the\nelementary policy. This abstract policy is also a better fit to the properties\nthat practitioners in LSST problem domains require for such methods to be\nwidely useful.\n"
  },
  {
    "id": "1205.2652",
    "title": "Complexity Analysis and Variational Inference for Interpretation-based\n  Probabilistic Description Logic",
    "abstract": "  This paper presents complexity analysis and variational methods for inference\nin probabilistic description logics featuring Boolean operators,\nquantification, qualified number restrictions, nominals, inverse roles and role\nhierarchies. Inference is shown to be PEXP-complete, and variational methods\nare designed so as to exploit logical inference whenever possible.\n"
  },
  {
    "id": "1205.2655",
    "title": "Mean Field Variational Approximation for Continuous-Time Bayesian\n  Networks",
    "abstract": "  Continuous-time Bayesian networks is a natural structured representation\nlanguage for multicomponent stochastic processes that evolve continuously over\ntime. Despite the compact representation, inference in such models is\nintractable even in relatively simple structured networks. Here we introduce a\nmean field variational approximation in which we use a product of inhomogeneous\nMarkov processes to approximate a distribution over trajectories. This\nvariational approach leads to a globally consistent distribution, which can be\nefficiently queried. Additionally, it provides a lower bound on the probability\nof observations, thus making it attractive for learning tasks. We provide the\ntheoretical foundations for the approximation, an efficient implementation that\nexploits the wide range of highly optimized ordinary differential equations\n(ODE) solvers, experimentally explore characterizations of processes for which\nthis approximation is suitable, and show applications to a large-scale\nrealworld inference problem.\n"
  },
  {
    "id": "1205.2659",
    "title": "Deterministic POMDPs Revisited",
    "abstract": "  We study a subclass of POMDPs, called Deterministic POMDPs, that is\ncharacterized by deterministic actions and observations. These models do not\nprovide the same generality of POMDPs yet they capture a number of interesting\nand challenging problems, and permit more efficient algorithms. Indeed, some of\nthe recent work in planning is built around such assumptions mainly by the\nquest of amenable models more expressive than the classical deterministic\nmodels. We provide results about the fundamental properties of Deterministic\nPOMDPs, their relation with AND/OR search problems and algorithms, and their\ncomputational complexity.\n"
  },
  {
    "id": "1205.2665",
    "title": "Lower Bound Bayesian Networks - An Efficient Inference of Lower Bounds\n  on Probability Distributions in Bayesian Networks",
    "abstract": "  We present a new method to propagate lower bounds on conditional probability\ndistributions in conventional Bayesian networks. Our method guarantees to\nprovide outer approximations of the exact lower bounds. A key advantage is that\nwe can use any available algorithms and tools for Bayesian networks in order to\nrepresent and infer lower bounds. This new method yields results that are\nprovable exact for trees with binary variables, and results which are\ncompetitive to existing approximations in credal networks for all other network\nstructures. Our method is not limited to a specific kind of network structure.\nBasically, it is also not restricted to a specific kind of inference, but we\nrestrict our analysis to prognostic inference in this article. The\ncomputational complexity is superior to that of other existing approaches.\n"
  },
  {
    "id": "1205.2857",
    "title": "Operations on soft sets revisited",
    "abstract": "  Soft sets, as a mathematical tool for dealing with uncertainty, have recently\ngained considerable attention, including some successful applications in\ninformation processing, decision, demand analysis, and forecasting. To\nconstruct new soft sets from given soft sets, some operations on soft sets have\nbeen proposed. Unfortunately, such operations cannot keep all classical\nset-theoretic laws true for soft sets. In this paper, we redefine the\nintersection, complement, and difference of soft sets and investigate the\nalgebraic properties of these operations along with a known union operation. We\nfind that the new operation system on soft sets inherits all basic properties\nof operations on classical sets, which justifies our definitions.\n"
  },
  {
    "id": "1205.3054",
    "title": "Approximate Modified Policy Iteration",
    "abstract": "  Modified policy iteration (MPI) is a dynamic programming (DP) algorithm that\ncontains the two celebrated policy and value iteration methods. Despite its\ngenerality, MPI has not been thoroughly studied, especially its approximation\nform which is used when the state and/or action spaces are large or infinite.\nIn this paper, we propose three implementations of approximate MPI (AMPI) that\nare extensions of well-known approximate DP algorithms: fitted-value iteration,\nfitted-Q iteration, and classification-based policy iteration. We provide error\npropagation analyses that unify those for approximate policy and value\niteration. On the last classification-based implementation, we develop a\nfinite-sample analysis that shows that MPI's main parameter allows to control\nthe balance between the estimation error of the classifier and the overall\nvalue function approximation.\n"
  },
  {
    "id": "1205.3964",
    "title": "Machine Recognition of Hand Written Characters using Neural Networks",
    "abstract": "  Even today in Twenty First Century Handwritten communication has its own\nstand and most of the times, in daily life it is globally using as means of\ncommunication and recording the information like to be shared with others.\nChallenges in handwritten characters recognition wholly lie in the variation\nand distortion of handwritten characters, since different people may use\ndifferent style of handwriting, and direction to draw the same shape of the\ncharacters of their known script. This paper demonstrates the nature of\nhandwritten characters, conversion of handwritten data into electronic data,\nand the neural network approach to make machine capable of recognizing hand\nwritten characters.\n"
  },
  {
    "id": "1205.5098",
    "title": "A Simplified Description of Fuzzy TOPSIS",
    "abstract": "  A simplified description of Fuzzy TOPSIS (Technique for Order Preference by\nSimilarity to Ideal Situation) is presented. We have adapted the TOPSIS\ndescription from existing Fuzzy theory literature and distilled the bare\nminimum concepts required for understanding and applying TOPSIS. An example has\nbeen worked out to illustrate the application of TOPSIS for a multi-criteria\ngroup decision making scenario.\n"
  },
  {
    "id": "1205.5866",
    "title": "Approximate Equalities on Rough Intuitionistic Fuzzy Sets and an\n  Analysis of Approximate Equalities",
    "abstract": "  In order to involve user knowledge in determining equality of sets, which may\nnot be equal in the mathematical sense, three types of approximate (rough)\nequalities were introduced by Novotny and Pawlak ([8, 9, 10]). These notions\nwere generalized by Tripathy, Mitra and Ojha ([13]), who introduced the\nconcepts of approximate (rough) equivalences of sets. Rough equivalences\ncapture equality of sets at a higher level than rough equalities. More\nproperties of these concepts were established in [14]. Combining the conditions\nfor the two types of approximate equalities, two more approximate equalities\nwere introduced by Tripathy [12] and a comparative analysis of their relative\nefficiency was provided. In [15], the four types of approximate equalities were\nextended by considering rough fuzzy sets instead of only rough sets. In fact\nthe concepts of leveled approximate equalities were introduced and properties\nwere studied. In this paper we proceed further by introducing and studying the\napproximate equalities based on rough intuitionistic fuzzy sets instead of\nrough fuzzy sets. That is we introduce the concepts of approximate\n(rough)equalities of intuitionistic fuzzy sets and study their properties. We\nprovide some real life examples to show the applications of rough equalities of\nfuzzy sets and rough equalities of intuitionistic fuzzy sets.\n"
  },
  {
    "id": "1206.0259",
    "title": "The Causal Topography of Cognition",
    "abstract": "  The causal structure of cognition can be simulated but not implemented\ncomputationally, just as the causal structure of a comet can be simulated but\nnot implemented computationally. The only thing that allows us even to imagine\notherwise is that cognition, unlike a comet, is invisible (to all but the\ncognizer).\n"
  },
  {
    "id": "1206.0918",
    "title": "Fuzzy Knowledge Representation Based on Possibilistic and Necessary\n  Bayesian Networks",
    "abstract": "  Within the framework proposed in this paper, we address the issue of\nextending the certain networks to a fuzzy certain networks in order to cope\nwith a vagueness and limitations of existing models for decision under\nimprecise and uncertain knowledge. This paper proposes a framework that\ncombines two disciplines to exploit their own advantages in uncertain and\nimprecise knowledge representation problems. The framework proposed is a\npossibilistic logic based one in which Bayesian nodes and their properties are\nrepresented by local necessity-valued knowledge base. Data in properties are\ninterpreted as set of valuated formulas. In our contribution possibilistic\nBayesian networks have a qualitative part and a quantitative part, represented\nby local knowledge bases. The general idea is to study how a fusion of these\ntwo formalisms would permit representing compact way to solve efficiently\nproblems for knowledge representation. We show how to apply possibility and\nnecessity measures to the problem of knowledge representation with large scale\ndata. On the other hand fuzzification of crisp certainty degrees to fuzzy\nvariables improves the quality of the network and tends to bring smoothness and\nrobustness in the network performance. The general aim is to provide a new\napproach for decision under uncertainty that combines three methodologies:\nBayesian networks certainty distribution and fuzzy logic.\n"
  },
  {
    "id": "1206.1061",
    "title": "Use of Fuzzy Sets in Semantic Nets for Providing On-Line Assistance to\n  User of Technological Systems",
    "abstract": "  The main objective of this paper is to develop a new semantic Network\nstructure, based on the fuzzy sets theory, used in Artificial Intelligent\nsystem in order to provide effective on-line assistance to users of new\ntechnological systems. This Semantic Networks is used to describe the knowledge\nof an \"ideal\" expert while fuzzy sets are used both to describe the approximate\nand uncertain knowledge of novice users who intervene to match fuzzy labels of\na query with categories from an \"ideal\" expert. The technical system we\nconsider is a word processor software, with Objects such as \"Word\" and Goals\nsuch as \"Cut\" or \"Copy\". We suggest to consider the set of the system's Goals\nas a set of linguistic variables to which corresponds a set of possible\nlinguistic values based on the fuzzy set. We consider, therefore, a set of\ninterpretation's levels for these possible values to which corresponds a set of\nmembership functions. We also propose a method to measure the similarity degree\nbetween different fuzzy linguistic variables for the partition of the semantic\nnetwork in class of similar objects to make easy the diagnosis of the user's\nfuzzy queries.\n"
  },
  {
    "id": "1206.1291",
    "title": "Feature Weighting for Improving Document Image Retrieval System\n  Performance",
    "abstract": "  Feature weighting is a technique used to approximate the optimal degree of\ninfluence of individual features. This paper presents a feature weighting\nmethod for Document Image Retrieval System (DIRS) based on keyword spotting. In\nthis method, we weight the feature using coefficient of multiple correlations.\nCoefficient of multiple correlations can be used to describe the synthesized\neffects and correlation of each feature. The aim of this paper is to show that\nfeature weighting increases the performance of DIRS. After applying the feature\nweighting method to DIRS the average precision is 93.23% and average recall\nbecome 98.66% respectively\n"
  },
  {
    "id": "1206.1319",
    "title": "Certain Bayesian Network based on Fuzzy knowledge Bases",
    "abstract": "  In this paper, we are trying to examine trade offs between fuzzy logic and\ncertain Bayesian networks and we propose to combine their respective advantages\ninto fuzzy certain Bayesian networks (FCBN), a certain Bayesian networks of\nfuzzy random variables. This paper deals with different definitions and\nclassifications of uncertainty, sources of uncertainty, and theories and\nmethodologies presented to deal with uncertainty. Fuzzification of crisp\ncertainty degrees to fuzzy variables improves the quality of the network and\ntends to bring smoothness and robustness in the network performance. The aim is\nto provide a new approach for decision under uncertainty that combines three\nmethodologies: Bayesian networks certainty distribution and fuzzy logic. Within\nthe framework proposed in this paper, we address the issue of extending the\ncertain networks to a fuzzy certain networks in order to cope with a vagueness\nand limitations of existing models for decision under imprecise and uncertain\nknowledge.\n"
  },
  {
    "id": "1206.1414",
    "title": "An Intelligent Approach for Negotiating between chains in Supply Chain\n  Management Systems",
    "abstract": "  Holding commercial negotiations and selecting the best supplier in supply\nchain management systems are among weaknesses of producers in production\nprocess. Therefore, applying intelligent systems may have an effective role in\nincreased speed and improved quality in the selections .This paper introduces a\nsystem which tries to trade using multi-agents systems and holding negotiations\nbetween any agents. In this system, an intelligent agent is considered for each\nsegment of chains which it tries to send order and receive the response with\nattendance in negotiation medium and communication with other agents .This\npaper introduces how to communicate between agents, characteristics of\nmulti-agent and standard registration medium of each agent in the environment.\nJADE (Java Application Development Environment) was used for implementation and\nsimulation of agents cooperation.\n"
  },
  {
    "id": "1206.1418",
    "title": "A weighted combination similarity measure for mobility patterns in\n  wireless networks",
    "abstract": "  The similarity between trajectory patterns in clustering has played an\nimportant role in discovering movement behaviour of different groups of mobile\nobjects. Several approaches have been proposed to measure the similarity\nbetween sequences in trajectory data. Most of these measures are based on\nEuclidean space or on spatial network and some of them have been concerned with\ntemporal aspect or ordering types. However, they are not appropriate to\ncharacteristics of spatiotemporal mobility patterns in wireless networks. In\nthis paper, we propose a new similarity measure for mobility patterns in\ncellular space of wireless network. The framework for constructing our measure\nis composed of two phases as follows. First, we present formal definitions to\ncapture mathematically two spatial and temporal similarity measures for\nmobility patterns. And then, we define the total similarity measure by means of\na weighted combination of these similarities. The truth of the partial and\ntotal similarity measures are proved in mathematics. Furthermore, instead of\nthe time interval or ordering, our work makes use of the timestamp at which two\nmobility patterns share the same cell. A case study is also described to give a\ncomparison of the combination measure with other ones.\n"
  },
  {
    "id": "1206.1458",
    "title": "Dispelling Classes Gradually to Improve Quality of Feature Reduction\n  Approaches",
    "abstract": "  Feature reduction is an important concept which is used for reducing\ndimensions to decrease the computation complexity and time of classification.\nSince now many approaches have been proposed for solving this problem, but\nalmost all of them just presented a fix output for each input dataset that some\nof them aren't satisfied cases for classification. In this we proposed an\napproach as processing input dataset to increase accuracy rate of each feature\nextraction methods. First of all, a new concept called dispelling classes\ngradually (DCG) is proposed to increase separability of classes based on their\nlabels. Next, this method is used to process input dataset of the feature\nreduction approaches to decrease the misclassification error rate of their\noutputs more than when output is achieved without any processing. In addition\nour method has a good quality to collate with noise based on adapting dataset\nwith feature reduction approaches. In the result part, two conditions (With\nprocess and without that) are compared to support our idea by using some of UCI\ndatasets.\n"
  },
  {
    "id": "1206.1534",
    "title": "Software Aging Analysis of Web Server Using Neural Networks",
    "abstract": "  Software aging is a phenomenon that refers to progressive performance\ndegradation or transient failures or even crashes in long running software\nsystems such as web servers. It mainly occurs due to the deterioration of\noperating system resource, fragmentation and numerical error accumulation. A\nprimitive method to fight against software aging is software rejuvenation.\nSoftware rejuvenation is a proactive fault management technique aimed at\ncleaning up the system internal state to prevent the occurrence of more severe\ncrash failures in the future. It involves occasionally stopping the running\nsoftware, cleaning its internal state and restarting it. An optimized schedule\nfor performing the software rejuvenation has to be derived in advance because a\nlong running application could not be put down now and then as it may lead to\nwaste of cost. This paper proposes a method to derive an accurate and optimized\nschedule for rejuvenation of a web server (Apache) by using Radial Basis\nFunction (RBF) based Feed Forward Neural Network, a variant of Artificial\nNeural Networks (ANN). Aging indicators are obtained through experimental setup\ninvolving Apache web server and clients, which acts as input to the neural\nnetwork model. This method is better than existing ones because usage of RBF\nleads to better accuracy and speed in convergence.\n"
  },
  {
    "id": "1206.1678",
    "title": "A Distributed Optimized Patient Scheduling using Partial Information",
    "abstract": "  A software agent may be a member of a Multi-Agent System (MAS) which is\ncollectively performing a range of complex and intelligent tasks. In the\nhospital, scheduling decisions are finding difficult to schedule because of the\ndynamic changes and distribution. In order to face this problem with dynamic\nchanges in the hospital, a new method, Distributed Optimized Patient Scheduling\nwith Grouping (DOPSG) has been proposed. The goal of this method is that there\nis no necessity for knowing patient agents information globally. With minimal\ninformation this method works effectively. Scheduling problem can be solved for\nmultiple departments in the hospital. Patient agents have been scheduled to the\nresource agent based on the patient priority to reduce the waiting time of\npatient agent and to reduce idle time of resources.\n"
  },
  {
    "id": "1206.1724",
    "title": "Softening Fuzzy Knowledge Representation Tool with the Learning of New\n  Words in Natural Language",
    "abstract": "  The approach described here allows using membership function to represent\nimprecise and uncertain knowledge by learning in Fuzzy Semantic Networks. This\nrepresentation has a great practical interest due to the possibility to realize\non the one hand, the construction of this membership function from a simple\nvalue expressing the degree of interpretation of an Object or a Goal as\ncompared to an other and on the other hand, the adjustment of the membership\nfunction during the apprenticeship. We show, how to use these membership\nfunctions to represent the interpretation of an Object (respectively of a Goal)\nuser as compared to an system Object (respectively to a Goal). We also show the\npossibility to make decision for each representation of an user Object compared\nto a system Object. This decision is taken by determining decision coefficient\ncalculates according to the nucleus of the membership function of the user\nObject.\n"
  },
  {
    "id": "1206.1794",
    "title": "Fuzzy Knowledge Representation, Learning and Optimization with Bayesian\n  Analysis in Fuzzy Semantic Networks",
    "abstract": "  This paper presents a method of optimization, based on both Bayesian Analysis\ntechnical and Gallois Lattice, of a Fuzzy Semantic Networks. The technical\nSystem we use learn by interpreting an unknown word using the links created\nbetween this new word and known words. The main link is provided by the context\nof the query. When novice's query is confused with an unknown verb (goal)\napplied to a known noun denoting either an object in the ideal user's Network\nor an object in the user's Network, the system infer that this new verb\ncorresponds to one of the known goal. With the learning of new words in natural\nlanguage as the interpretation, which was produced in agreement with the user,\nthe system improves its representation scheme at each experiment with a new\nuser and, in addition, takes advantage of previous discussions with users. The\nsemantic Net of user objects thus obtained by these kinds of learning is not\nalways optimal because some relationships between couple of user objects can be\ngeneralized and others suppressed according to values of forces that\ncharacterize them. Indeed, to simplify the obtained Net, we propose to proceed\nto an inductive Bayesian analysis, on the Net obtained from Gallois lattice.\nThe objective of this analysis can be seen as an operation of filtering of the\nobtained descriptive graph.\n"
  },
  {
    "id": "1206.2347",
    "title": "Uncertain and Approximative Knowledge Representation to Reasoning on\n  Classification with a Fuzzy Networks Based System",
    "abstract": "  The approach described here allows to use the fuzzy Object Based\nRepresentation of imprecise and uncertain knowledge. This representation has a\ngreat practical interest due to the possibility to realize reasoning on\nclassification with a fuzzy semantic network based system. For instance, the\ndistinction between necessary, possible and user classes allows to take into\naccount exceptions that may appear on fuzzy knowledge-base and facilitates\nintegration of user's Objects in the base. This approach describes the\ntheoretical aspects of the architecture of the whole experimental A.I. system\nwe built in order to provide effective on-line assistance to users of new\ntechnological systems: the understanding of \"how it works\" and \"how to complete\ntasks\" from queries in quite natural languages. In our model, procedural\nsemantic networks are used to describe the knowledge of an \"ideal\" expert while\nfuzzy sets are used both to describe the approximative and uncertain knowledge\nof novice users in fuzzy semantic networks which intervene to match fuzzy\nlabels of a query with categories from our \"ideal\" expert.\n"
  },
  {
    "id": "1206.3111",
    "title": "The third open Answer Set Programming competition",
    "abstract": "  Answer Set Programming (ASP) is a well-established paradigm of declarative\nprogramming in close relationship with other declarative formalisms such as SAT\nModulo Theories, Constraint Handling Rules, FO(.), PDDL and many others. Since\nits first informal editions, ASP systems have been compared in the now\nwell-established ASP Competition. The Third (Open) ASP Competition, as the\nsequel to the ASP Competitions Series held at the University of Potsdam in\nGermany (2006-2007) and at the University of Leuven in Belgium in 2009, took\nplace at the University of Calabria (Italy) in the first half of 2011.\nParticipants competed on a pre-selected collection of benchmark problems, taken\nfrom a variety of domains as well as real world applications. The Competition\nran on two tracks: the Model and Solve (M&S) Track, based on an open problem\nencoding, and open language, and open to any kind of system based on a\ndeclarative specification paradigm; and the System Track, run on the basis of\nfixed, public problem encodings, written in a standard ASP language. This paper\ndiscusses the format of the Competition and the rationale behind it, then\nreports the results for both tracks. Comparison with the second ASP competition\nand state-of-the-art solutions for some of the benchmark domains is eventually\ndiscussed.\n  To appear in Theory and Practice of Logic Programming (TPLP).\n"
  },
  {
    "id": "1206.3232",
    "title": "AND/OR Importance Sampling",
    "abstract": "  The paper introduces AND/OR importance sampling for probabilistic graphical\nmodels. In contrast to importance sampling, AND/OR importance sampling caches\nsamples in the AND/OR space and then extracts a new sample mean from the stored\nsamples. We prove that AND/OR importance sampling may have lower variance than\nimportance sampling; thereby providing a theoretical justification for\npreferring it over importance sampling. Our empirical evaluation demonstrates\nthat AND/OR importance sampling is far more accurate than importance sampling\nin many cases.\n"
  },
  {
    "id": "1206.3233",
    "title": "Speeding Up Planning in Markov Decision Processes via Automatically\n  Constructed Abstractions",
    "abstract": "  In this paper, we consider planning in stochastic shortest path (SSP)\nproblems, a subclass of Markov Decision Problems (MDP). We focus on medium-size\nproblems whose state space can be fully enumerated. This problem has numerous\nimportant applications, such as navigation and planning under uncertainty. We\npropose a new approach for constructing a multi-level hierarchy of\nprogressively simpler abstractions of the original problem. Once computed, the\nhierarchy can be used to speed up planning by first finding a policy for the\nmost abstract level and then recursively refining it into a solution to the\noriginal problem. This approach is fully automated and delivers a speed-up of\ntwo orders of magnitude over a state-of-the-art MDP solver on sample problems\nwhile returning near-optimal solutions. We also prove theoretical bounds on the\nloss of solution optimality resulting from the use of abstractions.\n"
  },
  {
    "id": "1206.3244",
    "title": "Bayesian network learning by compiling to weighted MAX-SAT",
    "abstract": "  The problem of learning discrete Bayesian networks from data is encoded as a\nweighted MAX-SAT problem and the MaxWalkSat local search algorithm is used to\naddress it. For each dataset, the per-variable summands of the (BDeu) marginal\nlikelihood for different choices of parents ('family scores') are computed\nprior to applying MaxWalkSat. Each permissible choice of parents for each\nvariable is encoded as a distinct propositional atom and the associated family\nscore encoded as a 'soft' weighted single-literal clause. Two approaches to\nenforcing acyclicity are considered: either by encoding the ancestor relation\nor by attaching a total order to each graph and encoding that. The latter\napproach gives better results. Learning experiments have been conducted on 21\nsynthetic datasets sampled from 7 BNs. The largest dataset has 10,000\ndatapoints and 60 variables producing (for the 'ancestor' encoding) a weighted\nCNF input file with 19,932 atoms and 269,367 clauses. For most datasets,\nMaxWalkSat quickly finds BNs with higher BDeu score than the 'true' BN. The\neffect of adding prior information is assessed. It is further shown that\nBayesian model averaging can be effected by collecting BNs generated during the\nsearch.\n"
  },
  {
    "id": "1206.3246",
    "title": "Strategy Selection in Influence Diagrams using Imprecise Probabilities",
    "abstract": "  This paper describes a new algorithm to solve the decision making problem in\nInfluence Diagrams based on algorithms for credal networks. Decision nodes are\nassociated to imprecise probability distributions and a reformulation is\nintroduced that finds the global maximum strategy with respect to the expected\nutility. We work with Limited Memory Influence Diagrams, which generalize most\nInfluence Diagram proposals and handle simultaneous decisions. Besides the\nglobal optimum method, we explore an anytime approximate solution with a\nguaranteed maximum error and show that imprecise probabilities are handled in a\nstraightforward way. Complexity issues and experiments with random diagrams and\nan effects-based military planning problem are discussed.\n"
  },
  {
    "id": "1206.3248",
    "title": "Knowledge Combination in Graphical Multiagent Model",
    "abstract": "  A graphical multiagent model (GMM) represents a joint distribution over the\nbehavior of a set of agents. One source of knowledge about agents' behavior may\ncome from gametheoretic analysis, as captured by several graphical game\nrepresentations developed in recent years. GMMs generalize this approach to\nexpress arbitrary distributions, based on game descriptions or other sources of\nknowledge bearing on beliefs about agent behavior. To illustrate the\nflexibility of GMMs, we exhibit game-derived models that allow probabilistic\ndeviation from equilibrium, as well as models based on heuristic action choice.\nWe investigate three different methods of integrating these models into a\nsingle model representing the combined knowledge sources. To evaluate the\npredictive performance of the combined model, we treat as actual outcome the\nbehavior produced by a reinforcement learning process. We find that combining\nthe two knowledge sources, using any of the methods, provides better\npredictions than either source alone. Among the combination methods, mixing\ndata outperforms the opinion pool and direct update methods investigated in\nthis empirical trial.\n"
  },
  {
    "id": "1206.3250",
    "title": "Almost Optimal Intervention Sets for Causal Discovery",
    "abstract": "  We conjecture that the worst case number of experiments necessary and\nsufficient to discover a causal graph uniquely given its observational Markov\nequivalence class can be specified as a function of the largest clique in the\nMarkov equivalence class. We provide an algorithm that computes intervention\nsets that we believe are optimal for the above task. The algorithm builds on\ninsights gained from the worst case analysis in Eberhardt et al. (2005) for\nsequences of experiments when all possible directed acyclic graphs over N\nvariables are considered. A simulation suggests that our conjecture is correct.\nWe also show that a generalization of our conjecture to other classes of\npossible graph hypotheses cannot be given easily, and in what sense the\nalgorithm is then no longer optimal.\n"
  },
  {
    "id": "1206.3263",
    "title": "Sparse Stochastic Finite-State Controllers for POMDPs",
    "abstract": "  Bounded policy iteration is an approach to solving infinite-horizon POMDPs\nthat represents policies as stochastic finite-state controllers and iteratively\nimproves a controller by adjusting the parameters of each node using linear\nprogramming. In the original algorithm, the size of the linear programs, and\nthus the complexity of policy improvement, depends on the number of parameters\nof each node, which grows with the size of the controller. But in practice, the\nnumber of parameters of a node with non-zero values is often very small, and\ndoes not grow with the size of the controller. Based on this observation, we\ndevelop a version of bounded policy iteration that leverages the sparse\nstructure of a stochastic finite-state controller. In each iteration, it\nimproves a policy by the same amount as the original algorithm, but with much\nbetter scalability.\n"
  },
  {
    "id": "1206.3264",
    "title": "Sampling First Order Logical Particles",
    "abstract": "  Approximate inference in dynamic systems is the problem of estimating the\nstate of the system given a sequence of actions and partial observations. High\nprecision estimation is fundamental in many applications like diagnosis,\nnatural language processing, tracking, planning, and robotics. In this paper we\npresent an algorithm that samples possible deterministic executions of a\nprobabilistic sequence. The algorithm takes advantage of a compact\nrepresentation (using first order logic) for actions and world states to\nimprove the precision of its estimation. Theoretical and empirical results show\nthat the algorithm's expected error is smaller than propositional sampling and\nSequential Monte Carlo (SMC) sampling techniques.\n"
  },
  {
    "id": "1206.3265",
    "title": "The Computational Complexity of Sensitivity Analysis and Parameter\n  Tuning",
    "abstract": "  While known algorithms for sensitivity analysis and parameter tuning in\nprobabilistic networks have a running time that is exponential in the size of\nthe network, the exact computational complexity of these problems has not been\nestablished as yet. In this paper we study several variants of the tuning\nproblem and show that these problems are NPPP-complete in general. We further\nshow that the problems remain NP-complete or PP-complete, for a number of\nrestricted variants. These complexity results provide insight in whether or not\nrecent achievements in sensitivity analysis and tuning can be extended to more\ngeneral, practicable methods.\n"
  },
  {
    "id": "1206.3266",
    "title": "Partitioned Linear Programming Approximations for MDPs",
    "abstract": "  Approximate linear programming (ALP) is an efficient approach to solving\nlarge factored Markov decision processes (MDPs). The main idea of the method is\nto approximate the optimal value function by a set of basis functions and\noptimize their weights by linear programming (LP). This paper proposes a new\nALP approximation. Comparing to the standard ALP formulation, we decompose the\nconstraint space into a set of low-dimensional spaces. This structure allows\nfor solving the new LP efficiently. In particular, the constraints of the LP\ncan be satisfied in a compact form without an exponential dependence on the\ntreewidth of ALP constraints. We study both practical and theoretical aspects\nof the proposed approach. Moreover, we demonstrate its scale-up potential on an\nMDP with more than 2^100 states.\n"
  },
  {
    "id": "1206.3271",
    "title": "Learning Arithmetic Circuits",
    "abstract": "  Graphical models are usually learned without regard to the cost of doing\ninference with them. As a result, even if a good model is learned, it may\nperform poorly at prediction, because it requires approximate inference. We\npropose an alternative: learning models with a score function that directly\npenalizes the cost of inference. Specifically, we learn arithmetic circuits\nwith a penalty on the number of edges in the circuit (in which the cost of\ninference is linear). Our algorithm is equivalent to learning a Bayesian\nnetwork with context-specific independence by greedily splitting conditional\ndistributions, at each step scoring the candidates by compiling the resulting\nnetwork into an arithmetic circuit, and using its size as the penalty. We show\nhow this can be done efficiently, without compiling a circuit from scratch for\neach candidate. Experiments on several real-world domains show that our\nalgorithm is able to learn tractable models with very large treewidth, and\nyields more accurate predictions than a standard context-specific Bayesian\nnetwork learner, in far less time.\n"
  },
  {
    "id": "1206.3272",
    "title": "Improving Gradient Estimation by Incorporating Sensor Data",
    "abstract": "  An efficient policy search algorithm should estimate the local gradient of\nthe objective function, with respect to the policy parameters, from as few\ntrials as possible. Whereas most policy search methods estimate this gradient\nby observing the rewards obtained during policy trials, we show, both\ntheoretically and empirically, that taking into account the sensor data as well\ngives better gradient estimates and hence faster learning. The reason is that\nrewards obtained during policy execution vary from trial to trial due to noise\nin the environment; sensor data, which correlates with the noise, can be used\nto partially correct for this variation, resulting in an estimatorwith lower\nvariance.\n"
  },
  {
    "id": "1206.3276",
    "title": "Explanation Trees for Causal Bayesian Networks",
    "abstract": "  Bayesian networks can be used to extract explanations about the observed\nstate of a subset of variables. In this paper, we explicate the desiderata of\nan explanation and confront them with the concept of explanation proposed by\nexisting methods. The necessity of taking into account causal approaches when a\ncausal graph is available is discussed. We then introduce causal explanation\ntrees, based on the construction of explanation trees using the measure of\ncausal information ow (Ay and Polani, 2006). This approach is compared to\nseveral other methods on known networks.\n"
  },
  {
    "id": "1206.3281",
    "title": "Model-Based Bayesian Reinforcement Learning in Large Structured Domains",
    "abstract": "  Model-based Bayesian reinforcement learning has generated significant\ninterest in the AI community as it provides an elegant solution to the optimal\nexploration-exploitation tradeoff in classical reinforcement learning.\nUnfortunately, the applicability of this type of approach has been limited to\nsmall domains due to the high complexity of reasoning about the joint posterior\nover model parameters. In this paper, we consider the use of factored\nrepresentations combined with online planning techniques, to improve\nscalability of these methods. The main contribution of this paper is a Bayesian\nframework for learning the structure and parameters of a dynamical system,\nwhile also simultaneously planning a (near-)optimal sequence of actions.\n"
  },
  {
    "id": "1206.3282",
    "title": "Improving the Accuracy and Efficiency of MAP Inference for Markov Logic",
    "abstract": "  In this work we present Cutting Plane Inference (CPI), a Maximum A Posteriori\n(MAP) inference method for Statistical Relational Learning. Framed in terms of\nMarkov Logic and inspired by the Cutting Plane Method, it can be seen as a meta\nalgorithm that instantiates small parts of a large and complex Markov Network\nand then solves these using a conventional MAP method. We evaluate CPI on two\ntasks, Semantic Role Labelling and Joint Entity Resolution, while plugging in\ntwo different MAP inference methods: the current method of choice for MAP\ninference in Markov Logic, MaxWalkSAT, and Integer Linear Programming. We\nobserve that when used with CPI both methods are significantly faster than when\nused alone. In addition, CPI improves the accuracy of MaxWalkSAT and maintains\nthe exactness of Integer Linear Programming.\n"
  },
  {
    "id": "1206.3283",
    "title": "Observation Subset Selection as Local Compilation of Performance\n  Profiles",
    "abstract": "  Deciding what to sense is a crucial task, made harder by dependencies and by\na nonadditive utility function. We develop approximation algorithms for\nselecting an optimal set of measurements, under a dependency structure modeled\nby a tree-shaped Bayesian network (BN). Our approach is a generalization of\ncomposing anytime algorithm represented by conditional performance profiles.\nThis is done by relaxing the input monotonicity assumption, and extending the\nlocal compilation technique to more general classes of performance profiles\n(PPs). We apply the extended scheme to selecting a subset of measurements for\nchoosing a maximum expectation variable in a binary valued BN, and for\nminimizing the worst variance in a Gaussian BN.\n"
  },
  {
    "id": "1206.3284",
    "title": "Bounding Search Space Size via (Hyper)tree Decompositions",
    "abstract": "  This paper develops a measure for bounding the performance of AND/OR search\nalgorithms for solving a variety of queries over graphical models. We show how\ndrawing a connection to the recent notion of hypertree decompositions allows to\nexploit determinism in the problem specification and produce tighter bounds. We\ndemonstrate on a variety of practical problem instances that we are often able\nto improve upon existing bounds by several orders of magnitude.\n"
  },
  {
    "id": "1206.3286",
    "title": "New Techniques for Algorithm Portfolio Design",
    "abstract": "  We present and evaluate new techniques for designing algorithm portfolios. In\nour view, the problem has both a scheduling aspect and a machine learning\naspect. Prior work has largely addressed one of the two aspects in isolation.\nBuilding on recent work on the scheduling aspect of the problem, we present a\ntechnique that addresses both aspects simultaneously and has attractive\ntheoretical guarantees. Experimentally, we show that this technique can be used\nto improve the performance of state-of-the-art algorithms for Boolean\nsatisfiability, zero-one integer programming, and A.I. planning.\n"
  },
  {
    "id": "1206.3289",
    "title": "Efficient inference in persistent Dynamic Bayesian Networks",
    "abstract": "  Numerous temporal inference tasks such as fault monitoring and anomaly\ndetection exhibit a persistence property: for example, if something breaks, it\nstays broken until an intervention. When modeled as a Dynamic Bayesian Network,\npersistence adds dependencies between adjacent time slices, often making exact\ninference over time intractable using standard inference algorithms. However,\nwe show that persistence implies a regular structure that can be exploited for\nefficient inference. We present three successively more general classes of\nmodels: persistent causal chains (PCCs), persistent causal trees (PCTs) and\npersistent polytrees (PPTs), and the corresponding exact inference algorithms\nthat exploit persistence. We show that analytic asymptotic bounds for our\nalgorithms compare favorably to junction tree inference; and we demonstrate\nempirically that we can perform exact smoothing on the order of 100 times\nfaster than the approximate Boyen-Koller method on randomly generated instances\nof persistent tree models. We also show how to handle non-persistent variables\nand how persistence can be exploited effectively for approximate filtering.\n"
  },
  {
    "id": "1206.3291",
    "title": "Hierarchical POMDP Controller Optimization by Likelihood Maximization",
    "abstract": "  Planning can often be simpli ed by decomposing the task into smaller tasks\narranged hierarchically. Charlin et al. [4] recently showed that the hierarchy\ndiscovery problem can be framed as a non-convex optimization problem. However,\nthe inherent computational di culty of solving such an optimization problem\nmakes it hard to scale to realworld problems. In another line of research,\nToussaint et al. [18] developed a method to solve planning problems by\nmaximumlikelihood estimation. In this paper, we show how the hierarchy\ndiscovery problem in partially observable domains can be tackled using a\nsimilar maximum likelihood approach. Our technique rst transforms the problem\ninto a dynamic Bayesian network through which a hierarchical structure can\nnaturally be discovered while optimizing the policy. Experimental results\ndemonstrate that this approach scales better than previous techniques based on\nnon-convex optimization.\n"
  },
  {
    "id": "1206.3292",
    "title": "Identifying Dynamic Sequential Plans",
    "abstract": "  We address the problem of identifying dynamic sequential plans in the\nframework of causal Bayesian networks, and show that the problem is reduced to\nidentifying causal effects, for which there are complete identi cation\nalgorithms available in the literature.\n"
  },
  {
    "id": "1206.3295",
    "title": "Refractor Importance Sampling",
    "abstract": "  In this paper we introduce Refractor Importance Sampling (RIS), an\nimprovement to reduce error variance in Bayesian network importance sampling\npropagation under evidential reasoning. We prove the existence of a collection\nof importance functions that are close to the optimal importance function under\nevidential reasoning. Based on this theoretic result we derive the RIS\nalgorithm. RIS approaches the optimal importance function by applying localized\narc changes to minimize the divergence between the evidence-adjusted importance\nfunction and the optimal importance function. The validity and performance of\nRIS is empirically tested with a large setof synthetic Bayesian networks and\ntwo real-world networks.\n"
  },
  {
    "id": "1206.3296",
    "title": "Inference for Multiplicative Models",
    "abstract": "  The paper introduces a generalization for known probabilistic models such as\nlog-linear and graphical models, called here multiplicative models. These\nmodels, that express probabilities via product of parameters are shown to\ncapture multiple forms of contextual independence between variables, including\ndecision graphs and noisy-OR functions. An inference algorithm for\nmultiplicative models is provided and its correctness is proved. The complexity\nanalysis of the inference algorithm uses a more refined parameter than the\ntree-width of the underlying graph, and shows the computational cost does not\nexceed that of the variable elimination algorithm in graphical models. The\npaper ends with examples where using the new models and algorithm is\ncomputationally beneficial.\n"
  },
  {
    "id": "1206.3318",
    "title": "On Local Regret",
    "abstract": "  Online learning aims to perform nearly as well as the best hypothesis in\nhindsight. For some hypothesis classes, though, even finding the best\nhypothesis offline is challenging. In such offline cases, local search\ntechniques are often employed and only local optimality guaranteed. For online\ndecision-making with such hypothesis classes, we introduce local regret, a\ngeneralization of regret that aims to perform nearly as well as only nearby\nhypotheses. We then present a general algorithm to minimize local regret with\narbitrary locality graphs. We also show how the graph structure can be\nexploited to drastically speed learning. These algorithms are then demonstrated\non a diverse set of online problems: online disjunct learning, online Max-SAT,\nand online decision tree learning.\n"
  },
  {
    "id": "1206.3536",
    "title": "Identifying Independence in Relational Models",
    "abstract": "  The rules of d-separation provide a framework for deriving conditional\nindependence facts from model structure. However, this theory only applies to\nsimple directed graphical models. We introduce relational d-separation, a\ntheory for deriving conditional independence in relational models. We provide a\nsound, complete, and computationally efficient method for relational\nd-separation, and we present empirical results that demonstrate effectiveness.\n"
  },
  {
    "id": "1206.3551",
    "title": "Sensitivity analysis in decision circuits",
    "abstract": "  Decision circuits have been developed to perform efficient evaluation of\ninfluence diagrams [Bhattacharjya and Shachter, 2007], building on the advances\nin arithmetic circuits for belief network inference [Darwiche,2003]. In the\nprocess of model building and analysis, we perform sensitivity analysis to\nunderstand how the optimal solution changes in response to changes in the\nmodel. When sequential decision problems under uncertainty are represented as\ndecision circuits, we can exploit the efficient solution process embodied in\nthe decision circuit and the wealth of derivative information available to\ncompute the value of information for the uncertainties in the problem and the\neffects of changes to model parameters on the value and the optimal strategy.\n"
  },
  {
    "id": "1206.3959",
    "title": "Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial\n  Intelligence (2009)",
    "abstract": "  This is the Proceedings of the Twenty-Fifth Conference on Uncertainty in\nArtificial Intelligence, which was held in Montreal, QC, Canada, June 18 - 21\n2009.\n"
  },
  {
    "id": "1206.5242",
    "title": "Studies in Lower Bounding Probabilities of Evidence using the Markov\n  Inequality",
    "abstract": "  Computing the probability of evidence even with known error bounds is\nNP-hard. In this paper we address this hard problem by settling on an easier\nproblem. We propose an approximation which provides high confidence lower\nbounds on probability of evidence but does not have any guarantees in terms of\nrelative or absolute error. Our proposed approximation is a randomized\nimportance sampling scheme that uses the Markov inequality. However, a\nstraight-forward application of the Markov inequality may lead to poor lower\nbounds. We therefore propose several heuristic measures to improve its\nperformance in practice. Empirical evaluation of our scheme with state-of-\nthe-art lower bounding schemes reveals the promise of our approach.\n"
  },
  {
    "id": "1206.5244",
    "title": "Search for Choquet-optimal paths under uncertainty",
    "abstract": "  Choquet expected utility (CEU) is one of the most sophisticated decision\ncriteria used in decision theory under uncertainty. It provides a\ngeneralisation of expected utility enhancing both descriptive and prescriptive\npossibilities. In this paper, we investigate the use of CEU for path-planning\nunder uncertainty with a special focus on robust solutions. We first recall the\nmain features of the CEU model and introduce some examples showing its\ndescriptive potential. Then we focus on the search for Choquet-optimal paths in\nmultivalued implicit graphs where costs depend on different scenarios. After\ndiscussing complexity issues, we propose two different heuristic search\nalgorithms to solve the problem. Finally, numerical experiments are reported,\nshowing the practical efficiency of the proposed algorithms.\n"
  },
  {
    "id": "1206.5249",
    "title": "Learning Probabilistic Relational Dynamics for Multiple Tasks",
    "abstract": "  The ways in which an agent's actions affect the world can often be modeled\ncompactly using a set of relational probabilistic planning rules. This paper\naddresses the problem of learning such rule sets for multiple related tasks. We\ntake a hierarchical Bayesian approach, in which the system learns a prior\ndistribution over rule sets. We present a class of prior distributions\nparameterized by a rule set prototype that is stochastically modified to\nproduce a task-specific rule set. We also describe a coordinate ascent\nalgorithm that iteratively optimizes the task-specific rule sets and the prior\ndistribution. Experiments using this algorithm show that transferring\ninformation from related tasks significantly reduces the amount of training\ndata required to predict action effects in blocks-world domains.\n"
  },
  {
    "id": "1206.5251",
    "title": "Node Splitting: A Scheme for Generating Upper Bounds in Bayesian\n  Networks",
    "abstract": "  We formulate in this paper the mini-bucket algorithm for approximate\ninference in terms of exact inference on an approximate model produced by\nsplitting nodes in a Bayesian network. The new formulation leads to a number of\ntheoretical and practical implications. First, we show that branchand- bound\nsearch algorithms that use minibucket bounds may operate in a drastically\nreduced search space. Second, we show that the proposed formulation inspires\nnew minibucket heuristics and allows us to analyze existing heuristics from a\nnew perspective. Finally, we show that this new formulation allows mini-bucket\napproximations to benefit from recent advances in exact inference, allowing one\nto significantly increase the reach of these approximations.\n"
  },
  {
    "id": "1206.5255",
    "title": "Minimax regret based elicitation of generalized additive utilities",
    "abstract": "  We describe the semantic foundations for elicitation of generalized\nadditively independent (GAI) utilities using the minimax regret criterion, and\npropose several new query types and strategies for this purpose. Computational\nfeasibility is obtained by exploiting the local GAI structure in the model. Our\nresults provide a practical approach for implementing preference-based\nconstrained configuration optimization as well as effective search in\nmultiattribute product databases.\n"
  },
  {
    "id": "1206.5257",
    "title": "Evaluating influence diagrams with decision circuits",
    "abstract": "  Although a number of related algorithms have been developed to evaluate\ninfluence diagrams, exploiting the conditional independence in the diagram, the\nexact solution has remained intractable for many important problems. In this\npaper we introduce decision circuits as a means to exploit the local structure\nusually found in decision problems and to improve the performance of influence\ndiagram analysis. This work builds on the probabilistic inference algorithms\nusing arithmetic circuits to represent Bayesian belief networks [Darwiche,\n2003]. Once compiled, these arithmetic circuits efficiently evaluate\nprobabilistic queries on the belief network, and methods have been developed to\nexploit both the global and local structure of the network. We show that\ndecision circuits can be constructed in a similar fashion and promise similar\nbenefits.\n"
  },
  {
    "id": "1206.5258",
    "title": "Optimizing Memory-Bounded Controllers for Decentralized POMDPs",
    "abstract": "  We present a memory-bounded optimization approach for solving\ninfinite-horizon decentralized POMDPs. Policies for each agent are represented\nby stochastic finite state controllers. We formulate the problem of optimizing\nthese policies as a nonlinear program, leveraging powerful existing nonlinear\noptimization techniques for solving the problem. While existing solvers only\nguarantee locally optimal solutions, we show that our formulation produces\nhigher quality controllers than the state-of-the-art approach. We also\nincorporate a shared source of randomness in the form of a correlation device\nto further increase solution quality with only a limited increase in space and\ntime. Our experimental results show that nonlinear optimization can be used to\nprovide high quality, concise solutions to decentralized decision problems\nunder uncertainty.\n"
  },
  {
    "id": "1206.5260",
    "title": "Reasoning at the Right Time Granularity",
    "abstract": "  Most real-world dynamic systems are composed of different components that\noften evolve at very different rates. In traditional temporal graphical models,\nsuch as dynamic Bayesian networks, time is modeled at a fixed granularity,\ngenerally selected based on the rate at which the fastest component evolves.\nInference must then be performed at this fastest granularity, potentially at\nsignificant computational cost. Continuous Time Bayesian Networks (CTBNs) avoid\ntime-slicing in the representation by modeling the system as evolving\ncontinuously over time. The expectation-propagation (EP) inference algorithm of\nNodelman et al. (2005) can then vary the inference granularity over time, but\nthe granularity is uniform across all parts of the system, and must be selected\nin advance. In this paper, we provide a new EP algorithm that utilizes a\ngeneral cluster graph architecture where clusters contain distributions that\ncan overlap in both space (set of variables) and time. This architecture allows\ndifferent parts of the system to be modeled at very different time\ngranularities, according to their current rate of evolution. We also provide an\ninformation-theoretic criterion for dynamically re-partitioning the clusters\nduring inference to tune the level of approximation to the current rate of\nevolution. This avoids the need to hand-select the appropriate granularity, and\nallows the granularity to adapt as information is transmitted across the\nnetwork. We present experiments demonstrating that this approach can result in\nsignificant computational savings.\n"
  },
  {
    "id": "1206.5266",
    "title": "AND/OR Multi-Valued Decision Diagrams (AOMDDs) for Weighted Graphical\n  Models",
    "abstract": "  Compiling graphical models has recently been under intense investigation,\nespecially for probabilistic modeling and processing. We present here a novel\ndata structure for compiling weighted graphical models (in particular,\nprobabilistic models), called AND/OR Multi-Valued Decision Diagram (AOMDD).\nThis is a generalization of our previous work on constraint networks, to\nweighted models. The AOMDD is based on the frameworks of AND/OR search spaces\nfor graphical models, and Ordered Binary Decision Diagrams (OBDD). The AOMDD is\na canonical representation of a graphical model, and its size and compilation\ntime are bounded exponentially by the treewidth of the graph, rather than\npathwidth as is known for OBDDs. We discuss a Variable Elimination schedule for\ncompilation, and present the general APPLY algorithm that combines two weighted\nAOMDDs, and also present a search based method for compilation method. The\npreliminary experimental evaluation is quite encouraging, showing the potential\nof the AOMDD data structure.\n"
  },
  {
    "id": "1206.5268",
    "title": "Best-First AND/OR Search for Most Probable Explanations",
    "abstract": "  The paper evaluates the power of best-first search over AND/OR search spaces\nfor solving the Most Probable Explanation (MPE) task in Bayesian networks. The\nmain virtue of the AND/OR representation of the search space is its sensitivity\nto the structure of the problem, which can translate into significant time\nsavings. In recent years depth-first AND/OR Branch-and- Bound algorithms were\nshown to be very effective when exploring such search spaces, especially when\nusing caching. Since best-first strategies are known to be superior to\ndepth-first when memory is utilized, exploring the best-first control strategy\nis called for. The main contribution of this paper is in showing that a recent\nextension of AND/OR search algorithms from depth-first Branch-and-Bound to\nbest-first is indeed very effective for computing the MPE in Bayesian networks.\nWe demonstrate empirically the superiority of the best-first search approach on\nvarious probabilistic networks.\n"
  },
  {
    "id": "1206.5271",
    "title": "Learning Bayesian Network Structure from Correlation-Immune Data",
    "abstract": "  Searching the complete space of possible Bayesian networks is intractable for\nproblems of interesting size, so Bayesian network structure learning\nalgorithms, such as the commonly used Sparse Candidate algorithm, employ\nheuristics. However, these heuristics also restrict the types of relationships\nthat can be learned exclusively from data. They are unable to learn\nrelationships that exhibit \"correlation-immunity\", such as parity. To learn\nBayesian networks in the presence of correlation-immune relationships, we\nextend the Sparse Candidate algorithm with a technique called \"skewing\". This\ntechnique uses the observation that relationships that are correlation-immune\nunder a specific input distribution may not be correlation-immune under\nanother, sufficiently different distribution. We show that by extending Sparse\nCandidate with this technique we are able to discover relationships between\nrandom variables that are approximately correlation-immune, with a\nsignificantly lower computational cost than the alternative of considering\nmultiple parents of a node at a time.\n"
  },
  {
    "id": "1206.5273",
    "title": "Survey Propagation Revisited",
    "abstract": "  Survey propagation (SP) is an exciting new technique that has been remarkably\nsuccessful at solving very large hard combinatorial problems, such as\ndetermining the satisfiability of Boolean formulas. In a promising attempt at\nunderstanding the success of SP, it was recently shown that SP can be viewed as\na form of belief propagation, computing marginal probabilities over certain\nobjects called covers of a formula. This explanation was, however, shortly\ndismissed by experiments suggesting that non-trivial covers simply do not exist\nfor large formulas. In this paper, we show that these experiments were\nmisleading: not only do covers exist for large hard random formulas, SP is\nsurprisingly accurate at computing marginals over these covers despite the\nexistence of many cycles in the formulas. This re-opens a potentially simpler\nline of reasoning for understanding SP, in contrast to some alternative lines\nof explanation that have been proposed assuming covers do not exist.\n"
  },
  {
    "id": "1206.5276",
    "title": "Template Based Inference in Symmetric Relational Markov Random Fields",
    "abstract": "  Relational Markov Random Fields are a general and flexible framework for\nreasoning about the joint distribution over attributes of a large number of\ninteracting entities. The main computational difficulty in learning such models\nis inference. Even when dealing with complete data, where one can summarize a\nlarge domain by sufficient statistics, learning requires one to compute the\nexpectation of the sufficient statistics given different parameter choices. The\ntypical solution to this problem is to resort to approximate inference\nprocedures, such as loopy belief propagation. Although these procedures are\nquite efficient, they still require computation that is on the order of the\nnumber of interactions (or features) in the model. When learning a large\nrelational model over a complex domain, even such approximations require\nunrealistic running time. In this paper we show that for a particular class of\nrelational MRFs, which have inherent symmetry, we can perform the inference\nneeded for learning procedures using a template-level belief propagation. This\nprocedure's running time is proportional to the size of the relational model\nrather than the size of the domain. Moreover, we show that this computational\nprocedure is equivalent to sychronous loopy belief propagation. This enables a\ndramatic speedup in inference and learning time. We use this procedure to learn\nrelational MRFs for capturing the joint distribution of large protein-protein\ninteraction networks.\n"
  },
  {
    "id": "1206.5284",
    "title": "More-or-Less CP-Networks",
    "abstract": "  Preferences play an important role in our everyday lives. CP-networks, or\nCP-nets in short, are graphical models for representing conditional qualitative\npreferences under ceteris paribus (\"all else being equal\") assumptions. Despite\ntheir intuitive nature and rich representation, dominance testing with CP-nets\nis computationally complex, even when the CP-nets are restricted to\nbinary-valued preferences. Tractable algorithms exist for binary CP-nets, but\nthese algorithms are incomplete for multi-valued CPnets. In this paper, we\nidentify a class of multivalued CP-nets, which we call more-or-less CPnets,\nthat have the same computational complexity as binary CP-nets. More-or-less\nCP-nets exploit the monotonicity of the attribute values and use intervals to\naggregate values that induce similar preferences. We then present a search\ncontrol rule for dominance testing that effectively prunes the search space\nwhile preserving completeness.\n"
  },
  {
    "id": "1206.5287",
    "title": "Policy Iteration for Relational MDPs",
    "abstract": "  Relational Markov Decision Processes are a useful abstraction for complex\nreinforcement learning problems and stochastic planning problems. Recent work\ndeveloped representation schemes and algorithms for planning in such problems\nusing the value iteration algorithm. However, exact versions of more complex\nalgorithms, including policy iteration, have not been developed or analyzed.\nThe paper investigates this potential and makes several contributions. First we\nobserve two anomalies for relational representations showing that the value of\nsome policies is not well defined or cannot be calculated for restricted\nrepresentation schemes used in the literature. On the other hand, we develop a\nvariant of policy iteration that can get around these anomalies. The algorithm\nincludes an aspect of policy improvement in the process of policy evaluation\nand thus differs from the original algorithm. We show that despite this\ndifference the algorithm converges to the optimal policy.\n"
  },
  {
    "id": "1206.5292",
    "title": "Markov Logic in Infinite Domains",
    "abstract": "  Combining first-order logic and probability has long been a goal of AI.\nMarkov logic (Richardson & Domingos, 2006) accomplishes this by attaching\nweights to first-order formulas and viewing them as templates for features of\nMarkov networks. Unfortunately, it does not have the full power of first-order\nlogic, because it is only defined for finite domains. This paper extends Markov\nlogic to infinite domains, by casting it in the framework of Gibbs measures\n(Georgii, 1988). We show that a Markov logic network (MLN) admits a Gibbs\nmeasure as long as each ground atom has a finite number of neighbors. Many\ninteresting cases fall in this category. We also show that an MLN admits a\nunique measure if the weights of its non-unit clauses are small enough. We then\nexamine the structure of the set of consistent measures in the non-unique case.\nMany important phenomena, including systems with phase transitions, are\nrepresented by MLNs with non-unique measures. We relate the problem of\nsatisfiability in first-order logic to the properties of MLN measures, and\ndiscuss how Markov logic relates to previous infinite models.\n"
  },
  {
    "id": "1206.5294",
    "title": "What Counterfactuals Can Be Tested",
    "abstract": "  Counterfactual statements, e.g., \"my headache would be gone had I taken an\naspirin\" are central to scientific discourse, and are formally interpreted as\nstatements derived from \"alternative worlds\". However, since they invoke\nhypothetical states of affairs, often incompatible with what is actually known\nor observed, testing counterfactuals is fraught with conceptual and practical\ndifficulties. In this paper, we provide a complete characterization of\n\"testable counterfactuals,\" namely, counterfactual statements whose\nprobabilities can be inferred from physical experiments. We provide complete\nprocedures for discerning whether a given counterfactual is testable and, if\nso, expressing its probability in terms of experimental data.\n"
  },
  {
    "id": "1206.5295",
    "title": "Improved Memory-Bounded Dynamic Programming for Decentralized POMDPs",
    "abstract": "  Memory-Bounded Dynamic Programming (MBDP) has proved extremely effective in\nsolving decentralized POMDPs with large horizons. We generalize the algorithm\nand improve its scalability by reducing the complexity with respect to the\nnumber of observations from exponential to polynomial. We derive error bounds\non solution quality with respect to this new approximation and analyze the\nconvergence behavior. To evaluate the effectiveness of the improvements, we\nintroduce a new, larger benchmark problem. Experimental results show that\ndespite the high complexity of decentralized POMDPs, scalable solution\ntechniques such as MBDP perform surprisingly well.\n"
  },
  {
    "id": "1206.5698",
    "title": "Relational Approach to Knowledge Engineering for POMDP-based Assistance\n  Systems as a Translation of a Psychological Model",
    "abstract": "  Assistive systems for persons with cognitive disabilities (e.g. dementia) are\ndifficult to build due to the wide range of different approaches people can\ntake to accomplishing the same task, and the significant uncertainties that\narise from both the unpredictability of client's behaviours and from noise in\nsensor readings. Partially observable Markov decision process (POMDP) models\nhave been used successfully as the reasoning engine behind such assistive\nsystems for small multi-step tasks such as hand washing. POMDP models are a\npowerful, yet flexible framework for modelling assistance that can deal with\nuncertainty and utility. Unfortunately, POMDPs usually require a very labour\nintensive, manual procedure for their definition and construction. Our previous\nwork has described a knowledge driven method for automatically generating POMDP\nactivity recognition and context sensitive prompting systems for complex tasks.\nWe call the resulting POMDP a SNAP (SyNdetic Assistance Process). The\nspreadsheet-like result of the analysis does not correspond to the POMDP model\ndirectly and the translation to a formal POMDP representation is required. To\ndate, this translation had to be performed manually by a trained POMDP expert.\nIn this paper, we formalise and automate this translation process using a\nprobabilistic relational model (PRM) encoded in a relational database. We\ndemonstrate the method by eliciting three assistance tasks from non-experts. We\nvalidate the resulting POMDP models using case-based simulations to show that\nthey are reasonable for the domains. We also show a complete case study of a\ndesigner specifying one database, including an evaluation in a real-life\nexperiment with a human actor.\n"
  },
  {
    "id": "1206.5833",
    "title": "Revision of Defeasible Logic Preferences",
    "abstract": "  There are several contexts of non-monotonic reasoning where a priority\nbetween rules is established whose purpose is preventing conflicts.\n  One formalism that has been widely employed for non-monotonic reasoning is\nthe sceptical one known as Defeasible Logic. In Defeasible Logic the tool used\nfor conflict resolution is a preference relation between rules, that\nestablishes the priority among them.\n  In this paper we investigate how to modify such a preference relation in a\ndefeasible logic theory in order to change the conclusions of the theory\nitself. We argue that the approach we adopt is applicable to legal reasoning\nwhere users, in general, cannot change facts or rules, but can propose their\npreferences about the relative strength of the rules.\n  We provide a comprehensive study of the possible combinatorial cases and we\nidentify and analyse the cases where the revision process is successful.\n  After this analysis, we identify three revision/update operators and study\nthem against the AGM postulates for belief revision operators, to discover that\nonly a part of these postulates are satisfied by the three operators.\n"
  },
  {
    "id": "1206.5928",
    "title": "CAPIR: Collaborative Action Planning with Intention Recognition",
    "abstract": "  We apply decision theoretic techniques to construct non-player characters\nthat are able to assist a human player in collaborative games. The method is\nbased on solving Markov decision processes, which can be difficult when the\ngame state is described by many variables. To scale to more complex games, the\nmethod allows decomposition of a game task into subtasks, each of which can be\nmodelled by a Markov decision process. Intention recognition is used to infer\nthe subtask that the human is currently performing, allowing the helper to\nassist the human in performing the correct task. Experiments show that the\nmethod can be effective, giving near-human level performance in helping a human\nin a collaborative game.\n"
  },
  {
    "id": "1206.5940",
    "title": "Bootstrapping Monte Carlo Tree Search with an Imperfect Heuristic",
    "abstract": "  We consider the problem of using a heuristic policy to improve the value\napproximation by the Upper Confidence Bound applied in Trees (UCT) algorithm in\nnon-adversarial settings such as planning with large-state space Markov\nDecision Processes. Current improvements to UCT focus on either changing the\naction selection formula at the internal nodes or the rollout policy at the\nleaf nodes of the search tree. In this work, we propose to add an auxiliary arm\nto each of the internal nodes, and always use the heuristic policy to roll out\nsimulations at the auxiliary arms. The method aims to get fast convergence to\noptimal values at states where the heuristic policy is optimal, while retaining\nsimilar approximation as the original UCT in other states. We show that\nbootstrapping with the proposed method in the new algorithm, UCT-Aux, performs\nbetter compared to the original UCT algorithm and its variants in two benchmark\nexperiment settings. We also examine conditions under which UCT-Aux works well.\n"
  },
  {
    "id": "1206.6817",
    "title": "A Variational Approach for Approximating Bayesian Networks by Edge\n  Deletion",
    "abstract": "  We consider in this paper the formulation of approximate inference in\nBayesian networks as a problem of exact inference on an approximate network\nthat results from deleting edges (to reduce treewidth). We have shown in\nearlier work that deleting edges calls for introducing auxiliary network\nparameters to compensate for lost dependencies, and proposed intuitive\nconditions for determining these parameters. We have also shown that our method\ncorresponds to IBP when enough edges are deleted to yield a polytree, and\ncorresponds to some generalizations of IBP when fewer edges are deleted. In\nthis paper, we propose a different criteria for determining auxiliary\nparameters based on optimizing the KL-divergence between the original and\napproximate networks. We discuss the relationship between the two methods for\nselecting parameters, shedding new light on IBP and its generalizations. We\nalso discuss the application of our new method to approximating inference\nproblems which are exponential in constrained treewidth, including MAP and\nnonmyopic value of information.\n"
  },
  {
    "id": "1206.6819",
    "title": "On the Robustness of Most Probable Explanations",
    "abstract": "  In Bayesian networks, a Most Probable Explanation (MPE) is a complete\nvariable instantiation with a highest probability given the current evidence.\nIn this paper, we discuss the problem of finding robustness conditions of the\nMPE under single parameter changes. Specifically, we ask the question: How much\nchange in a single network parameter can we afford to apply while keeping the\nMPE unchanged? We will describe a procedure, which is the first of its kind,\nthat computes this answer for each parameter in the Bayesian network variable\nin time O(n exp(w)), where n is the number of network variables and w is its\ntreewidth.\n"
  },
  {
    "id": "1206.6822",
    "title": "Cutset Sampling with Likelihood Weighting",
    "abstract": "  The paper analyzes theoretically and empirically the performance of\nlikelihood weighting (LW) on a subset of nodes in Bayesian networks. The\nproposed scheme requires fewer samples to converge due to reduction in sampling\nvariance. The method exploits the structure of the network to bound the\ncomplexity of exact inference used to compute sampling distributions, similar\nto Gibbs cutset sampling. Yet, the extension of the previosly proposed cutset\nsampling principles to likelihood weighting is non-trivial due to differences\nin the sampling processes of Gibbs sampler and LW. We demonstrate empirically\nthat likelihood weighting on a cutset (LWLC) is effective time-wise and has a\nlower rejection rate than LW when applied to networks with many deterministic\nprobabilities. Finally, we show that the performance of likelihood weighting on\na cutset can be improved further by caching computed sampling distributions\nand, consequently, learning 'zeros' of the target distribution.\n"
  },
  {
    "id": "1206.6823",
    "title": "An Efficient Triplet-based Algorithm for Evidential Reasoning",
    "abstract": "  Linear-time computational techniques have been developed for combining\nevidence which is available on a number of contending hypotheses. They offer a\nmeans of making the computation-intensive calculations involved more efficient\nin certain circumstances. Unfortunately, they restrict the orthogonal sum of\nevidential functions to the dichotomous structure applies only to elements and\ntheir complements. In this paper, we present a novel evidence structure in\nterms of a triplet and a set of algorithms for evidential reasoning. The merit\nof this structure is that it divides a set of evidence into three subsets,\ndistinguishing trivial evidential elements from important ones focusing some\nparticular elements. It avoids the deficits of the dichotomous structure in\nrepresenting the preference of evidence and estimating the basic probability\nassignment of evidence. We have established a formalism for this structure and\nthe general formulae for combining pieces of evidence in the form of the\ntriplet, which have been theoretically justified.\n"
  },
  {
    "id": "1206.6827",
    "title": "Linear Algebra Approach to Separable Bayesian Networks",
    "abstract": "  Separable Bayesian Networks, or the Influence Model, are dynamic Bayesian\nNetworks in which the conditional probability distribution can be separated\ninto a function of only the marginal distribution of a node's neighbors,\ninstead of the joint distributions. In terms of modeling, separable networks\nhas rendered possible siginificant reduction in complexity, as the state space\nis only linear in the number of variables on the network, in contrast to a\ntypical state space which is exponential. In this work, We describe the\nconnection between an arbitrary Conditional Probability Table (CPT) and\nseparable systems using linear algebra. We give an alternate proof on the\nequivalence of sufficiency and separability. We present a computational method\nfor testing whether a given CPT is separable.\n"
  },
  {
    "id": "1206.6831",
    "title": "Pearl's Calculus of Intervention Is Complete",
    "abstract": "  This paper is concerned with graphical criteria that can be used to solve the\nproblem of identifying casual effects from nonexperimental data in a causal\nBayesian network structure, i.e., a directed acyclic graph that represents\ncausal relationships. We first review Pearl's work on this topic [Pearl, 1995],\nin which several useful graphical criteria are presented. Then we present a\ncomplete algorithm [Huang and Valtorta, 2006b] for the identifiability problem.\nBy exploiting the completeness of this algorithm, we prove that the three basic\ndo-calculus rules that Pearl presents are complete, in the sense that, if a\ncausal effect is identifiable, there exists a sequence of applications of the\nrules of the do-calculus that transforms the causal effect formula into a\nformula that only includes observational quantities.\n"
  },
  {
    "id": "1206.6834",
    "title": "A new axiomatization for likelihood gambles",
    "abstract": "  This paper studies a new and more general axiomatization than one presented\npreviously for preference on likelihood gambles. Likelihood gambles describe\nactions in a situation where a decision maker knows multiple probabilistic\nmodels and a random sample generated from one of those models but does not know\nprior probability of models. This new axiom system is inspired by Jensen's\naxiomatization of probabilistic gambles. Our approach provides a new\nperspective to the role of data in decision making under ambiguity. It avoids\none of the most controversial issue of Bayesian methodology namely the\nassumption of prior probability.\n"
  },
  {
    "id": "1206.6835",
    "title": "Dimension Reduction in Singularly Perturbed Continuous-Time Bayesian\n  Networks",
    "abstract": "  Continuous-time Bayesian networks (CTBNs) are graphical representations of\nmulti-component continuous-time Markov processes as directed graphs. The edges\nin the network represent direct influences among components. The joint rate\nmatrix of the multi-component process is specified by means of conditional rate\nmatrices for each component separately. This paper addresses the situation\nwhere some of the components evolve on a time scale that is much shorter\ncompared to the time scale of the other components. In this paper, we prove\nthat in the limit where the separation of scales is infinite, the Markov\nprocess converges (in distribution, or weakly) to a reduced, or effective\nMarkov process that only involves the slow components. We also demonstrate that\nfor reasonable separation of scale (an order of magnitude) the reduced process\nis a good approximation of the marginal process over the slow components. We\nprovide a simple procedure for building a reduced CTBN for this effective\nprocess, with conditional rate matrices that can be directly calculated from\nthe original CTBN, and discuss the implications for approximate reasoning in\nlarge systems.\n"
  },
  {
    "id": "1206.6836",
    "title": "Methods for computing state similarity in Markov Decision Processes",
    "abstract": "  A popular approach to solving large probabilistic systems relies on\naggregating states based on a measure of similarity. Many approaches in the\nliterature are heuristic. A number of recent methods rely instead on metrics\nbased on the notion of bisimulation, or behavioral equivalence between states\n(Givan et al, 2001, 2003; Ferns et al, 2004). An integral component of such\nmetrics is the Kantorovich metric between probability distributions. However,\nwhile this metric enables many satisfying theoretical properties, it is costly\nto compute in practice. In this paper, we use techniques from network\noptimization and statistical sampling to overcome this problem. We obtain in\nthis manner a variety of distance functions for MDP state aggregation, which\ndiffer in the tradeoff between time and space complexity, as well as the\nquality of the aggregation. We provide an empirical evaluation of these\ntrade-offs.\n"
  },
  {
    "id": "1206.6837",
    "title": "Residual Belief Propagation: Informed Scheduling for Asynchronous\n  Message Passing",
    "abstract": "  Inference for probabilistic graphical models is still very much a practical\nchallenge in large domains. The commonly used and effective belief propagation\n(BP) algorithm and its generalizations often do not converge when applied to\nhard, real-life inference tasks. While it is widely recognized that the\nscheduling of messages in these algorithms may have significant consequences,\nthis issue remains largely unexplored. In this work, we address the question of\nhow to schedule messages for asynchronous propagation so that a fixed point is\nreached faster and more often. We first show that any reasonable asynchronous\nBP converges to a unique fixed point under conditions similar to those that\nguarantee convergence of synchronous BP. In addition, we show that the\nconvergence rate of a simple round-robin schedule is at least as good as that\nof synchronous propagation. We then propose residual belief propagation (RBP),\na novel, easy-to-implement, asynchronous propagation algorithm that schedules\nmessages in an informed way, that pushes down a bound on the distance from the\nfixed point. Finally, we demonstrate the superiority of RBP over\nstate-of-the-art methods for a variety of challenging synthetic and real-life\nproblems: RBP converges significantly more often than other methods; and it\nsignificantly reduces running time until convergence, even when other methods\nconverge.\n"
  },
  {
    "id": "1206.6841",
    "title": "Asymmetric separation for local independence graphs",
    "abstract": "  Directed possibly cyclic graphs have been proposed by Didelez (2000) and\nNodelmann et al. (2002) in order to represent the dynamic dependencies among\nstochastic processes. These dependencies are based on a generalization of\nGranger-causality to continuous time, first developed by Schweder (1970) for\nMarkov processes, who called them local dependencies. They deserve special\nattention as they are asymmetric unlike stochastic (in)dependence. In this\npaper we focus on their graphical representation and develop a suitable, i.e.\nasymmetric notion of separation, called delta-separation. The properties of\nthis graph separation as well as of local independence are investigated in\ndetail within a framework of asymmetric (semi)graphoids allowing a deeper\ninsight into what information can be read off these graphs.\n"
  },
  {
    "id": "1206.6844",
    "title": "From influence diagrams to multi-operator cluster DAGs",
    "abstract": "  There exist several architectures to solve influence diagrams using local\ncomputations, such as the Shenoy-Shafer, the HUGIN, or the Lazy Propagation\narchitectures. They all extend usual variable elimination algorithms thanks to\nthe use of so-called 'potentials'. In this paper, we introduce a new\narchitecture, called the Multi-operator Cluster DAG architecture, which can\nproduce decompositions with an improved constrained induced-width, and\ntherefore induce potentially exponential gains. Its principle is to benefit\nfrom the composite nature of influence diagrams, instead of using uniform\npotentials, in order to better analyze the problem structure.\n"
  },
  {
    "id": "1206.6849",
    "title": "General-Purpose MCMC Inference over Relational Structures",
    "abstract": "  Tasks such as record linkage and multi-target tracking, which involve\nreconstructing the set of objects that underlie some observed data, are\nparticularly challenging for probabilistic inference. Recent work has achieved\nefficient and accurate inference on such problems using Markov chain Monte\nCarlo (MCMC) techniques with customized proposal distributions. Currently,\nimplementing such a system requires coding MCMC state representations and\nacceptance probability calculations that are specific to a particular\napplication. An alternative approach, which we pursue in this paper, is to use\na general-purpose probabilistic modeling language (such as BLOG) and a generic\nMetropolis-Hastings MCMC algorithm that supports user-supplied proposal\ndistributions. Our algorithm gains flexibility by using MCMC states that are\nonly partial descriptions of possible worlds; we provide conditions under which\nMCMC over partial worlds yields correct answers to queries. We also show how to\nuse a context-specific Bayes net to identify the factors in the acceptance\nprobability that need to be computed for a given proposed move. Experimental\nresults on a citation matching task show that our general-purpose MCMC engine\ncompares favorably with an application-specific system.\n"
  },
  {
    "id": "1206.6854",
    "title": "Belief Update in CLG Bayesian Networks With Lazy Propagation",
    "abstract": "  In recent years Bayesian networks (BNs) with a mixture of continuous and\ndiscrete variables have received an increasing level of attention. We present\nan architecture for exact belief update in Conditional Linear Gaussian BNs (CLG\nBNs). The architecture is an extension of lazy propagation using operations of\nLauritzen & Jensen [6] and Cowell [2]. By decomposing clique and separator\npotentials into sets of factors, the proposed architecture takes advantage of\nindependence and irrelevance properties induced by the structure of the graph\nand the evidence. The resulting benefits are illustrated by examples. Results\nof a preliminary empirical performance evaluation indicate a significant\npotential of the proposed architecture.\n"
  },
  {
    "id": "1206.6856",
    "title": "Reasoning about Uncertainty in Metric Spaces",
    "abstract": "  We set up a model for reasoning about metric spaces with belief theoretic\nmeasures. The uncertainty in these spaces stems from both probability and\nmetric. To represent both aspect of uncertainty, we choose an expected distance\nfunction as a measure of uncertainty. A formal logical system is constructed\nfor the reasoning about expected distance. Soundness and completeness are shown\nfor this logic. For reasoning on product metric space with uncertainty, a new\nmetric is defined and shown to have good properties.\n"
  },
  {
    "id": "1206.6859",
    "title": "Propagation of Delays in the National Airspace System",
    "abstract": "  The National Airspace System (NAS) is a large and complex system with\nthousands of interrelated components: administration, control centers,\nairports, airlines, aircraft, passengers, etc. The complexity of the NAS\ncreates many difficulties in management and control. One of the most pressing\nproblems is flight delay. Delay creates high cost to airlines, complaints from\npassengers, and difficulties for airport operations. As demand on the system\nincreases, the delay problem becomes more and more prominent. For this reason,\nit is essential for the Federal Aviation Administration to understand the\ncauses of delay and to find ways to reduce delay. Major contributing factors to\ndelay are congestion at the origin airport, weather, increasing demand, and air\ntraffic management (ATM) decisions such as the Ground Delay Programs (GDP).\nDelay is an inherently stochastic phenomenon. Even if all known causal factors\ncould be accounted for, macro-level national airspace system (NAS) delays could\nnot be predicted with certainty from micro-level aircraft information. This\npaper presents a stochastic model that uses Bayesian Networks (BNs) to model\nthe relationships among different components of aircraft delay and the causal\nfactors that affect delays. A case study on delays of departure flights from\nChicago O'Hare international airport (ORD) to Hartsfield-Jackson Atlanta\nInternational Airport (ATL) reveals how local and system level environmental\nand human-caused factors combine to affect components of delay, and how these\ncomponents contribute to the final arrival delay at the destination airport.\n"
  },
  {
    "id": "1206.6869",
    "title": "Recognizing Activities and Spatial Context Using Wearable Sensors",
    "abstract": "  We introduce a new dynamic model with the capability of recognizing both\nactivities that an individual is performing as well as where that ndividual is\nlocated. Our model is novel in that it utilizes a dynamic graphical model to\njointly estimate both activity and spatial context over time based on the\nsimultaneous use of asynchronous observations consisting of GPS measurements,\nand measurements from a small mountable sensor board. Joint inference is quite\ndesirable as it has the ability to improve accuracy of the model. A key goal,\nhowever, in designing our overall system is to be able to perform accurate\ninference decisions while minimizing the amount of hardware an individual must\nwear. This minimization leads to greater comfort and flexibility, decreased\npower requirements and therefore increased battery life, and reduced cost. We\nshow results indicating that our joint measurement model outperforms\nmeasurements from either the sensor board or GPS alone, using two types of\nprobabilistic inference procedures, namely particle filtering and pruned exact\ninference.\n"
  },
  {
    "id": "1206.6875",
    "title": "A simple approach for finding the globally optimal Bayesian network\n  structure",
    "abstract": "  We study the problem of learning the best Bayesian network structure with\nrespect to a decomposable score such as BDe, BIC or AIC. This problem is known\nto be NP-hard, which means that solving it becomes quickly infeasible as the\nnumber of variables increases. Nevertheless, in this paper we show that it is\npossible to learn the best Bayesian network structure with over 30 variables,\nwhich covers many practically interesting cases. Our algorithm is less\ncomplicated and more efficient than the techniques presented earlier. It can be\neasily parallelized, and offers a possibility for efficient exploration of the\nbest networks consistent with different variable orderings. In the experimental\npart of the paper we compare the performance of the algorithm to the previous\nstate-of-the-art algorithm. Free source-code and an online-demo can be found at\nhttp://b-course.hiit.fi/bene.\n"
  },
  {
    "id": "1206.6879",
    "title": "Practical Linear Value-approximation Techniques for First-order MDPs",
    "abstract": "  Recent work on approximate linear programming (ALP) techniques for\nfirst-order Markov Decision Processes (FOMDPs) represents the value function\nlinearly w.r.t. a set of first-order basis functions and uses linear\nprogramming techniques to determine suitable weights. This approach offers the\nadvantage that it does not require simplification of the first-order value\nfunction, and allows one to solve FOMDPs independent of a specific domain\ninstantiation. In this paper, we address several questions to enhance the\napplicability of this work: (1) Can we extend the first-order ALP framework to\napproximate policy iteration to address performance deficiencies of previous\napproaches? (2) Can we automatically generate basis functions and evaluate\ntheir impact on value function quality? (3) How can we decompose intractable\nproblems with universally quantified rewards into tractable subproblems? We\npropose answers to these questions along with a number of novel optimizations\nand provide a comparative empirical evaluation on logistics problems from the\nICAPS 2004 Probabilistic Planning Competition.\n"
  },
  {
    "id": "1206.7064",
    "title": "Software Verification and Graph Similarity for Automated Evaluation of\n  Students' Assignments",
    "abstract": "  In this paper we promote introducing software verification and control flow\ngraph similarity measurement in automated evaluation of students' programs. We\npresent a new grading framework that merges results obtained by combination of\nthese two approaches with results obtained by automated testing, leading to\nimproved quality and precision of automated grading. These two approaches are\nalso useful in providing a comprehensible feedback that can help students to\nimprove the quality of their programs We also present our corresponding tools\nthat are publicly available and open source. The tools are based on LLVM\nlow-level intermediate code representation, so they could be applied to a\nnumber of programming languages. Experimental evaluation of the proposed\ngrading framework is performed on a corpus of university students' programs\nwritten in programming language C. Results of the experiments show that\nautomatically generated grades are highly correlated with manually determined\ngrades suggesting that the presented tools can find real-world applications in\nstudying and grading.\n"
  },
  {
    "id": "1207.0117",
    "title": "Rule Based Expert System for Cerebral Palsy Diagnosis",
    "abstract": "  The use of Artificial Intelligence is finding prominence not only in core\ncomputer areas, but also in cross disciplinary areas including medical\ndiagnosis. In this paper, we present a rule based Expert System used in\ndiagnosis of Cerebral Palsy. The expert system takes user input and depending\non the symptoms of the patient, diagnoses if the patient is suffering from\nCerebral Palsy. The Expert System also classifies the Cerebral Palsy as mild,\nmoderate or severe based on the presented symptoms.\n"
  },
  {
    "id": "1207.0206",
    "title": "Alternative Restart Strategies for CMA-ES",
    "abstract": "  This paper focuses on the restart strategy of CMA-ES on multi-modal\nfunctions. A first alternative strategy proceeds by decreasing the initial\nstep-size of the mutation while doubling the population size at each restart. A\nsecond strategy adaptively allocates the computational budget among the restart\nsettings in the BIPOP scheme. Both restart strategies are validated on the BBOB\nbenchmark; their generality is also demonstrated on an independent real-world\nproblem suite related to spacecraft trajectory optimization.\n"
  },
  {
    "id": "1207.0262",
    "title": "Characteristic matrix of covering and its application to boolean matrix\n  decomposition and axiomatization",
    "abstract": "  Covering is an important type of data structure while covering-based rough\nsets provide an efficient and systematic theory to deal with covering data. In\nthis paper, we use boolean matrices to represent and axiomatize three types of\ncovering approximation operators. First, we define two types of characteristic\nmatrices of a covering which are essentially square boolean ones, and their\nproperties are studied. Through the characteristic matrices, three important\ntypes of covering approximation operators are concisely equivalently\nrepresented. Second, matrix representations of covering approximation operators\nare used in boolean matrix decomposition. We provide a sufficient and necessary\ncondition for a square boolean matrix to decompose into the boolean product of\nanother one and its transpose. And we develop an algorithm for this boolean\nmatrix decomposition. Finally, based on the above results, these three types of\ncovering approximation operators are axiomatized using boolean matrices. In a\nword, this work borrows extensively from boolean matrices and present a new\nview to study covering-based rough sets.\n"
  },
  {
    "id": "1207.0403",
    "title": "Robust Principal Component Analysis Using Statistical Estimators",
    "abstract": "  Principal Component Analysis (PCA) finds a linear mapping and maximizes the\nvariance of the data which makes PCA sensitive to outliers and may cause wrong\neigendirection. In this paper, we propose techniques to solve this problem; we\nuse the data-centering method and reestimate the covariance matrix using robust\nstatistic techniques such as median, robust scaling which is a booster to\ndata-centering and Huber M-estimator which measures the presentation of\noutliers and reweight them with small values. The results on several real world\ndata sets show that our proposed method handles outliers and gains better\nresults than the original PCA and provides the same accuracy with lower\ncomputation cost than the Kernel PCA using the polynomial kernel in\nclassification tasks.\n"
  },
  {
    "id": "1207.1230",
    "title": "Higher-Order Partial Least Squares (HOPLS): A Generalized Multi-Linear\n  Regression Method",
    "abstract": "  A new generalized multilinear regression model, termed the Higher-Order\nPartial Least Squares (HOPLS), is introduced with the aim to predict a tensor\n(multiway array) $\\tensor{Y}$ from a tensor $\\tensor{X}$ through projecting the\ndata onto the latent space and performing regression on the corresponding\nlatent variables. HOPLS differs substantially from other regression models in\nthat it explains the data by a sum of orthogonal Tucker tensors, while the\nnumber of orthogonal loadings serves as a parameter to control model complexity\nand prevent overfitting. The low dimensional latent space is optimized\nsequentially via a deflation operation, yielding the best joint subspace\napproximation for both $\\tensor{X}$ and $\\tensor{Y}$. Instead of decomposing\n$\\tensor{X}$ and $\\tensor{Y}$ individually, higher order singular value\ndecomposition on a newly defined generalized cross-covariance tensor is\nemployed to optimize the orthogonal loadings. A systematic comparison on both\nsynthetic data and real-world decoding of 3D movement trajectories from\nelectrocorticogram (ECoG) signals demonstrate the advantages of HOPLS over the\nexisting methods in terms of better predictive ability, suitability to handle\nsmall sample sizes, and robustness to noise.\n"
  },
  {
    "id": "1207.1350",
    "title": "Cost Sensitive Reachability Heuristics for Handling State Uncertainty",
    "abstract": "  While POMDPs provide a general platform for non-deterministic conditional\nplanning under a variety of quality metrics they have limited scalability. On\nthe other hand, non-deterministic conditional planners scale very well, but\nmany lack the ability to optimize plan quality metrics. We present a novel\ngeneralization of planning graph based heuristics that helps conditional\nplanners both scale and generate high quality plans when using actions with\nnonuniform costs. We make empirical comparisons with two state of the art\nplanners to show the benefit of our techniques.\n"
  }
]