[
  {
    "query": "face recognition",
    "results": [
      {
        "title": "Comparing Robustness of Neural Networks for Face Recognition",
        "abstract": "This study explores how noise impacts deep learning models used for face recognition in various environments."
      },
      {
        "title": "Object Detection using CNNs",
        "abstract": "We present a convolutional architecture optimized for general object detection tasks."
      },
      {
        "title": "Facial Biometrics: Advances and Challenges",
        "abstract": "The paper reviews recent developments in facial recognition technologies and compares biometric systems."
      }
    ]
  },
  {
    "query": "graph neural networks",
    "results": [
      {
        "title": "Graph Attention Networks",
        "abstract": "We introduce an attention-based model for learning on graph-structured data, achieving state-of-the-art results."
      },
      {
        "title": "Graph Convolutional Networks for Semi-Supervised Learning",
        "abstract": "This work presents a GCN-based approach to classify nodes using limited labels in a graph."
      },
      {
        "title": "Neural Models for Tabular Data",
        "abstract": "We explore the application of transformers and feed-forward nets on structured tabular datasets."
      }
    ]
  },
  {
    "query": "reinforcement learning",
    "results": [
      {
        "title": "Deep Reinforcement Learning: An Overview",
        "abstract": "This paper provides a comprehensive overview of deep RL methods and their applications."
      },
      {
        "title": "Q-learning in Multi-Agent Environments",
        "abstract": "We study how Q-learning adapts to multi-agent coordination and decision-making settings."
      },
      {
        "title": "Supervised Learning for Sequence Modeling",
        "abstract": "A comparison of supervised learning methods applied to sequence prediction tasks."
      }
    ]
  },
  {
    "query": "face recognition",
    "results": [
      {
        "title": "The Good Old Davis-Putnam Procedure Helps Counting Models",
        "abstract": "  As was shown recently, many important AI problems require counting the number\nof models of propositional formulas. The problem of counting models of such\nformulas is, according to present knowledge, computationally intractable in a\nworst case. Based on the Davis-Putnam procedure, we present an algorithm, CDP,\nthat computes the exact number of models of a propositional CNF or DNF formula\nF. Let m and n be the number of clauses and variables of F, respectively, and\nlet p denote the probability that a literal l of F occurs in a clause C of F,\nthen the average running time of CDP is shown to be O(nm^d), where\nd=-1/log(1-p). The practical performance of CDP has been estimated in a series\nof experiments on a wide variety of CNF formulas.\n"
      },
      {
        "title": "Freedom: A Measure of Second-order Uncertainty for Intervalic\n  Probability Schemes",
        "abstract": "  This paper discusses a new measure that is adaptable to certain intervalic\nprobability frameworks, possibility theory, and belief theory. As such, it has\nthe potential for wide use in knowledge engineering, expert systems, and\nrelated problems in the human sciences. This measure (denoted here by F) has\nbeen introduced in Smithson (1988) and is more formally discussed in Smithson\n(1989a)o Here, I propose to outline the conceptual basis for F and compare its\nproperties with other measures of second-order uncertainty. I will argue that F\nis an indicator of nonspecificity or alternatively, of freedom, as\ndistinguished from either ambiguity or vagueness.\n"
      },
      {
        "title": "Support and Plausibility Degrees in Generalized Functional Models",
        "abstract": "  By discussing several examples, the theory of generalized functional models\nis shown to be very natural for modeling some situations of reasoning under\nuncertainty. A generalized functional model is a pair (f, P) where f is a\nfunction describing the interactions between a parameter variable, an\nobservation variable and a random source, and P is a probability distribution\nfor the random source. Unlike traditional functional models, generalized\nfunctional models do not require that there is only one value of the parameter\nvariable that is compatible with an observation and a realization of the random\nsource. As a consequence, the results of the analysis of a generalized\nfunctional model are not expressed in terms of probability distributions but\nrather by support and plausibility functions. The analysis of a generalized\nfunctional model is very logical and is inspired from ideas already put forward\nby R.A. Fisher in his theory of fiducial probability.\n"
      },
      {
        "title": "Space Efficiency of Propositional Knowledge Representation Formalisms",
        "abstract": "  We investigate the space efficiency of a Propositional Knowledge\nRepresentation (PKR) formalism. Intuitively, the space efficiency of a\nformalism F in representing a certain piece of knowledge A, is the size of the\nshortest formula of F that represents A. In this paper we assume that knowledge\nis either a set of propositional interpretations (models) or a set of\npropositional formulae (theorems). We provide a formal way of talking about the\nrelative ability of PKR formalisms to compactly represent a set of models or a\nset of theorems. We introduce two new compactness measures, the corresponding\nclasses, and show that the relative space efficiency of a PKR formalism in\nrepresenting models/theorems is directly related to such classes. In\nparticular, we consider formalisms for nonmonotonic reasoning, such as\ncircumscription and default logic, as well as belief revision operators and the\nstable model semantics for logic programs with negation. One interesting result\nis that formalisms with the same time complexity do not necessarily belong to\nthe same space efficiency class.\n"
      },
      {
        "title": "Node discovery in a networked organization",
        "abstract": "  In this paper, I present a method to solve a node discovery problem in a\nnetworked organization. Covert nodes refer to the nodes which are not\nobservable directly. They affect social interactions, but do not appear in the\nsurveillance logs which record the participants of the social interactions.\nDiscovering the covert nodes is defined as identifying the suspicious logs\nwhere the covert nodes would appear if the covert nodes became overt. A\nmathematical model is developed for the maximal likelihood estimation of the\nnetwork behind the social interactions and for the identification of the\nsuspicious logs. Precision, recall, and F measure characteristics are\ndemonstrated with the dataset generated from a real organization and the\ncomputationally synthesized datasets. The performance is close to the\ntheoretical limit for any covert nodes in the networks of any topologies and\nsizes if the ratio of the number of observation to the number of possible\ncommunication patterns is large.\n"
      },
      {
        "title": "A Temporal Description Logic for Reasoning about Actions and Plans",
        "abstract": "  A class of interval-based temporal languages for uniformly representing and\nreasoning about actions and plans is presented. Actions are represented by\ndescribing what is true while the action itself is occurring, and plans are\nconstructed by temporally relating actions and world states. The temporal\nlanguages are members of the family of Description Logics, which are\ncharacterized by high expressivity combined with good computational properties.\nThe subsumption problem for a class of temporal Description Logics is\ninvestigated and sound and complete decision procedures are given. The basic\nlanguage TL-F is considered first: it is the composition of a temporal logic TL\n-- able to express interval temporal networks -- together with the non-temporal\nlogic F -- a Feature Description Logic. It is proven that subsumption in this\nlanguage is an NP-complete problem. Then it is shown how to reason with the\nmore expressive languages TLU-FU and TL-ALCF. The former adds disjunction both\nat the temporal and non-temporal sides of the language, the latter extends the\nnon-temporal side with set-valued features (i.e., roles) and a propositionally\ncomplete language.\n"
      },
      {
        "title": "An Alternative Proof Method for Possibilistic Logic and its Application\n  to Terminological Logics",
        "abstract": "  Possibilistic logic, an extension of first-order logic, deals with\nuncertainty that can be estimated in terms of possibility and necessity\nmeasures. Syntactically, this means that a first-order formula is equipped with\na possibility degree or a necessity degree that expresses to what extent the\nformula is possibly or necessarily true. Possibilistic resolution yields a\ncalculus for possibilistic logic which respects the semantics developed for\npossibilistic logic. A drawback, which possibilistic resolution inherits from\nclassical resolution, is that it may not terminate if applied to formulas\nbelonging to decidable fragments of first-order logic. Therefore we propose an\nalternative proof method for possibilistic logic. The main feature of this\nmethod is that it completely abstracts from a concrete calculus but uses as\nbasic operation a test for classical entailment. We then instantiate\npossibilistic logic with a terminological logic, which is a decidable subclass\no f first-order logic but nevertheless much more expressive than propositional\nlogic. This yields an extension of terminological logics towards the\nrepresentation of uncertain knowledge which is satisfactory from a semantic as\nwell as algorithmic point of view.\n"
      },
      {
        "title": "Entailment in Probability of Thresholded Generalizations",
        "abstract": "  A nonmonotonic logic of thresholded generalizations is presented. Given\npropositions A and B from a language L and a positive integer k, the\nthresholded generalization A=>B{k} means that the conditional probability\nP(B|A) falls short of one by no more than c*d^k. A two-level probability\nstructure is defined. At the lower level, a model is defined to be a\nprobability function on L. At the upper level, there is a probability\ndistribution over models. A definition is given of what it means for a\ncollection of thresholded generalizations to entail another thresholded\ngeneralization. This nonmonotonic entailment relation, called \"entailment in\nprobability\", has the feature that its conclusions are \"probabilistically\ntrustworthy\" meaning that, given true premises, it is improbable that an\nentailed conclusion would be false. A procedure is presented for ascertaining\nwhether any given collection of premises entails any given conclusion. It is\nshown that entailment in probability is closely related to Goldszmidt and\nPearl's System-Z^+, thereby demonstrating that the conclusions of System-Z^+\nare probabilistically trustworthy.\n"
      },
      {
        "title": "Descriptive-complexity based distance for fuzzy sets",
        "abstract": "  A new distance function dist(A,B) for fuzzy sets A and B is introduced. It is\nbased on the descriptive complexity, i.e., the number of bits (on average) that\nare needed to describe an element in the symmetric difference of the two sets.\nThe distance gives the amount of additional information needed to describe any\none of the two sets given the other. We prove its mathematical properties and\nperform pattern clustering on data based on this distance.\n"
      },
      {
        "title": "Detecting Causal Relations in the Presence of Unmeasured Variables",
        "abstract": "  The presence of latent variables can greatly complicate inferences about\ncausal relations between measured variables from statistical data. In many\ncases, the presence of latent variables makes it impossible to determine for\ntwo measured variables A and B, whether A causes B, B causes A, or there is\nsome common cause. In this paper I present several theorems that state\nconditions under which it is possible to reliably infer the causal relation\nbetween two measured variables, regardless of whether latent variables are\nacting or not.\n"
      },
      {
        "title": "Maximum Uncertainty Procedures for Interval-Valued Probability\n  Distributions",
        "abstract": "  Measures of uncertainty and divergence are introduced for interval-valued\nprobability distributions and are shown to have desirable mathematical\nproperties. A maximum uncertainty inference procedure for marginal interval\ndistributions is presented. A technique for reconstruction of interval\ndistributions from projections is developed based on this inference procedure\n"
      },
      {
        "title": "An Application of Proof-Theory in Answer Set Programming",
        "abstract": "  We apply proof-theoretic techniques in answer Set Programming. The main\nresults include: 1. A characterization of continuity properties of\nGelfond-Lifschitz operator for logic program. 2. A propositional\ncharacterization of stable models of logic programs (without referring to loop\nformulas.\n"
      },
      {
        "title": "A Fuzzy Logic Approach to Target Tracking",
        "abstract": "  This paper discusses a target tracking problem in which no dynamic\nmathematical model is explicitly assumed. A nonlinear filter based on the fuzzy\nIf-then rules is developed. A comparison with a Kalman filter is made, and\nempirical results show that the performance of the fuzzy filter is better.\nIntensive simulations suggest that theoretical justification of the empirical\nresults is possible.\n"
      },
      {
        "title": "Support and Plausibility Degrees in Generalized Functional Models",
        "abstract": "  By discussing several examples, the theory of generalized functional models\nis shown to be very natural for modeling some situations of reasoning under\nuncertainty. A generalized functional model is a pair (f, P) where f is a\nfunction describing the interactions between a parameter variable, an\nobservation variable and a random source, and P is a probability distribution\nfor the random source. Unlike traditional functional models, generalized\nfunctional models do not require that there is only one value of the parameter\nvariable that is compatible with an observation and a realization of the random\nsource. As a consequence, the results of the analysis of a generalized\nfunctional model are not expressed in terms of probability distributions but\nrather by support and plausibility functions. The analysis of a generalized\nfunctional model is very logical and is inspired from ideas already put forward\nby R.A. Fisher in his theory of fiducial probability.\n"
      },
      {
        "title": "Considerations on Construction Ontologies",
        "abstract": "  The paper proposes an analysis on some existent ontologies, in order to point\nout ways to resolve semantic heterogeneity in information systems. Authors are\nhighlighting the tasks in a Knowledge Acquisiton System and identifying aspects\nrelated to the addition of new information to an intelligent system. A solution\nis proposed, as a combination of ontology reasoning services and natural\nlanguages generation. A multi-agent system will be conceived with an extractor\nagent, a reasoner agent and a competence management agent.\n"
      },
      {
        "title": "PHA*: Finding the Shortest Path with A* in An Unknown Physical\n  Environment",
        "abstract": "  We address the problem of finding the shortest path between two points in an\nunknown real physical environment, where a traveling agent must move around in\nthe environment to explore unknown territory. We introduce the Physical-A*\nalgorithm (PHA*) for solving this problem. PHA* expands all the mandatory nodes\nthat A* would expand and returns the shortest path between the two points.\nHowever, due to the physical nature of the problem, the complexity of the\nalgorithm is measured by the traveling effort of the moving agent and not by\nthe number of generated nodes, as in standard A*. PHA* is presented as a\ntwo-level algorithm, such that its high level, A*, chooses the next node to be\nexpanded and its low level directs the agent to that node in order to explore\nit. We present a number of variations for both the high-level and low-level\nprocedures and evaluate their performance theoretically and experimentally. We\nshow that the travel cost of our best variation is fairly close to the optimal\ntravel cost, assuming that the mandatory nodes of A* are known in advance. We\nthen generalize our algorithm to the multi-agent case, where a number of\ncooperative agents are designed to solve the problem. Specifically, we provide\nan experimental implementation for such a system. It should be noted that the\nproblem addressed here is not a navigation problem, but rather a problem of\nfinding the shortest path between two points for future usage.\n"
      },
      {
        "title": "A Complete Calculus for Possibilistic Logic Programming with Fuzzy\n  Propositional Variables",
        "abstract": "  In this paper we present a propositional logic programming language for\nreasoning under possibilistic uncertainty and representing vague knowledge.\nFormulas are represented by pairs (A, c), where A is a many-valued proposition\nand c is value in the unit interval [0,1] which denotes a lower bound on the\nbelief on A in terms of necessity measures. Belief states are modeled by\npossibility distributions on the set of all many-valued interpretations. In\nthis framework, (i) we define a syntax and a semantics of the general\nunderlying uncertainty logic; (ii) we provide a modus ponens-style calculus for\na sublanguage of Horn-rules and we prove that it is complete for determining\nthe maximum degree of possibilistic belief with which a fuzzy propositional\nvariable can be entailed from a set of formulas; and finally, (iii) we show how\nthe computation of a partial matching between fuzzy propositional variables, in\nterms of necessity measures for fuzzy sets, can be included in our logic\nprogramming system.\n"
      },
      {
        "title": "Probabilistic Reasoning about Actions in Nonmonotonic Causal Theories",
        "abstract": "  We present the language {m P}{cal C}+ for probabilistic reasoning about\nactions, which is a generalization of the action language {cal C}+ that allows\nto deal with probabilistic as well as nondeterministic effects of actions. We\ndefine a formal semantics of {m P}{cal C}+ in terms of probabilistic\ntransitions between sets of states. Using a concept of a history and its belief\nstate, we then show how several important problems in reasoning about actions\ncan be concisely formulated in our formalism.\n"
      },
      {
        "title": "Belief and Surprise - A Belief-Function Formulation",
        "abstract": "  We motivate and describe a theory of belief in this paper. This theory is\ndeveloped with the following view of human belief in mind. Consider the belief\nthat an event E will occur (or has occurred or is occurring). An agent either\nentertains this belief or does not entertain this belief (i.e., there is no\n\"grade\" in entertaining the belief). If the agent chooses to exercise \"the will\nto believe\" and entertain this belief, he/she/it is entitled to a degree of\nconfidence c (1 > c > 0) in doing so. Adopting this view of human belief, we\nconjecture that whenever an agent entertains the belief that E will occur with\nc degree of confidence, the agent will be surprised (to the extent c) upon\nrealizing that E did not occur.\n"
      },
      {
        "title": "Some improved results on communication between information systems",
        "abstract": "  To study the communication between information systems, Wang et al. [C. Wang,\nC. Wu, D. Chen, Q. Hu, and C. Wu, Communicating between information systems,\nInformation Sciences 178 (2008) 3228-3239] proposed two concepts of type-1 and\ntype-2 consistent functions. Some properties of such functions and induced\nrelation mappings have been investigated there. In this paper, we provide an\nimprovement of the aforementioned work by disclosing the symmetric relationship\nbetween type-1 and type-2 consistent functions. We present more properties of\nconsistent functions and induced relation mappings and improve upon several\ndeficient assertions in the original work. In particular, we unify and extend\ntype-1 and type-2 consistent functions into the so-called\nneighborhood-consistent functions. This provides a convenient means for\nstudying the communication between information systems based on various\nneighborhoods.\n"
      },
      {
        "title": "Randomized Algorithms for the Loop Cutset Problem",
        "abstract": "  We show how to find a minimum weight loop cutset in a Bayesian network with\nhigh probability. Finding such a loop cutset is the first step in the method of\nconditioning for inference. Our randomized algorithm for finding a loop cutset\noutputs a minimum loop cutset after O(c 6^k kn) steps with probability at least\n1 - (1 - 1/(6^k))^c6^k, where c > 1 is a constant specified by the user, k is\nthe minimal size of a minimum weight loop cutset, and n is the number of\nvertices. We also show empirically that a variant of this algorithm often finds\na loop cutset that is closer to the minimum weight loop cutset than the ones\nfound by the best deterministic algorithms known.\n"
      },
      {
        "title": "Model Based Framework for Estimating Mutation Rate of Hepatitis C Virus\n  in Egypt",
        "abstract": "  Hepatitis C virus (HCV) is a widely spread disease all over the world. HCV\nhas very high mutation rate that makes it resistant to antibodies. Modeling HCV\nto identify the virus mutation process is essential to its detection and\npredicting its evolution. This paper presents a model based framework for\nestimating mutation rate of HCV in two steps. Firstly profile hidden Markov\nmodel (PHMM) architecture was builder to select the sequences which represents\nsequence per year. Secondly mutation rate was calculated by using pair-wise\ndistance method between sequences. A pilot study is conducted on NS5B zone of\nHCV dataset of genotype 4 subtype a (HCV4a) in Egypt.\n"
      },
      {
        "title": "An improved approach to attribute reduction with covering rough sets",
        "abstract": "  Attribute reduction is viewed as an important preprocessing step for pattern\nrecognition and data mining. Most of researches are focused on attribute\nreduction by using rough sets. Recently, Tsang et al. discussed attribute\nreduction with covering rough sets in the paper [E. C.C. Tsang, D. Chen, Daniel\nS. Yeung, Approximations and reducts with covering generalized rough sets,\nComputers and Mathematics with Applications 56 (2008) 279-289], where an\napproach based on discernibility matrix was presented to compute all attribute\nreducts. In this paper, we provide an improved approach by constructing simpler\ndiscernibility matrix with covering rough sets, and then proceed to improve\nsome characterizations of attribute reduction provided by Tsang et al. It is\nproved that the improved discernible matrix is equivalent to the old one, but\nthe computational complexity of discernible matrix is greatly reduced.\n"
      },
      {
        "title": "A Complete Calculus for Possibilistic Logic Programming with Fuzzy\n  Propositional Variables",
        "abstract": "  In this paper we present a propositional logic programming language for\nreasoning under possibilistic uncertainty and representing vague knowledge.\nFormulas are represented by pairs (A, c), where A is a many-valued proposition\nand c is value in the unit interval [0,1] which denotes a lower bound on the\nbelief on A in terms of necessity measures. Belief states are modeled by\npossibility distributions on the set of all many-valued interpretations. In\nthis framework, (i) we define a syntax and a semantics of the general\nunderlying uncertainty logic; (ii) we provide a modus ponens-style calculus for\na sublanguage of Horn-rules and we prove that it is complete for determining\nthe maximum degree of possibilistic belief with which a fuzzy propositional\nvariable can be entailed from a set of formulas; and finally, (iii) we show how\nthe computation of a partial matching between fuzzy propositional variables, in\nterms of necessity measures for fuzzy sets, can be included in our logic\nprogramming system.\n"
      },
      {
        "title": "A Monte-Carlo Algorithm for Dempster-Shafer Belief",
        "abstract": "  A very computationally-efficient Monte-Carlo algorithm for the calculation of\nDempster-Shafer belief is described. If Bel is the combination using Dempster's\nRule of belief functions Bel, ..., Bel,7, then, for subset b of the frame C),\nBel(b) can be calculated in time linear in 1(31 and m (given that the weight of\nconflict is bounded). The algorithm can also be used to improve the complexity\nof the Shenoy-Shafer algorithms on Markov trees, and be generalised to\ncalculate Dempster-Shafer Belief over other logics.\n"
      },
      {
        "title": "MCE Reasoning in Recursive Causal Networks",
        "abstract": "  A probabilistic method of reasoning under uncertainty is proposed based on\nthe principle of Minimum Cross Entropy (MCE) and concept of Recursive Causal\nModel (RCM). The dependency and correlations among the variables are described\nin a special language BNDL (Belief Networks Description Language). Beliefs are\npropagated among the clauses of the BNDL programs representing the underlying\nprobabilistic distributions. BNDL interpreters in both Prolog and C has been\ndeveloped and the performance of the method is compared with those of the\nothers.\n"
      },
      {
        "title": "Counterfactuals and Policy Analysis in Structural Models",
        "abstract": "  Evaluation of counterfactual queries (e.g., \"If A were true, would C have\nbeen true?\") is important to fault diagnosis, planning, determination of\nliability, and policy analysis. We present a method of revaluating\ncounterfactuals when the underlying causal model is represented by structural\nmodels - a nonlinear generalization of the simultaneous equations models\ncommonly used in econometrics and social sciences. This new method provides a\ncoherent means for evaluating policies involving the control of variables\nwhich, prior to enacting the policy were influenced by other variables in the\nsystem.\n"
      },
      {
        "title": "Cautious Propagation in Bayesian Networks",
        "abstract": "  Consider the situation where some evidence e has been entered to a Bayesian\nnetwork. When performing conflict analysis, sensitivity analysis, or when\nanswering questions like \"What if the finding on X had been y instead of x?\"\nyou need probabilities P (e'| h), where e' is a subset of e, and h is a\nconfiguration of a (possibly empty) set of variables. Cautious propagation is a\nmodification of HUGIN propagation into a Shafer-Shenoy-like architecture. It is\nless efficient than HUGIN propagation; however, it provides easy access to P\n(e'| h) for a great deal of relevant subsets e'.\n"
      },
      {
        "title": "d-Separation: From Theorems to Algorithms",
        "abstract": "  An efficient algorithm is developed that identifies all independencies\nimplied by the topology of a Bayesian network. Its correctness and maximality\nstems from the soundness and completeness of d-separation with respect to\nprobability theory. The algorithm runs in time O (l E l) where E is the number\nof edges in the network.\n"
      },
      {
        "title": "Belief and Surprise - A Belief-Function Formulation",
        "abstract": "  We motivate and describe a theory of belief in this paper. This theory is\ndeveloped with the following view of human belief in mind. Consider the belief\nthat an event E will occur (or has occurred or is occurring). An agent either\nentertains this belief or does not entertain this belief (i.e., there is no\n\"grade\" in entertaining the belief). If the agent chooses to exercise \"the will\nto believe\" and entertain this belief, he/she/it is entitled to a degree of\nconfidence c (1 > c > 0) in doing so. Adopting this view of human belief, we\nconjecture that whenever an agent entertains the belief that E will occur with\nc degree of confidence, the agent will be surprised (to the extent c) upon\nrealizing that E did not occur.\n"
      },
      {
        "title": "Pruning Bayesian Networks for Efficient Computation",
        "abstract": "  This paper analyzes the circumstances under which Bayesian networks can be\npruned in order to reduce computational complexity without altering the\ncomputation for variables of interest. Given a problem instance which consists\nof a query and evidence for a set of nodes in the network, it is possible to\ndelete portions of the network which do not participate in the computation for\nthe query. Savings in computational complexity can be large when the original\nnetwork is not singly connected. Results analogous to those described in this\npaper have been derived before [Geiger, Verma, and Pearl 89, Shachter 88] but\nthe implications for reducing complexity of the computations in Bayesian\nnetworks have not been stated explicitly. We show how a preprocessing step can\nbe used to prune a Bayesian network prior to using standard algorithms to solve\na given problem instance. We also show how our results can be used in a\nparallel distributed implementation in order to achieve greater savings. We\ndefine a computationally equivalent subgraph of a Bayesian network. The\nalgorithm developed in [Geiger, Verma, and Pearl 89] is modified to construct\nthe subgraphs described in this paper with O(e) complexity, where e is the\nnumber of edges in the Bayesian network. Finally, we define a minimal\ncomputationally equivalent subgraph and prove that the subgraphs described are\nminimal.\n"
      },
      {
        "title": "PDDL2.1: An Extension to PDDL for Expressing Temporal Planning Domains",
        "abstract": "  In recent years research in the planning community has moved increasingly\ntoward s application of planners to realistic problems involving both time and\nmany typ es of resources. For example, interest in planning demonstrated by the\nspace res earch community has inspired work in observation scheduling,\nplanetary rover ex ploration and spacecraft control domains. Other temporal and\nresource-intensive domains including logistics planning, plant control and\nmanufacturing have also helped to focus the community on the modelling and\nreasoning issues that must be confronted to make planning technology meet the\nchallenges of application. The International Planning Competitions have acted\nas an important motivating fo rce behind the progress that has been made in\nplanning since 1998. The third com petition (held in 2002) set the planning\ncommunity the challenge of handling tim e and numeric resources. This\nnecessitated the development of a modelling langua ge capable of expressing\ntemporal and numeric properties of planning domains. In this paper we describe\nthe language, PDDL2.1, that was used in the competition. We describe the syntax\nof the language, its formal semantics and the validation of concurrent plans.\nWe observe that PDDL2.1 has considerable modelling power --- exceeding the\ncapabilities of current planning technology --- and presents a number of\nimportant challenges to the research community.\n"
      },
      {
        "title": "The Planning Spectrum - One, Two, Three, Infinity",
        "abstract": "  Linear Temporal Logic (LTL) is widely used for defining conditions on the\nexecution paths of dynamic systems. In the case of dynamic systems that allow\nfor nondeterministic evolutions, one has to specify, along with an LTL formula\nf, which are the paths that are required to satisfy the formula. Two extreme\ncases are the universal interpretation A.f, which requires that the formula be\nsatisfied for all execution paths, and the existential interpretation E.f,\nwhich requires that the formula be satisfied for some execution path.\n  When LTL is applied to the definition of goals in planning problems on\nnondeterministic domains, these two extreme cases are too restrictive. It is\noften impossible to develop plans that achieve the goal in all the\nnondeterministic evolutions of a system, and it is too weak to require that the\ngoal is satisfied by some execution.\n  In this paper we explore alternative interpretations of an LTL formula that\nare between these extreme cases. We define a new language that permits an\narbitrary combination of the A and E quantifiers, thus allowing, for instance,\nto require that each finite execution can be extended to an execution\nsatisfying an LTL formula (AE.f), or that there is some finite execution whose\nextensions all satisfy an LTL formula (EA.f). We show that only eight of these\ncombinations of path quantifiers are relevant, corresponding to an alternation\nof the quantifiers of length one (A and E), two (AE and EA), three (AEA and\nEAE), and infinity ((AE)* and (EA)*). We also present a planning algorithm for\nthe new language that is based on an automata-theoretic approach, and study its\ncomplexity.\n"
      },
      {
        "title": "A Unifying Survey of Reinforced, Sensitive and Stigmergic Agent-Based\n  Approaches for E-GTSP",
        "abstract": "  The Generalized Traveling Salesman Problem (GTSP) is one of the NP-hard\ncombinatorial optimization problems. A variant of GTSP is E-GTSP where E,\nmeaning equality, has the constraint: exactly one node from a cluster of a\ngraph partition is visited. The main objective of the E-GTSP is to find a\nminimum cost tour passing through exactly one node from each cluster of an\nundirected graph. Agent-based approaches involving are successfully used\nnowadays for solving real life complex problems. The aim of the current paper\nis to illustrate some variants of agent-based algorithms including ant-based\nmodels with specific properties for solving E-GTSP.\n"
      },
      {
        "title": "Generating Bayesian Networks from Probability Logic Knowledge Bases",
        "abstract": "  We present a method for dynamically generating Bayesian networks from\nknowledge bases consisting of first-order probability logic sentences. We\npresent a subset of probability logic sufficient for representing the class of\nBayesian networks with discrete-valued nodes. We impose constraints on the form\nof the sentences that guarantee that the knowledge base contains all the\nprobabilistic information necessary to generate a network. We define the\nconcept of d-separation for knowledge bases and prove that a knowledge base\nwith independence conditions defined by d-separation is a complete\nspecification of a probability distribution. We present a network generation\nalgorithm that, given an inference problem in the form of a query Q and a set\nof evidence E, generates a network to compute P(Q|E). We prove the algorithm to\nbe correct.\n"
      },
      {
        "title": "An improved approach to attribute reduction with covering rough sets",
        "abstract": "  Attribute reduction is viewed as an important preprocessing step for pattern\nrecognition and data mining. Most of researches are focused on attribute\nreduction by using rough sets. Recently, Tsang et al. discussed attribute\nreduction with covering rough sets in the paper [E. C.C. Tsang, D. Chen, Daniel\nS. Yeung, Approximations and reducts with covering generalized rough sets,\nComputers and Mathematics with Applications 56 (2008) 279-289], where an\napproach based on discernibility matrix was presented to compute all attribute\nreducts. In this paper, we provide an improved approach by constructing simpler\ndiscernibility matrix with covering rough sets, and then proceed to improve\nsome characterizations of attribute reduction provided by Tsang et al. It is\nproved that the improved discernible matrix is equivalent to the old one, but\nthe computational complexity of discernible matrix is greatly reduced.\n"
      },
      {
        "title": "Evidential Reasoning in a Categorial Perspective: Conjunction and\n  Disjunction of Belief Functions",
        "abstract": "  The categorial approach to evidential reasoning can be seen as a combination\nof the probability kinematics approach of Richard Jeffrey (1965) and the\nmaximum (cross-) entropy inference approach of E. T. Jaynes (1957). As a\nconsequence of that viewpoint, it is well known that category theory provides\nnatural definitions for logical connectives. In particular, disjunction and\nconjunction are modelled by general categorial constructions known as products\nand coproducts. In this paper, I focus mainly on Dempster-Shafer theory of\nbelief functions for which I introduce a category I call Dempster?s category. I\nprove the existence of and give explicit formulas for conjunction and\ndisjunction in the subcategory of separable belief functions. In Dempster?s\ncategory, the new defined conjunction can be seen as the most cautious\nconjunction of beliefs, and thus no assumption about distinctness (of the\nsources) of beliefs is needed as opposed to Dempster?s rule of combination,\nwhich calls for distinctness (of the sources) of beliefs.\n"
      },
      {
        "title": "Decision Principles to justify Carnap's Updating Method and to Suggest\n  Corrections of Probability Judgments (Invited Talks)",
        "abstract": "  This paper uses decision-theoretic principles to obtain new insights into the\nassessment and updating of probabilities. First, a new foundation of\nBayesianism is given. It does not require infinite atomless uncertainties as\ndid Savage s classical result, AND can therefore be applied TO ANY finite\nBayesian network.It neither requires linear utility AS did de Finetti s\nclassical result, AND r ntherefore allows FOR the empirically AND normatively\ndesirable risk r naversion.Finally, BY identifying AND fixing utility IN an\nelementary r nmanner, our result can readily be applied TO identify methods OF\nr nprobability updating.Thus, a decision - theoretic foundation IS given r nto\nthe computationally efficient method OF inductive reasoning r ndeveloped BY\nRudolf Carnap.Finally, recent empirical findings ON r nprobability assessments\nare discussed.It leads TO suggestions FOR r ncorrecting biases IN probability\nassessments, AND FOR an alternative r nto the Dempster - Shafer belief\nfunctions that avoids the reduction TO r ndegeneracy after multiple updatings.r\nn\n"
      },
      {
        "title": "Logarithmic Time Parallel Bayesian Inference",
        "abstract": "  I present a parallel algorithm for exact probabilistic inference in Bayesian\nnetworks. For polytree networks with n variables, the worst-case time\ncomplexity is O(log n) on a CREW PRAM (concurrent-read, exclusive-write\nparallel random-access machine) with n processors, for any constant number of\nevidence variables. For arbitrary networks, the time complexity is O(r^{3w}*log\nn) for n processors, or O(w*log n) for r^{3w}*n processors, where r is the\nmaximum range of any variable, and w is the induced width (the maximum clique\nsize), after moralizing and triangulating the network.\n"
      },
      {
        "title": "A New Sufficient Condition for 1-Coverage to Imply Connectivity",
        "abstract": "  An effective approach for energy conservation in wireless sensor networks is\nscheduling sleep intervals for extraneous nodes while the remaining nodes stay\nactive to provide continuous service. For the sensor network to operate\nsuccessfully the active nodes must maintain both sensing coverage and network\nconnectivity, It proved before if the communication range of nodes is at least\ntwice the sensing range, complete coverage of a convex area implies\nconnectivity among the working set of nodes. In this paper we consider a\nrectangular region A = a *b, such that R a R b s s {\\pounds}, {\\pounds}, where\ns R is the sensing range of nodes. and put a constraint on minimum allowed\ndistance between nodes(s). according to this constraint we present a new lower\nbound for communication range relative to sensing range of sensors(s 2 + 3 *R)\nthat complete coverage of considered area implies connectivity among the\nworking set of nodes; also we present a new distribution method, that satisfy\nour constraint.\n"
      },
      {
        "title": "R&D Analyst: An Interactive Approach to Normative Decision System Model\n  Construction",
        "abstract": "  This paper describes the architecture of R&D Analyst, a commercial\nintelligent decision system for evaluating corporate research and development\nprojects and portfolios. In analyzing projects, R&D Analyst interactively\nguides a user in constructing an influence diagram model for an individual\nresearch project. The system's interactive approach can be clearly explained\nfrom a blackboard system perspective. The opportunistic reasoning emphasis of\nblackboard systems satisfies the flexibility requirements of model\nconstruction, thereby suggesting that a similar architecture would be valuable\nfor developing normative decision systems in other domains. Current research is\naimed at extending the system architecture to explicitly consider of sequential\ndecisions involving limited temporal, financial, and physical resources.\n"
      },
      {
        "title": "Parameter Adjustment in Bayes Networks. The generalized noisy OR-gate",
        "abstract": "  Spiegelhalter and Lauritzen [15] studied sequential learning in Bayesian\nnetworks and proposed three models for the representation of conditional\nprobabilities. A forth model, shown here, assumes that the parameter\ndistribution is given by a product of Gaussian functions and updates them from\nthe _ and _r messages of evidence propagation. We also generalize the noisy\nOR-gate for multivalued variables, develop the algorithm to compute probability\nin time proportional to the number of parents (even in networks with loops) and\napply the learning model to this gate.\n"
      },
      {
        "title": "On the comparison of plans: Proposition of an instability measure for\n  dynamic machine scheduling",
        "abstract": "  On the basis of an analysis of previous research, we present a generalized\napproach for measuring the difference of plans with an exemplary application to\nmachine scheduling. Our work is motivated by the need for such measures, which\nare used in dynamic scheduling and planning situations. In this context,\nquantitative approaches are needed for the assessment of the robustness and\nstability of schedules. Obviously, any `robustness' or `stability' of plans has\nto be defined w. r. t. the particular situation and the requirements of the\nhuman decision maker. Besides the proposition of an instability measure, we\ntherefore discuss possibilities of obtaining meaningful information from the\ndecision maker for the implementation of the introduced approach.\n"
      },
      {
        "title": "A Constraint Logic Programming Approach for Computing Ordinal\n  Conditional Functions",
        "abstract": "  In order to give appropriate semantics to qualitative conditionals of the\nform \"if A then normally B\", ordinal conditional functions (OCFs) ranking the\npossible worlds according to their degree of plausibility can be used. An OCF\naccepting all conditionals of a knowledge base R can be characterized as the\nsolution of a constraint satisfaction problem. We present a high-level,\ndeclarative approach using constraint logic programming techniques for solving\nthis constraint satisfaction problem. In particular, the approach developed\nhere supports the generation of all minimal solutions; these minimal solutions\nare of special interest as they provide a basis for model-based inference from\nR.\n"
      },
      {
        "title": "Hybrid Probabilistic Programs: Algorithms and Complexity",
        "abstract": "  Hybrid Probabilistic Programs (HPPs) are logic programs that allow the\nprogrammer to explicitly encode his knowledge of the dependencies between\nevents being described in the program. In this paper, we classify HPPs into\nthree classes called HPP_1,HPP_2 and HPP_r,r>= 3. For these classes, we provide\nthree types of results for HPPs. First, we develop algorithms to compute the\nset of all ground consequences of an HPP. Then we provide algorithms and\ncomplexity results for the problems of entailment (\"Given an HPP P and a query\nQ as input, is Q a logical consequence of P?\") and consistency (\"Given an HPP P\nas input, is P consistent?\"). Our results provide a fine characterization of\nwhen polynomial algorithms exist for the above problems, and when these\nproblems become intractable.\n"
      },
      {
        "title": "Support and Plausibility Degrees in Generalized Functional Models",
        "abstract": "  By discussing several examples, the theory of generalized functional models\nis shown to be very natural for modeling some situations of reasoning under\nuncertainty. A generalized functional model is a pair (f, P) where f is a\nfunction describing the interactions between a parameter variable, an\nobservation variable and a random source, and P is a probability distribution\nfor the random source. Unlike traditional functional models, generalized\nfunctional models do not require that there is only one value of the parameter\nvariable that is compatible with an observation and a realization of the random\nsource. As a consequence, the results of the analysis of a generalized\nfunctional model are not expressed in terms of probability distributions but\nrather by support and plausibility functions. The analysis of a generalized\nfunctional model is very logical and is inspired from ideas already put forward\nby R.A. Fisher in his theory of fiducial probability.\n"
      },
      {
        "title": "A Polynomial Time Algorithm for Finding Bayesian Probabilities from\n  Marginal Constraints",
        "abstract": "  A method of calculating probability values from a system of marginal\nconstraints is presented. Previous systems for finding the probability of a\nsingle attribute have either made an independence assumption concerning the\nevidence or have required, in the worst case, time exponential in the number of\nattributes of the system. In this paper a closed form solution to the\nprobability of an attribute given the evidence is found. The closed form\nsolution, however does not enforce the (non-linear) constraint that all terms\nin the underlying distribution be positive. The equation requires O(r^3) steps\nto evaluate, where r is the number of independent marginal constraints\ndescribing the system at the time of evaluation. Furthermore, a marginal\nconstraint may be exchanged with a new constraint, and a new solution\ncalculated in O(r^2) steps. This method is appropriate for calculating\nprobabilities in a real time expert system\n"
      },
      {
        "title": "Cautious Propagation in Bayesian Networks",
        "abstract": "  Consider the situation where some evidence e has been entered to a Bayesian\nnetwork. When performing conflict analysis, sensitivity analysis, or when\nanswering questions like \"What if the finding on X had been y instead of x?\"\nyou need probabilities P (e'| h), where e' is a subset of e, and h is a\nconfiguration of a (possibly empty) set of variables. Cautious propagation is a\nmodification of HUGIN propagation into a Shafer-Shenoy-like architecture. It is\nless efficient than HUGIN propagation; however, it provides easy access to P\n(e'| h) for a great deal of relevant subsets e'.\n"
      },
      {
        "title": "d-Separation: From Theorems to Algorithms",
        "abstract": "  An efficient algorithm is developed that identifies all independencies\nimplied by the topology of a Bayesian network. Its correctness and maximality\nstems from the soundness and completeness of d-separation with respect to\nprobability theory. The algorithm runs in time O (l E l) where E is the number\nof edges in the network.\n"
      },
      {
        "title": "Belief and Surprise - A Belief-Function Formulation",
        "abstract": "  We motivate and describe a theory of belief in this paper. This theory is\ndeveloped with the following view of human belief in mind. Consider the belief\nthat an event E will occur (or has occurred or is occurring). An agent either\nentertains this belief or does not entertain this belief (i.e., there is no\n\"grade\" in entertaining the belief). If the agent chooses to exercise \"the will\nto believe\" and entertain this belief, he/she/it is entitled to a degree of\nconfidence c (1 > c > 0) in doing so. Adopting this view of human belief, we\nconjecture that whenever an agent entertains the belief that E will occur with\nc degree of confidence, the agent will be surprised (to the extent c) upon\nrealizing that E did not occur.\n"
      },
      {
        "title": "Pruning Bayesian Networks for Efficient Computation",
        "abstract": "  This paper analyzes the circumstances under which Bayesian networks can be\npruned in order to reduce computational complexity without altering the\ncomputation for variables of interest. Given a problem instance which consists\nof a query and evidence for a set of nodes in the network, it is possible to\ndelete portions of the network which do not participate in the computation for\nthe query. Savings in computational complexity can be large when the original\nnetwork is not singly connected. Results analogous to those described in this\npaper have been derived before [Geiger, Verma, and Pearl 89, Shachter 88] but\nthe implications for reducing complexity of the computations in Bayesian\nnetworks have not been stated explicitly. We show how a preprocessing step can\nbe used to prune a Bayesian network prior to using standard algorithms to solve\na given problem instance. We also show how our results can be used in a\nparallel distributed implementation in order to achieve greater savings. We\ndefine a computationally equivalent subgraph of a Bayesian network. The\nalgorithm developed in [Geiger, Verma, and Pearl 89] is modified to construct\nthe subgraphs described in this paper with O(e) complexity, where e is the\nnumber of edges in the Bayesian network. Finally, we define a minimal\ncomputationally equivalent subgraph and prove that the subgraphs described are\nminimal.\n"
      },
      {
        "title": "PDDL2.1: An Extension to PDDL for Expressing Temporal Planning Domains",
        "abstract": "  In recent years research in the planning community has moved increasingly\ntoward s application of planners to realistic problems involving both time and\nmany typ es of resources. For example, interest in planning demonstrated by the\nspace res earch community has inspired work in observation scheduling,\nplanetary rover ex ploration and spacecraft control domains. Other temporal and\nresource-intensive domains including logistics planning, plant control and\nmanufacturing have also helped to focus the community on the modelling and\nreasoning issues that must be confronted to make planning technology meet the\nchallenges of application. The International Planning Competitions have acted\nas an important motivating fo rce behind the progress that has been made in\nplanning since 1998. The third com petition (held in 2002) set the planning\ncommunity the challenge of handling tim e and numeric resources. This\nnecessitated the development of a modelling langua ge capable of expressing\ntemporal and numeric properties of planning domains. In this paper we describe\nthe language, PDDL2.1, that was used in the competition. We describe the syntax\nof the language, its formal semantics and the validation of concurrent plans.\nWe observe that PDDL2.1 has considerable modelling power --- exceeding the\ncapabilities of current planning technology --- and presents a number of\nimportant challenges to the research community.\n"
      },
      {
        "title": "The Planning Spectrum - One, Two, Three, Infinity",
        "abstract": "  Linear Temporal Logic (LTL) is widely used for defining conditions on the\nexecution paths of dynamic systems. In the case of dynamic systems that allow\nfor nondeterministic evolutions, one has to specify, along with an LTL formula\nf, which are the paths that are required to satisfy the formula. Two extreme\ncases are the universal interpretation A.f, which requires that the formula be\nsatisfied for all execution paths, and the existential interpretation E.f,\nwhich requires that the formula be satisfied for some execution path.\n  When LTL is applied to the definition of goals in planning problems on\nnondeterministic domains, these two extreme cases are too restrictive. It is\noften impossible to develop plans that achieve the goal in all the\nnondeterministic evolutions of a system, and it is too weak to require that the\ngoal is satisfied by some execution.\n  In this paper we explore alternative interpretations of an LTL formula that\nare between these extreme cases. We define a new language that permits an\narbitrary combination of the A and E quantifiers, thus allowing, for instance,\nto require that each finite execution can be extended to an execution\nsatisfying an LTL formula (AE.f), or that there is some finite execution whose\nextensions all satisfy an LTL formula (EA.f). We show that only eight of these\ncombinations of path quantifiers are relevant, corresponding to an alternation\nof the quantifiers of length one (A and E), two (AE and EA), three (AEA and\nEAE), and infinity ((AE)* and (EA)*). We also present a planning algorithm for\nthe new language that is based on an automata-theoretic approach, and study its\ncomplexity.\n"
      },
      {
        "title": "A Unifying Survey of Reinforced, Sensitive and Stigmergic Agent-Based\n  Approaches for E-GTSP",
        "abstract": "  The Generalized Traveling Salesman Problem (GTSP) is one of the NP-hard\ncombinatorial optimization problems. A variant of GTSP is E-GTSP where E,\nmeaning equality, has the constraint: exactly one node from a cluster of a\ngraph partition is visited. The main objective of the E-GTSP is to find a\nminimum cost tour passing through exactly one node from each cluster of an\nundirected graph. Agent-based approaches involving are successfully used\nnowadays for solving real life complex problems. The aim of the current paper\nis to illustrate some variants of agent-based algorithms including ant-based\nmodels with specific properties for solving E-GTSP.\n"
      },
      {
        "title": "Generating Bayesian Networks from Probability Logic Knowledge Bases",
        "abstract": "  We present a method for dynamically generating Bayesian networks from\nknowledge bases consisting of first-order probability logic sentences. We\npresent a subset of probability logic sufficient for representing the class of\nBayesian networks with discrete-valued nodes. We impose constraints on the form\nof the sentences that guarantee that the knowledge base contains all the\nprobabilistic information necessary to generate a network. We define the\nconcept of d-separation for knowledge bases and prove that a knowledge base\nwith independence conditions defined by d-separation is a complete\nspecification of a probability distribution. We present a network generation\nalgorithm that, given an inference problem in the form of a query Q and a set\nof evidence E, generates a network to compute P(Q|E). We prove the algorithm to\nbe correct.\n"
      },
      {
        "title": "An improved approach to attribute reduction with covering rough sets",
        "abstract": "  Attribute reduction is viewed as an important preprocessing step for pattern\nrecognition and data mining. Most of researches are focused on attribute\nreduction by using rough sets. Recently, Tsang et al. discussed attribute\nreduction with covering rough sets in the paper [E. C.C. Tsang, D. Chen, Daniel\nS. Yeung, Approximations and reducts with covering generalized rough sets,\nComputers and Mathematics with Applications 56 (2008) 279-289], where an\napproach based on discernibility matrix was presented to compute all attribute\nreducts. In this paper, we provide an improved approach by constructing simpler\ndiscernibility matrix with covering rough sets, and then proceed to improve\nsome characterizations of attribute reduction provided by Tsang et al. It is\nproved that the improved discernible matrix is equivalent to the old one, but\nthe computational complexity of discernible matrix is greatly reduced.\n"
      },
      {
        "title": "Evidential Reasoning in a Categorial Perspective: Conjunction and\n  Disjunction of Belief Functions",
        "abstract": "  The categorial approach to evidential reasoning can be seen as a combination\nof the probability kinematics approach of Richard Jeffrey (1965) and the\nmaximum (cross-) entropy inference approach of E. T. Jaynes (1957). As a\nconsequence of that viewpoint, it is well known that category theory provides\nnatural definitions for logical connectives. In particular, disjunction and\nconjunction are modelled by general categorial constructions known as products\nand coproducts. In this paper, I focus mainly on Dempster-Shafer theory of\nbelief functions for which I introduce a category I call Dempster?s category. I\nprove the existence of and give explicit formulas for conjunction and\ndisjunction in the subcategory of separable belief functions. In Dempster?s\ncategory, the new defined conjunction can be seen as the most cautious\nconjunction of beliefs, and thus no assumption about distinctness (of the\nsources) of beliefs is needed as opposed to Dempster?s rule of combination,\nwhich calls for distinctness (of the sources) of beliefs.\n"
      },
      {
        "title": "Probabilistic Reasoning about Actions in Nonmonotonic Causal Theories",
        "abstract": "  We present the language {m P}{cal C}+ for probabilistic reasoning about\nactions, which is a generalization of the action language {cal C}+ that allows\nto deal with probabilistic as well as nondeterministic effects of actions. We\ndefine a formal semantics of {m P}{cal C}+ in terms of probabilistic\ntransitions between sets of states. Using a concept of a history and its belief\nstate, we then show how several important problems in reasoning about actions\ncan be concisely formulated in our formalism.\n"
      },
      {
        "title": "Belief and Surprise - A Belief-Function Formulation",
        "abstract": "  We motivate and describe a theory of belief in this paper. This theory is\ndeveloped with the following view of human belief in mind. Consider the belief\nthat an event E will occur (or has occurred or is occurring). An agent either\nentertains this belief or does not entertain this belief (i.e., there is no\n\"grade\" in entertaining the belief). If the agent chooses to exercise \"the will\nto believe\" and entertain this belief, he/she/it is entitled to a degree of\nconfidence c (1 > c > 0) in doing so. Adopting this view of human belief, we\nconjecture that whenever an agent entertains the belief that E will occur with\nc degree of confidence, the agent will be surprised (to the extent c) upon\nrealizing that E did not occur.\n"
      },
      {
        "title": "Some improved results on communication between information systems",
        "abstract": "  To study the communication between information systems, Wang et al. [C. Wang,\nC. Wu, D. Chen, Q. Hu, and C. Wu, Communicating between information systems,\nInformation Sciences 178 (2008) 3228-3239] proposed two concepts of type-1 and\ntype-2 consistent functions. Some properties of such functions and induced\nrelation mappings have been investigated there. In this paper, we provide an\nimprovement of the aforementioned work by disclosing the symmetric relationship\nbetween type-1 and type-2 consistent functions. We present more properties of\nconsistent functions and induced relation mappings and improve upon several\ndeficient assertions in the original work. In particular, we unify and extend\ntype-1 and type-2 consistent functions into the so-called\nneighborhood-consistent functions. This provides a convenient means for\nstudying the communication between information systems based on various\nneighborhoods.\n"
      },
      {
        "title": "Randomized Algorithms for the Loop Cutset Problem",
        "abstract": "  We show how to find a minimum weight loop cutset in a Bayesian network with\nhigh probability. Finding such a loop cutset is the first step in the method of\nconditioning for inference. Our randomized algorithm for finding a loop cutset\noutputs a minimum loop cutset after O(c 6^k kn) steps with probability at least\n1 - (1 - 1/(6^k))^c6^k, where c > 1 is a constant specified by the user, k is\nthe minimal size of a minimum weight loop cutset, and n is the number of\nvertices. We also show empirically that a variant of this algorithm often finds\na loop cutset that is closer to the minimum weight loop cutset than the ones\nfound by the best deterministic algorithms known.\n"
      },
      {
        "title": "Model Based Framework for Estimating Mutation Rate of Hepatitis C Virus\n  in Egypt",
        "abstract": "  Hepatitis C virus (HCV) is a widely spread disease all over the world. HCV\nhas very high mutation rate that makes it resistant to antibodies. Modeling HCV\nto identify the virus mutation process is essential to its detection and\npredicting its evolution. This paper presents a model based framework for\nestimating mutation rate of HCV in two steps. Firstly profile hidden Markov\nmodel (PHMM) architecture was builder to select the sequences which represents\nsequence per year. Secondly mutation rate was calculated by using pair-wise\ndistance method between sequences. A pilot study is conducted on NS5B zone of\nHCV dataset of genotype 4 subtype a (HCV4a) in Egypt.\n"
      },
      {
        "title": "An improved approach to attribute reduction with covering rough sets",
        "abstract": "  Attribute reduction is viewed as an important preprocessing step for pattern\nrecognition and data mining. Most of researches are focused on attribute\nreduction by using rough sets. Recently, Tsang et al. discussed attribute\nreduction with covering rough sets in the paper [E. C.C. Tsang, D. Chen, Daniel\nS. Yeung, Approximations and reducts with covering generalized rough sets,\nComputers and Mathematics with Applications 56 (2008) 279-289], where an\napproach based on discernibility matrix was presented to compute all attribute\nreducts. In this paper, we provide an improved approach by constructing simpler\ndiscernibility matrix with covering rough sets, and then proceed to improve\nsome characterizations of attribute reduction provided by Tsang et al. It is\nproved that the improved discernible matrix is equivalent to the old one, but\nthe computational complexity of discernible matrix is greatly reduced.\n"
      },
      {
        "title": "A Complete Calculus for Possibilistic Logic Programming with Fuzzy\n  Propositional Variables",
        "abstract": "  In this paper we present a propositional logic programming language for\nreasoning under possibilistic uncertainty and representing vague knowledge.\nFormulas are represented by pairs (A, c), where A is a many-valued proposition\nand c is value in the unit interval [0,1] which denotes a lower bound on the\nbelief on A in terms of necessity measures. Belief states are modeled by\npossibility distributions on the set of all many-valued interpretations. In\nthis framework, (i) we define a syntax and a semantics of the general\nunderlying uncertainty logic; (ii) we provide a modus ponens-style calculus for\na sublanguage of Horn-rules and we prove that it is complete for determining\nthe maximum degree of possibilistic belief with which a fuzzy propositional\nvariable can be entailed from a set of formulas; and finally, (iii) we show how\nthe computation of a partial matching between fuzzy propositional variables, in\nterms of necessity measures for fuzzy sets, can be included in our logic\nprogramming system.\n"
      },
      {
        "title": "A Monte-Carlo Algorithm for Dempster-Shafer Belief",
        "abstract": "  A very computationally-efficient Monte-Carlo algorithm for the calculation of\nDempster-Shafer belief is described. If Bel is the combination using Dempster's\nRule of belief functions Bel, ..., Bel,7, then, for subset b of the frame C),\nBel(b) can be calculated in time linear in 1(31 and m (given that the weight of\nconflict is bounded). The algorithm can also be used to improve the complexity\nof the Shenoy-Shafer algorithms on Markov trees, and be generalised to\ncalculate Dempster-Shafer Belief over other logics.\n"
      },
      {
        "title": "MCE Reasoning in Recursive Causal Networks",
        "abstract": "  A probabilistic method of reasoning under uncertainty is proposed based on\nthe principle of Minimum Cross Entropy (MCE) and concept of Recursive Causal\nModel (RCM). The dependency and correlations among the variables are described\nin a special language BNDL (Belief Networks Description Language). Beliefs are\npropagated among the clauses of the BNDL programs representing the underlying\nprobabilistic distributions. BNDL interpreters in both Prolog and C has been\ndeveloped and the performance of the method is compared with those of the\nothers.\n"
      },
      {
        "title": "Counterfactuals and Policy Analysis in Structural Models",
        "abstract": "  Evaluation of counterfactual queries (e.g., \"If A were true, would C have\nbeen true?\") is important to fault diagnosis, planning, determination of\nliability, and policy analysis. We present a method of revaluating\ncounterfactuals when the underlying causal model is represented by structural\nmodels - a nonlinear generalization of the simultaneous equations models\ncommonly used in econometrics and social sciences. This new method provides a\ncoherent means for evaluating policies involving the control of variables\nwhich, prior to enacting the policy were influenced by other variables in the\nsystem.\n"
      },
      {
        "title": "Logarithmic Time Parallel Bayesian Inference",
        "abstract": "  I present a parallel algorithm for exact probabilistic inference in Bayesian\nnetworks. For polytree networks with n variables, the worst-case time\ncomplexity is O(log n) on a CREW PRAM (concurrent-read, exclusive-write\nparallel random-access machine) with n processors, for any constant number of\nevidence variables. For arbitrary networks, the time complexity is O(r^{3w}*log\nn) for n processors, or O(w*log n) for r^{3w}*n processors, where r is the\nmaximum range of any variable, and w is the induced width (the maximum clique\nsize), after moralizing and triangulating the network.\n"
      },
      {
        "title": "Computational Aspects of the Mobius Transform",
        "abstract": "  In this paper we associate with every (directed) graph G a transformation\ncalled the Mobius transformation of the graph G. The Mobius transformation of\nthe graph (O) is of major significance for Dempster-Shafer theory of evidence.\nHowever, because it is computationally very heavy, the Mobius transformation\ntogether with Dempster's rule of combination is a major obstacle to the use of\nDempster-Shafer theory for handling uncertainty in expert systems. The major\ncontribution of this paper is the discovery of the 'fast Mobius\ntransformations' of (O). These 'fast Mobius transformations' are the fastest\nalgorithms for computing the Mobius transformation of (O). As an easy but\nuseful application, we provide, via the commonality function, an algorithm for\ncomputing Dempster's rule of combination which is much faster than the usual\none.\n"
      },
      {
        "title": "Improving the Performance of maxRPC",
        "abstract": "  Max Restricted Path Consistency (maxRPC) is a local consistency for binary\nconstraints that can achieve considerably stronger pruning than arc\nconsistency. However, existing maxRRC algorithms suffer from overheads and\nredundancies as they can repeatedly perform many constraint checks without\ntriggering any value deletions. In this paper we propose techniques that can\nboost the performance of maxRPC algorithms. These include the combined use of\ntwo data structures to avoid many redundant constraint checks, and heuristics\nfor the efficient ordering and execution of certain operations. Based on these,\nwe propose two closely related algorithms. The first one which is a maxRPC\nalgorithm with optimal O(end^3) time complexity, displays good performance when\nused stand-alone, but is expensive to apply during search. The second one\napproximates maxRPC and has O(en^2d^4) time complexity, but a restricted\nversion with O(end^4) complexity can be very efficient when used during search.\nBoth algorithms have O(ed) space complexity. Experimental results demonstrate\nthat the resulting methods constantly outperform previous algorithms for\nmaxRPC, often by large margins, and constitute a more than viable alternative\nto arc consistency on many problems.\n"
      },
      {
        "title": "New Worst-Case Upper Bound for #XSAT",
        "abstract": "  An algorithm running in O(1.1995n) is presented for counting models for exact\nsatisfiability formulae(#XSAT). This is faster than the previously best\nalgorithm which runs in O(1.2190n). In order to improve the efficiency of the\nalgorithm, a new principle, i.e. the common literals principle, is addressed to\nsimplify formulae. This allows us to eliminate more common literals. In\naddition, we firstly inject the resolution principles into solving #XSAT\nproblem, and therefore this further improves the efficiency of the algorithm.\n"
      },
      {
        "title": "GANC: Greedy Agglomerative Normalized Cut",
        "abstract": "  This paper describes a graph clustering algorithm that aims to minimize the\nnormalized cut criterion and has a model order selection procedure. The\nperformance of the proposed algorithm is comparable to spectral approaches in\nterms of minimizing normalized cut. However, unlike spectral approaches, the\nproposed algorithm scales to graphs with millions of nodes and edges. The\nalgorithm consists of three components that are processed sequentially: a\ngreedy agglomerative hierarchical clustering procedure, model order selection,\nand a local refinement.\n  For a graph of n nodes and O(n) edges, the computational complexity of the\nalgorithm is O(n log^2 n), a major improvement over the O(n^3) complexity of\nspectral methods. Experiments are performed on real and synthetic networks to\ndemonstrate the scalability of the proposed approach, the effectiveness of the\nmodel order selection procedure, and the performance of the proposed algorithm\nin terms of minimizing the normalized cut metric.\n"
      },
      {
        "title": "The SeqBin Constraint Revisited",
        "abstract": "  We revisit the SeqBin constraint. This meta-constraint subsumes a number of\nimportant global constraints like Change, Smooth and IncreasingNValue. We show\nthat the previously proposed filtering algorithm for SeqBin has two drawbacks\neven under strong restrictions: it does not detect bounds disentailment and it\nis not idempotent. We identify the cause for these problems, and propose a new\npropagator that overcomes both issues. Our algorithm is based on a connection\nto the problem of finding a path of a given cost in a restricted $n$-partite\ngraph. Our propagator enforces domain consistency in O(nd^2) and, for special\ncases of SeqBin that include Change, Smooth and IncreasingNValue, in O(nd)\ntime.\n"
      },
      {
        "title": "Flow-Based Propagators for the SEQUENCE and Related Global Constraints",
        "abstract": "  We propose new filtering algorithms for the SEQUENCE constraint and some\nextensions of the SEQUENCE constraint based on network flows. We enforce domain\nconsistency on the SEQUENCE constraint in $O(n^2)$ time down a branch of the\nsearch tree. This improves upon the best existing domain consistency algorithm\nby a factor of $O(\\log n)$. The flows used in these algorithms are derived from\na linear program. Some of them differ from the flows used to propagate global\nconstraints like GCC since the domains of the variables are encoded as costs on\nthe edges rather than capacities. Such flows are efficient for maintaining\nbounds consistency over large domains and may be useful for other global\nconstraints.\n"
      },
      {
        "title": "The Weighted CFG Constraint",
        "abstract": "  We introduce the weighted CFG constraint and propose a propagation algorithm\nthat enforces domain consistency in $O(n^3|G|)$ time. We show that this\nalgorithm can be decomposed into a set of primitive arithmetic constraints\nwithout hindering propagation.\n"
      },
      {
        "title": "Dynamic Network Updating Techniques For Diagnostic Reasoning",
        "abstract": "  A new probabilistic network construction system, DYNASTY, is proposed for\ndiagnostic reasoning given variables whose probabilities change over time.\nDiagnostic reasoning is formulated as a sequential stochastic process, and is\nmodeled using influence diagrams. Given a set O of observations, DYNASTY\ncreates an influence diagram in order to devise the best action given O.\nSensitivity analyses are conducted to determine if the best network has been\ncreated, given the uncertainty in network parameters and topology. DYNASTY uses\nan equivalence class approach to provide decision thresholds for the\nsensitivity analysis. This equivalence-class approach to diagnostic reasoning\ndifferentiates diagnoses only if the required actions are different. A set of\nnetwork-topology updating algorithms are proposed for dynamically updating the\nnetwork when necessary.\n"
      },
      {
        "title": "A Polynomial Time Algorithm for Finding Bayesian Probabilities from\n  Marginal Constraints",
        "abstract": "  A method of calculating probability values from a system of marginal\nconstraints is presented. Previous systems for finding the probability of a\nsingle attribute have either made an independence assumption concerning the\nevidence or have required, in the worst case, time exponential in the number of\nattributes of the system. In this paper a closed form solution to the\nprobability of an attribute given the evidence is found. The closed form\nsolution, however does not enforce the (non-linear) constraint that all terms\nin the underlying distribution be positive. The equation requires O(r^3) steps\nto evaluate, where r is the number of independent marginal constraints\ndescribing the system at the time of evaluation. Furthermore, a marginal\nconstraint may be exchanged with a new constraint, and a new solution\ncalculated in O(r^2) steps. This method is appropriate for calculating\nprobabilities in a real time expert system\n"
      },
      {
        "title": "Computational Aspects of the Mobius Transform",
        "abstract": "  In this paper we associate with every (directed) graph G a transformation\ncalled the Mobius transformation of the graph G. The Mobius transformation of\nthe graph (O) is of major significance for Dempster-Shafer theory of evidence.\nHowever, because it is computationally very heavy, the Mobius transformation\ntogether with Dempster's rule of combination is a major obstacle to the use of\nDempster-Shafer theory for handling uncertainty in expert systems. The major\ncontribution of this paper is the discovery of the 'fast Mobius\ntransformations' of (O). These 'fast Mobius transformations' are the fastest\nalgorithms for computing the Mobius transformation of (O). As an easy but\nuseful application, we provide, via the commonality function, an algorithm for\ncomputing Dempster's rule of combination which is much faster than the usual\none.\n"
      },
      {
        "title": "On Testing Whether an Embedded Bayesian Network Represents a Probability\n  Model",
        "abstract": "  Testing the validity of probabilistic models containing unmeasured (hidden)\nvariables is shown to be a hard task. We show that the task of testing whether\nmodels are structurally incompatible with the data at hand, requires an\nexponential number of independence evaluations, each of the form: \"X is\nconditionally independent of Y, given Z.\" In contrast, a linear number of such\nevaluations is required to test a standard Bayesian network (one per vertex).\nOn the positive side, we show that if a network with hidden variables G has a\ntree skeleton, checking whether G represents a given probability model P\nrequires the polynomial number of such independence evaluations. Moreover, we\nprovide an algorithm that efficiently constructs a tree-structured Bayesian\nnetwork (with hidden variables) that represents P if such a network exists, and\nfurther recognizes when such a network does not exist.\n"
      },
      {
        "title": "On Reasonable and Forced Goal Orderings and their Use in an\n  Agenda-Driven Planning Algorithm",
        "abstract": "  The paper addresses the problem of computing goal orderings, which is one of\nthe longstanding issues in AI planning. It makes two new contributions. First,\nit formally defines and discusses two different goal orderings, which are\ncalled the reasonable and the forced ordering. Both orderings are defined for\nsimple STRIPS operators as well as for more complex ADL operators supporting\nnegation and conditional effects. The complexity of these orderings is\ninvestigated and their practical relevance is discussed. Secondly, two\ndifferent methods to compute reasonable goal orderings are developed. One of\nthem is based on planning graphs, while the other investigates the set of\nactions directly. Finally, it is shown how the ordering relations, which have\nbeen derived for a given set of goals G, can be used to compute a so-called\ngoal agenda that divides G into an ordered set of subgoals. Any planner can\nthen, in principle, use the goal agenda to plan for increasing sets of\nsubgoals. This can lead to an exponential complexity reduction, as the solution\nto a complex planning problem is found by solving easier subproblems. Since\nonly a polynomial overhead is caused by the goal agenda computation, a\npotential exists to dramatically speed up planning algorithms as we demonstrate\nin the empirical evaluation, where we use this method in the IPP planner.\n"
      },
      {
        "title": "Marginalizing in Undirected Graph and Hypergraph Models",
        "abstract": "  Given an undirected graph G or hypergraph X model for a given set of\nvariables V, we introduce two marginalization operators for obtaining the\nundirected graph GA or hypergraph HA associated with a given subset A c V such\nthat the marginal distribution of A factorizes according to GA or HA,\nrespectively. Finally, we illustrate the method by its application to some\npractical examples. With them we show that hypergraph models allow defining a\nfiner factorization or performing a more precise conditional independence\nanalysis than undirected graph models.\n"
      },
      {
        "title": "Defining Explanation in Probabilistic Systems",
        "abstract": "  As probabilistic systems gain popularity and are coming into wider use, the\nneed for a mechanism that explains the system's findings and recommendations\nbecomes more critical. The system will also need a mechanism for ordering\ncompeting explanations. We examine two representative approaches to explanation\nin the literature - one due to G\\\"ardenfors and one due to Pearl - and show\nthat both suffer from significant problems. We propose an approach to defining\na notion of \"better explanation\" that combines some of the features of both\ntogether with more recent work by Pearl and others on causality.\n"
      },
      {
        "title": "Dealing with uncertainty in fuzzy inductive reasoning methodology",
        "abstract": "  The aim of this research is to develop a reasoning under uncertainty strategy\nin the context of the Fuzzy Inductive Reasoning (FIR) methodology. FIR emerged\nfrom the General Systems Problem Solving developed by G. Klir. It is a data\ndriven methodology based on systems behavior rather than on structural\nknowledge. It is a very useful tool for both the modeling and the prediction of\nthose systems for which no previous structural knowledge is available. FIR\nreasoning is based on pattern rules synthesized from the available data. The\nsize of the pattern rule base can be very large making the prediction process\nquite difficult. In order to reduce the size of the pattern rule base, it is\npossible to automatically extract classical Sugeno fuzzy rules starting from\nthe set of pattern rules. The Sugeno rule base preserves pattern rules\nknowledge as much as possible. In this process some information is lost but\nrobustness is considerably increased. In the forecasting process either the\npattern rule base or the Sugeno fuzzy rule base can be used. The first option\nis desirable when the computational resources make it possible to deal with the\noverall pattern rule base or when the extracted fuzzy rules are not accurate\nenough due to uncertainty associated to the original data. In the second\noption, the prediction process is done by means of the classical Sugeno\ninference system. If the amount of uncertainty associated to the data is small,\nthe predictions obtained using the Sugeno fuzzy rule base will be very\naccurate. In this paper a mixed pattern/fuzzy rules strategy is proposed to\ndeal with uncertainty in such a way that the best of both perspectives is used.\nAreas in the data space with a higher level of uncertainty are identified by\nmeans of the so-called error models. The prediction process in these areas\nmakes use of a mixed pattern/fuzzy rules scheme, whereas areas identified with\na lower level of uncertainty only use the Sugeno fuzzy rule base. The proposed\nstrategy is applied to a real biomedical system, i.e., the central nervous\nsystem control of the cardiovascular system.\n"
      },
      {
        "title": "Logarithmic Time Parallel Bayesian Inference",
        "abstract": "  I present a parallel algorithm for exact probabilistic inference in Bayesian\nnetworks. For polytree networks with n variables, the worst-case time\ncomplexity is O(log n) on a CREW PRAM (concurrent-read, exclusive-write\nparallel random-access machine) with n processors, for any constant number of\nevidence variables. For arbitrary networks, the time complexity is O(r^{3w}*log\nn) for n processors, or O(w*log n) for r^{3w}*n processors, where r is the\nmaximum range of any variable, and w is the induced width (the maximum clique\nsize), after moralizing and triangulating the network.\n"
      },
      {
        "title": "GANC: Greedy Agglomerative Normalized Cut",
        "abstract": "  This paper describes a graph clustering algorithm that aims to minimize the\nnormalized cut criterion and has a model order selection procedure. The\nperformance of the proposed algorithm is comparable to spectral approaches in\nterms of minimizing normalized cut. However, unlike spectral approaches, the\nproposed algorithm scales to graphs with millions of nodes and edges. The\nalgorithm consists of three components that are processed sequentially: a\ngreedy agglomerative hierarchical clustering procedure, model order selection,\nand a local refinement.\n  For a graph of n nodes and O(n) edges, the computational complexity of the\nalgorithm is O(n log^2 n), a major improvement over the O(n^3) complexity of\nspectral methods. Experiments are performed on real and synthetic networks to\ndemonstrate the scalability of the proposed approach, the effectiveness of the\nmodel order selection procedure, and the performance of the proposed algorithm\nin terms of minimizing the normalized cut metric.\n"
      },
      {
        "title": "On characterizing Inclusion of Bayesian Networks",
        "abstract": "  Every directed acyclic graph (DAG) over a finite non-empty set of variables\n(= nodes) N induces an independence model over N, which is a list of\nconditional independence statements over N.The inclusion problem is how to\ncharacterize (in graphical terms) whether all independence statements in the\nmodel induced by a DAG K are in the model induced by a second DAG L. Meek\n(1997) conjectured that this inclusion holds iff there exists a sequence of\nDAGs from L to K such that only certain 'legal' arrow reversal and 'legal'\narrow adding operations are performed to get the next DAG in the sequence.In\nthis paper we give several characterizations of inclusion of DAG models and\nverify Meek's conjecture in the case that the DAGs K and L differ in at most\none adjacency. As a warming up a rigorous proof of well-known graphical\ncharacterizations of equivalence of DAGs, which is a highly related problem, is\ngiven.\n"
      },
      {
        "title": "Reasoning With Qualitative Probabilities Can Be Tractable",
        "abstract": "  We recently described a formalism for reasoning with if-then rules that re\nexpressed with different levels of firmness [18]. The formalism interprets\nthese rules as extreme conditional probability statements, specifying orders of\nmagnitude of disbelief, which impose constraints over possible rankings of\nworlds. It was shown that, once we compute a priority function Z+ on the rules,\nthe degree to which a given query is confirmed or denied can be computed in\nO(log n`) propositional satisfiability tests, where n is the number of rules in\nthe knowledge base. In this paper, we show that computing Z+ requires O(n2 X\nlog n) satisfiability tests, not an exponential number as was conjectured in\n[18], which reduces to polynomial complexity in the case of Horn expressions.\nWe also show how reasoning with imprecise observations can be incorporated in\nour formalism and how the popular notions of belief revision and epistemic\nentrenchment are embodied naturally and tractably.\n"
      },
      {
        "title": "Bayesian Error-Bars for Belief Net Inference",
        "abstract": "  A Bayesian Belief Network (BN) is a model of a joint distribution over a\nsetof n variables, with a DAG structure to represent the immediate\ndependenciesbetween the variables, and a set of parameters (aka CPTables) to\nrepresent thelocal conditional probabilities of a node, given each assignment\nto itsparents. In many situations, these parameters are themselves random\nvariables - this may reflect the uncertainty of the domain expert, or may come\nfrom atraining sample used to estimate the parameter values. The distribution\noverthese \"CPtable variables\" induces a distribution over the response the\nBNwill return to any \"What is Pr(H | E)?\" query. This paper investigates\nthevariance of this response, showing first that it is asymptotically\nnormal,then providing its mean and asymptotical variance. We then present\naneffective general algorithm for computing this variance, which has the\nsamecomplexity as simply computing the (mean value of) the response itself -\nie,O(n 2^w), where n is the number of variables and w is the effective\ntreewidth. Finally, we provide empirical evidence that this algorithm,\nwhichincorporates assumptions and approximations, works effectively in\npractice,given only small samples.\n"
      },
      {
        "title": "On the Robustness of Most Probable Explanations",
        "abstract": "  In Bayesian networks, a Most Probable Explanation (MPE) is a complete\nvariable instantiation with a highest probability given the current evidence.\nIn this paper, we discuss the problem of finding robustness conditions of the\nMPE under single parameter changes. Specifically, we ask the question: How much\nchange in a single network parameter can we afford to apply while keeping the\nMPE unchanged? We will describe a procedure, which is the first of its kind,\nthat computes this answer for each parameter in the Bayesian network variable\nin time O(n exp(w)), where n is the number of network variables and w is its\ntreewidth.\n"
      },
      {
        "title": "Noisy Search with Comparative Feedback",
        "abstract": "  We present theoretical results in terms of lower and upper bounds on the\nquery complexity of noisy search with comparative feedback. In this search\nmodel, the noise in the feedback depends on the distance between query points\nand the search target. Consequently, the error probability in the feedback is\nnot fixed but varies for the queries posed by the search algorithm. Our results\nshow that a target out of n items can be found in O(log n) queries. We also\nshow the surprising result that for k possible answers per query, the speedup\nis not log k (as for k-ary search) but only log log k in some cases.\n"
      },
      {
        "title": "Lex-Partitioning: A New Option for BDD Search",
        "abstract": "  For the exploration of large state spaces, symbolic search using binary\ndecision diagrams (BDDs) can save huge amounts of memory and computation time.\nState sets are represented and modified by accessing and manipulating their\ncharacteristic functions. BDD partitioning is used to compute the image as the\ndisjunction of smaller subimages.\n  In this paper, we propose a novel BDD partitioning option. The partitioning\nis lexicographical in the binary representation of the states contained in the\nset that is represented by a BDD and uniform with respect to the number of\nstates represented. The motivation of controlling the state set sizes in the\npartitioning is to eventually bridge the gap between explicit and symbolic\nsearch.\n  Let n be the size of the binary state vector. We propose an O(n) ranking and\nunranking scheme that supports negated edges and operates on top of precomputed\nsatcount values. For the uniform split of a BDD, we then use unranking to\nprovide paths along which we partition the BDDs. In a shared BDD representation\nthe efforts are O(n). The algorithms are fully integrated in the CUDD library\nand evaluated in strongly solving general game playing benchmarks.\n"
      },
      {
        "title": "Local search for stable marriage problems",
        "abstract": "  The stable marriage (SM) problem has a wide variety of practical\napplications, ranging from matching resident doctors to hospitals, to matching\nstudents to schools, or more generally to any two-sided market. In the\nclassical formulation, n men and n women express their preferences (via a\nstrict total order) over the members of the other sex. Solving a SM problem\nmeans finding a stable marriage where stability is an envy-free notion: no man\nand woman who are not married to each other would both prefer each other to\ntheir partners or to being single. We consider both the classical stable\nmarriage problem and one of its useful variations (denoted SMTI) where the men\nand women express their preferences in the form of an incomplete preference\nlist with ties over a subset of the members of the other sex. Matchings are\npermitted only with people who appear in these lists, an we try to find a\nstable matching that marries as many people as possible. Whilst the SM problem\nis polynomial to solve, the SMTI problem is NP-hard. We propose to tackle both\nproblems via a local search approach, which exploits properties of the problems\nto reduce the size of the neighborhood and to make local moves efficiently. We\nevaluate empirically our algorithm for SM problems by measuring its runtime\nbehaviour and its ability to sample the lattice of all possible stable\nmarriages. We evaluate our algorithm for SMTI problems in terms of both its\nruntime behaviour and its ability to find a maximum cardinality stable\nmarriage.For SM problems, the number of steps of our algorithm grows only as\nO(nlog(n)), and that it samples very well the set of all stable marriages. It\nis thus a fair and efficient approach to generate stable marriages.Furthermore,\nour approach for SMTI problems is able to solve large problems, quickly\nreturning stable matchings of large and often optimal size despite the\nNP-hardness of this problem.\n"
      },
      {
        "title": "Steepest Ascent Hill Climbing For A Mathematical Problem",
        "abstract": "  The paper proposes artificial intelligence technique called hill climbing to\nfind numerical solutions of Diophantine Equations. Such equations are important\nas they have many applications in fields like public key cryptography, integer\nfactorization, algebraic curves, projective curves and data dependency in super\ncomputers. Importantly, it has been proved that there is no general method to\nfind solutions of such equations. This paper is an attempt to find numerical\nsolutions of Diophantine equations using steepest ascent version of Hill\nClimbing. The method, which uses tree representation to depict possible\nsolutions of Diophantine equations, adopts a novel methodology to generate\nsuccessors. The heuristic function used help to make the process of finding\nsolution as a minimization process. The work illustrates the effectiveness of\nthe proposed methodology using a class of Diophantine equations given by a1. x1\np1 + a2. x2 p2 + ...... + an . xn pn = N where ai and N are integers. The\nexperimental results validate that the procedure proposed is successful in\nfinding solutions of Diophantine Equations with sufficiently large powers and\nlarge number of variables.\n"
      },
      {
        "title": "Argument Calculus and Networks",
        "abstract": "  A major reason behind the success of probability calculus is that it\npossesses a number of valuable tools, which are based on the notion of\nprobabilistic independence. In this paper, I identify a notion of logical\nindependence that makes some of these tools available to a class of\npropositional databases, called argument databases. Specifically, I suggest a\ngraphical representation of argument databases, called argument networks, which\nresemble Bayesian networks. I also suggest an algorithm for reasoning with\nargument networks, which resembles a basic algorithm for reasoning with\nBayesian networks. Finally, I show that argument networks have several\napplications: Nonmonotonic reasoning, truth maintenance, and diagnosis.\n"
      },
      {
        "title": "PDDL 2.1: Representation vs. Computation",
        "abstract": "  I comment on the PDDL 2.1 language and its use in the planning competition,\nfocusing on the choices made for accommodating time and concurrency. I also\ndiscuss some methodological issues that have to do with the move toward more\nexpressive planning languages and the balance needed in planning research\nbetween semantics and computation.\n"
      },
      {
        "title": "Toward a Category Theory Design of Ontological Knowledge Bases",
        "abstract": "  I discuss (ontologies_and_ontological_knowledge_bases /\nformal_methods_and_theories) duality and its category theory extensions as a\nstep toward a solution to Knowledge-Based Systems Theory. In particular I focus\non the example of the design of elements of ontologies and ontological\nknowledge bases of next three electronic courses: Foundations of Research\nActivities, Virtual Modeling of Complex Systems and Introduction to String\nTheory.\n"
      },
      {
        "title": "An efficient approach for finding the MPE in belief networks",
        "abstract": "  Given a belief network with evidence, the task of finding the I most probable\nexplanations (MPE) in the belief network is that of identifying and ordering\nthe I most probable instantiations of the non-evidence nodes of the belief\nnetwork. Although many approaches have been proposed for solving this problem,\nmost work only for restricted topologies (i.e., singly connected belief\nnetworks). In this paper, we will present a new approach for finding I MPEs in\nan arbitrary belief network. First, we will present an algorithm for finding\nthe MPE in a belief network. Then, we will present a linear time algorithm for\nfinding the next MPE after finding the first MPE. And finally, we will discuss\nthe problem of finding the MPE for a subset of variables of a belief network,\nand show that the problem can be efficiently solved by this approach.\n"
      },
      {
        "title": "AI 3D Cybug Gaming",
        "abstract": "  In this short paper I briefly discuss 3D war Game based on artificial\nintelligence concepts called AI WAR. Going in to the details, I present the\nimportance of CAICL language and how this language is used in AI WAR. Moreover\nI also present a designed and implemented 3D War Cybug for AI WAR using CAICL\nand discus the implemented strategy to defeat its enemies during the game life.\n"
      },
      {
        "title": "The Power of Modeling - a Response to PDDL2.1",
        "abstract": "  In this commentary I argue that although PDDL is a very useful standard for\nthe planning competition, its design does not properly consider the issue of\ndomain modeling. Hence, I would not advocate its use in specifying planning\ndomains outside of the context of the planning competition. Rather, the field\nneeds to explore different approaches and grapple more directly with the\nproblem of effectively modeling and utilizing all of the diverse pieces of\nknowledge we typically have about planning domains.\n"
      },
      {
        "title": "Defeasible Decisions: What the Proposal is and isn't",
        "abstract": "  In two recent papers, I have proposed a description of decision analysis that\ndiffers from the Bayesian picture painted by Savage, Jeffrey and other classic\nauthors. Response to this view has been either overly enthusiastic or unduly\npessimistic. In this paper I try to place the idea in its proper place, which\nmust be somewhere in between. Looking at decision analysis as defeasible\nreasoning produces a framework in which planning and decision theory can be\nintegrated, but work on the details has barely begun. It also produces a\nframework in which the meta-decision regress can be stopped in a reasonable\nway, but it does not allow us to ignore meta-level decisions. The heuristics\nfor producing arguments that I have presented are only supposed to be\nsuggestive; but they are not open to the egregious errors about which some have\nworried. And though the idea is familiar to those who have studied heuristic\nsearch, it is somewhat richer because the control of dialectic is more\ninteresting than the deepening of search.\n"
      },
      {
        "title": "Policy Iteration for Factored MDPs",
        "abstract": "  Many large MDPs can be represented compactly using a dynamic Bayesian\nnetwork. Although the structure of the value function does not retain the\nstructure of the process, recent work has shown that value functions in\nfactored MDPs can often be approximated well using a decomposed value function:\na linear combination of <I>restricted</I> basis functions, each of which refers\nonly to a small subset of variables. An approximate value function for a\nparticular policy can be computed using approximate dynamic programming, but\nthis approach (and others) can only produce an approximation relative to a\ndistance metric which is weighted by the stationary distribution of the current\npolicy. This type of weighted projection is ill-suited to policy improvement.\nWe present a new approach to value determination, that uses a simple\nclosed-form computation to directly compute a least-squares decomposed\napproximation to the value function <I>for any weights</I>. We then use this\nvalue determination algorithm as a subroutine in a policy iteration process. We\nshow that, under reasonable restrictions, the policies induced by a factored\nvalue function are compactly represented, and can be manipulated efficiently in\na policy iteration process. We also present a method for computing error bounds\nfor decomposed value functions using a variable-elimination algorithm for\nfunction optimization. The complexity of all of our algorithms depends on the\nfactorization of system dynamics and of the approximate value function.\n"
      },
      {
        "title": "Evidential Reasoning in a Categorial Perspective: Conjunction and\n  Disjunction of Belief Functions",
        "abstract": "  The categorial approach to evidential reasoning can be seen as a combination\nof the probability kinematics approach of Richard Jeffrey (1965) and the\nmaximum (cross-) entropy inference approach of E. T. Jaynes (1957). As a\nconsequence of that viewpoint, it is well known that category theory provides\nnatural definitions for logical connectives. In particular, disjunction and\nconjunction are modelled by general categorial constructions known as products\nand coproducts. In this paper, I focus mainly on Dempster-Shafer theory of\nbelief functions for which I introduce a category I call Dempster?s category. I\nprove the existence of and give explicit formulas for conjunction and\ndisjunction in the subcategory of separable belief functions. In Dempster?s\ncategory, the new defined conjunction can be seen as the most cautious\nconjunction of beliefs, and thus no assumption about distinctness (of the\nsources) of beliefs is needed as opposed to Dempster?s rule of combination,\nwhich calls for distinctness (of the sources) of beliefs.\n"
      },
      {
        "title": "Probabilistic and Non-Monotonic Inference",
        "abstract": "  (l) I have enough evidence to render the sentence S probable. (la) So,\nrelative to what I know, it is rational of me to believe S. (2) Now that I have\nmore evidence, S may no longer be probable. (2a) So now, relative to what I\nknow, it is not rational of me to believe S. These seem a perfectly ordinary,\ncommon sense, pair of situations. Generally and vaguely, I take them to embody\nwhat I shall call probabilistic inference. This form of inference is clearly\nnon-monotonic. Relatively few people have taken this form of inference, based\non high probability, to serve as a foundation for non-monotonic logic or for a\nlogical or defeasible inference. There are exceptions: Jane Nutter [16] thinks\nthat sometimes probability has something to do with non-monotonic reasoning.\nJudea Pearl [ 17] has recently been exploring the possibility. There are any\nnumber of people whom one might call probability enthusiasts who feel that\nprobability provides all the answers by itself, with no need of help from\nlogic. Cheeseman [1], Henrion [5] and others think it useful to look at a\ndistribution of probabilities over a whole algebra of statements, to update\nthat distribution in the light of new evidence, and to use the latest updated\ndistribution of probability over the algebra as a basis for planning and\ndecision making. A slightly weaker form of this approach is captured by Nilsson\n[15], where one assumes certain probabilities for certain statements, and\ninfers the probabilities, or constraints on the probabilities of other\nstatement. None of this corresponds to what I call probabilistic inference. All\nof the inference that is taking place, either in Bayesian updating, or in\nprobabilistic logic, is strictly deductive. Deductive inference, particularly\nthat concerned with the distribution of classical probabilities or chances, is\nof great importance. But this is not to say that there is no important role for\nwhat earlier logicians have called \"ampliative\" or \"inductive\" or \"scientific\"\ninference, in which the conclusion goes beyond the premises, asserts more than\ndo the premises. This depends on what David Israel [6] has called \"real rules\nof inference\". It is characteristic of any such logic or inference procedure\nthat it can go wrong: that statements accepted at one point may be rejected at\na later point. Research underlying the results reported here has been partially\nsupported by the Signals Warfare Center of the United States Army.\n"
      },
      {
        "title": "Hybrid Model for Solving Multi-Objective Problems Using Evolutionary\n  Algorithm and Tabu Search",
        "abstract": "  This paper presents a new multi-objective hybrid model that makes cooperation\nbetween the strength of research of neighborhood methods presented by the tabu\nsearch (TS) and the important exploration capacity of evolutionary algorithm.\nThis model was implemented and tested in benchmark functions (ZDT1, ZDT2, and\nZDT3), using a network of computers.\n"
      },
      {
        "title": "On the comparison of plans: Proposition of an instability measure for\n  dynamic machine scheduling",
        "abstract": "  On the basis of an analysis of previous research, we present a generalized\napproach for measuring the difference of plans with an exemplary application to\nmachine scheduling. Our work is motivated by the need for such measures, which\nare used in dynamic scheduling and planning situations. In this context,\nquantitative approaches are needed for the assessment of the robustness and\nstability of schedules. Obviously, any `robustness' or `stability' of plans has\nto be defined w. r. t. the particular situation and the requirements of the\nhuman decision maker. Besides the proposition of an instability measure, we\ntherefore discuss possibilities of obtaining meaningful information from the\ndecision maker for the implementation of the introduced approach.\n"
      },
      {
        "title": "Etude de Mod\\`eles \\`a base de r\\'eseaux Bay\\'esiens pour l'aide au\n  diagnostic de tumeurs c\\'er\\'ebrales",
        "abstract": "  This article describes different models based on Bayesian networks RB\nmodeling expertise in the diagnosis of brain tumors. Indeed, they are well\nadapted to the representation of the uncertainty in the process of diagnosis of\nthese tumors. In our work, we first tested several structures derived from the\nBayesian network reasoning performed by doctors on the one hand and structures\ngenerated automatically on the other. This step aims to find the best structure\nthat increases diagnostic accuracy. The machine learning algorithms relate\nMWST-EM algorithms, SEM and SEM + T. To estimate the parameters of the Bayesian\nnetwork from a database incomplete, we have proposed an extension of the EM\nalgorithm by adding a priori knowledge in the form of the thresholds calculated\nby the first phase of the algorithm RBE . The very encouraging results obtained\nare discussed at the end of the paper\n"
      },
      {
        "title": "Evidential Reasoning in a Categorial Perspective: Conjunction and\n  Disjunction of Belief Functions",
        "abstract": "  The categorial approach to evidential reasoning can be seen as a combination\nof the probability kinematics approach of Richard Jeffrey (1965) and the\nmaximum (cross-) entropy inference approach of E. T. Jaynes (1957). As a\nconsequence of that viewpoint, it is well known that category theory provides\nnatural definitions for logical connectives. In particular, disjunction and\nconjunction are modelled by general categorial constructions known as products\nand coproducts. In this paper, I focus mainly on Dempster-Shafer theory of\nbelief functions for which I introduce a category I call Dempster?s category. I\nprove the existence of and give explicit formulas for conjunction and\ndisjunction in the subcategory of separable belief functions. In Dempster?s\ncategory, the new defined conjunction can be seen as the most cautious\nconjunction of beliefs, and thus no assumption about distinctness (of the\nsources) of beliefs is needed as opposed to Dempster?s rule of combination,\nwhich calls for distinctness (of the sources) of beliefs.\n"
      },
      {
        "title": "The Relationship between Knowledge, Belief and Certainty",
        "abstract": "  We consider the relation between knowledge and certainty, where a fact is\nknown if it is true at all worlds an agent considers possible and is certain if\nit holds with probability 1. We identify certainty with probabilistic belief.\nWe show that if we assume one fixed probability assignment, then the logic\nKD45, which has been identified as perhaps the most appropriate for belief,\nprovides a complete axiomatization for reasoning about certainty. Just as an\nagent may believe a fact although phi is false, he may be certain that a fact\nphi, is true although phi is false. However, it is easy to see that an agent\ncan have such false (probabilistic) beliefs only at a set of worlds of\nprobability 0. If we restrict attention to structures where all worlds have\npositive probability, then S5 provides a complete axiomatization. If we\nconsider a more general setting, where there might be a different probability\nassignment at each world, then by placing appropriate conditions on the support\nof the probability function (the set of worlds which have non-zero\nprobability), we can capture many other well-known modal logics, such as T and\nS4. Finally, we consider which axioms characterize structures satisfying\nMiller's principle.\n"
      },
      {
        "title": "Argument Calculus and Networks",
        "abstract": "  A major reason behind the success of probability calculus is that it\npossesses a number of valuable tools, which are based on the notion of\nprobabilistic independence. In this paper, I identify a notion of logical\nindependence that makes some of these tools available to a class of\npropositional databases, called argument databases. Specifically, I suggest a\ngraphical representation of argument databases, called argument networks, which\nresemble Bayesian networks. I also suggest an algorithm for reasoning with\nargument networks, which resembles a basic algorithm for reasoning with\nBayesian networks. Finally, I show that argument networks have several\napplications: Nonmonotonic reasoning, truth maintenance, and diagnosis.\n"
      },
      {
        "title": "PDDL 2.1: Representation vs. Computation",
        "abstract": "  I comment on the PDDL 2.1 language and its use in the planning competition,\nfocusing on the choices made for accommodating time and concurrency. I also\ndiscuss some methodological issues that have to do with the move toward more\nexpressive planning languages and the balance needed in planning research\nbetween semantics and computation.\n"
      },
      {
        "title": "Toward a Category Theory Design of Ontological Knowledge Bases",
        "abstract": "  I discuss (ontologies_and_ontological_knowledge_bases /\nformal_methods_and_theories) duality and its category theory extensions as a\nstep toward a solution to Knowledge-Based Systems Theory. In particular I focus\non the example of the design of elements of ontologies and ontological\nknowledge bases of next three electronic courses: Foundations of Research\nActivities, Virtual Modeling of Complex Systems and Introduction to String\nTheory.\n"
      },
      {
        "title": "An efficient approach for finding the MPE in belief networks",
        "abstract": "  Given a belief network with evidence, the task of finding the I most probable\nexplanations (MPE) in the belief network is that of identifying and ordering\nthe I most probable instantiations of the non-evidence nodes of the belief\nnetwork. Although many approaches have been proposed for solving this problem,\nmost work only for restricted topologies (i.e., singly connected belief\nnetworks). In this paper, we will present a new approach for finding I MPEs in\nan arbitrary belief network. First, we will present an algorithm for finding\nthe MPE in a belief network. Then, we will present a linear time algorithm for\nfinding the next MPE after finding the first MPE. And finally, we will discuss\nthe problem of finding the MPE for a subset of variables of a belief network,\nand show that the problem can be efficiently solved by this approach.\n"
      },
      {
        "title": "AI 3D Cybug Gaming",
        "abstract": "  In this short paper I briefly discuss 3D war Game based on artificial\nintelligence concepts called AI WAR. Going in to the details, I present the\nimportance of CAICL language and how this language is used in AI WAR. Moreover\nI also present a designed and implemented 3D War Cybug for AI WAR using CAICL\nand discus the implemented strategy to defeat its enemies during the game life.\n"
      },
      {
        "title": "The Power of Modeling - a Response to PDDL2.1",
        "abstract": "  In this commentary I argue that although PDDL is a very useful standard for\nthe planning competition, its design does not properly consider the issue of\ndomain modeling. Hence, I would not advocate its use in specifying planning\ndomains outside of the context of the planning competition. Rather, the field\nneeds to explore different approaches and grapple more directly with the\nproblem of effectively modeling and utilizing all of the diverse pieces of\nknowledge we typically have about planning domains.\n"
      },
      {
        "title": "Defeasible Decisions: What the Proposal is and isn't",
        "abstract": "  In two recent papers, I have proposed a description of decision analysis that\ndiffers from the Bayesian picture painted by Savage, Jeffrey and other classic\nauthors. Response to this view has been either overly enthusiastic or unduly\npessimistic. In this paper I try to place the idea in its proper place, which\nmust be somewhere in between. Looking at decision analysis as defeasible\nreasoning produces a framework in which planning and decision theory can be\nintegrated, but work on the details has barely begun. It also produces a\nframework in which the meta-decision regress can be stopped in a reasonable\nway, but it does not allow us to ignore meta-level decisions. The heuristics\nfor producing arguments that I have presented are only supposed to be\nsuggestive; but they are not open to the egregious errors about which some have\nworried. And though the idea is familiar to those who have studied heuristic\nsearch, it is somewhat richer because the control of dialectic is more\ninteresting than the deepening of search.\n"
      },
      {
        "title": "Policy Iteration for Factored MDPs",
        "abstract": "  Many large MDPs can be represented compactly using a dynamic Bayesian\nnetwork. Although the structure of the value function does not retain the\nstructure of the process, recent work has shown that value functions in\nfactored MDPs can often be approximated well using a decomposed value function:\na linear combination of <I>restricted</I> basis functions, each of which refers\nonly to a small subset of variables. An approximate value function for a\nparticular policy can be computed using approximate dynamic programming, but\nthis approach (and others) can only produce an approximation relative to a\ndistance metric which is weighted by the stationary distribution of the current\npolicy. This type of weighted projection is ill-suited to policy improvement.\nWe present a new approach to value determination, that uses a simple\nclosed-form computation to directly compute a least-squares decomposed\napproximation to the value function <I>for any weights</I>. We then use this\nvalue determination algorithm as a subroutine in a policy iteration process. We\nshow that, under reasonable restrictions, the policies induced by a factored\nvalue function are compactly represented, and can be manipulated efficiently in\na policy iteration process. We also present a method for computing error bounds\nfor decomposed value functions using a variable-elimination algorithm for\nfunction optimization. The complexity of all of our algorithms depends on the\nfactorization of system dynamics and of the approximate value function.\n"
      },
      {
        "title": "Evidential Reasoning in a Categorial Perspective: Conjunction and\n  Disjunction of Belief Functions",
        "abstract": "  The categorial approach to evidential reasoning can be seen as a combination\nof the probability kinematics approach of Richard Jeffrey (1965) and the\nmaximum (cross-) entropy inference approach of E. T. Jaynes (1957). As a\nconsequence of that viewpoint, it is well known that category theory provides\nnatural definitions for logical connectives. In particular, disjunction and\nconjunction are modelled by general categorial constructions known as products\nand coproducts. In this paper, I focus mainly on Dempster-Shafer theory of\nbelief functions for which I introduce a category I call Dempster?s category. I\nprove the existence of and give explicit formulas for conjunction and\ndisjunction in the subcategory of separable belief functions. In Dempster?s\ncategory, the new defined conjunction can be seen as the most cautious\nconjunction of beliefs, and thus no assumption about distinctness (of the\nsources) of beliefs is needed as opposed to Dempster?s rule of combination,\nwhich calls for distinctness (of the sources) of beliefs.\n"
      },
      {
        "title": "Probabilistic and Non-Monotonic Inference",
        "abstract": "  (l) I have enough evidence to render the sentence S probable. (la) So,\nrelative to what I know, it is rational of me to believe S. (2) Now that I have\nmore evidence, S may no longer be probable. (2a) So now, relative to what I\nknow, it is not rational of me to believe S. These seem a perfectly ordinary,\ncommon sense, pair of situations. Generally and vaguely, I take them to embody\nwhat I shall call probabilistic inference. This form of inference is clearly\nnon-monotonic. Relatively few people have taken this form of inference, based\non high probability, to serve as a foundation for non-monotonic logic or for a\nlogical or defeasible inference. There are exceptions: Jane Nutter [16] thinks\nthat sometimes probability has something to do with non-monotonic reasoning.\nJudea Pearl [ 17] has recently been exploring the possibility. There are any\nnumber of people whom one might call probability enthusiasts who feel that\nprobability provides all the answers by itself, with no need of help from\nlogic. Cheeseman [1], Henrion [5] and others think it useful to look at a\ndistribution of probabilities over a whole algebra of statements, to update\nthat distribution in the light of new evidence, and to use the latest updated\ndistribution of probability over the algebra as a basis for planning and\ndecision making. A slightly weaker form of this approach is captured by Nilsson\n[15], where one assumes certain probabilities for certain statements, and\ninfers the probabilities, or constraints on the probabilities of other\nstatement. None of this corresponds to what I call probabilistic inference. All\nof the inference that is taking place, either in Bayesian updating, or in\nprobabilistic logic, is strictly deductive. Deductive inference, particularly\nthat concerned with the distribution of classical probabilities or chances, is\nof great importance. But this is not to say that there is no important role for\nwhat earlier logicians have called \"ampliative\" or \"inductive\" or \"scientific\"\ninference, in which the conclusion goes beyond the premises, asserts more than\ndo the premises. This depends on what David Israel [6] has called \"real rules\nof inference\". It is characteristic of any such logic or inference procedure\nthat it can go wrong: that statements accepted at one point may be rejected at\na later point. Research underlying the results reported here has been partially\nsupported by the Signals Warfare Center of the United States Army.\n"
      },
      {
        "title": "Logarithmic Time Parallel Bayesian Inference",
        "abstract": "  I present a parallel algorithm for exact probabilistic inference in Bayesian\nnetworks. For polytree networks with n variables, the worst-case time\ncomplexity is O(log n) on a CREW PRAM (concurrent-read, exclusive-write\nparallel random-access machine) with n processors, for any constant number of\nevidence variables. For arbitrary networks, the time complexity is O(r^{3w}*log\nn) for n processors, or O(w*log n) for r^{3w}*n processors, where r is the\nmaximum range of any variable, and w is the induced width (the maximum clique\nsize), after moralizing and triangulating the network.\n"
      },
      {
        "title": "Computational Aspects of the Mobius Transform",
        "abstract": "  In this paper we associate with every (directed) graph G a transformation\ncalled the Mobius transformation of the graph G. The Mobius transformation of\nthe graph (O) is of major significance for Dempster-Shafer theory of evidence.\nHowever, because it is computationally very heavy, the Mobius transformation\ntogether with Dempster's rule of combination is a major obstacle to the use of\nDempster-Shafer theory for handling uncertainty in expert systems. The major\ncontribution of this paper is the discovery of the 'fast Mobius\ntransformations' of (O). These 'fast Mobius transformations' are the fastest\nalgorithms for computing the Mobius transformation of (O). As an easy but\nuseful application, we provide, via the commonality function, an algorithm for\ncomputing Dempster's rule of combination which is much faster than the usual\none.\n"
      },
      {
        "title": "Improving the Performance of maxRPC",
        "abstract": "  Max Restricted Path Consistency (maxRPC) is a local consistency for binary\nconstraints that can achieve considerably stronger pruning than arc\nconsistency. However, existing maxRRC algorithms suffer from overheads and\nredundancies as they can repeatedly perform many constraint checks without\ntriggering any value deletions. In this paper we propose techniques that can\nboost the performance of maxRPC algorithms. These include the combined use of\ntwo data structures to avoid many redundant constraint checks, and heuristics\nfor the efficient ordering and execution of certain operations. Based on these,\nwe propose two closely related algorithms. The first one which is a maxRPC\nalgorithm with optimal O(end^3) time complexity, displays good performance when\nused stand-alone, but is expensive to apply during search. The second one\napproximates maxRPC and has O(en^2d^4) time complexity, but a restricted\nversion with O(end^4) complexity can be very efficient when used during search.\nBoth algorithms have O(ed) space complexity. Experimental results demonstrate\nthat the resulting methods constantly outperform previous algorithms for\nmaxRPC, often by large margins, and constitute a more than viable alternative\nto arc consistency on many problems.\n"
      },
      {
        "title": "New Worst-Case Upper Bound for #XSAT",
        "abstract": "  An algorithm running in O(1.1995n) is presented for counting models for exact\nsatisfiability formulae(#XSAT). This is faster than the previously best\nalgorithm which runs in O(1.2190n). In order to improve the efficiency of the\nalgorithm, a new principle, i.e. the common literals principle, is addressed to\nsimplify formulae. This allows us to eliminate more common literals. In\naddition, we firstly inject the resolution principles into solving #XSAT\nproblem, and therefore this further improves the efficiency of the algorithm.\n"
      },
      {
        "title": "GANC: Greedy Agglomerative Normalized Cut",
        "abstract": "  This paper describes a graph clustering algorithm that aims to minimize the\nnormalized cut criterion and has a model order selection procedure. The\nperformance of the proposed algorithm is comparable to spectral approaches in\nterms of minimizing normalized cut. However, unlike spectral approaches, the\nproposed algorithm scales to graphs with millions of nodes and edges. The\nalgorithm consists of three components that are processed sequentially: a\ngreedy agglomerative hierarchical clustering procedure, model order selection,\nand a local refinement.\n  For a graph of n nodes and O(n) edges, the computational complexity of the\nalgorithm is O(n log^2 n), a major improvement over the O(n^3) complexity of\nspectral methods. Experiments are performed on real and synthetic networks to\ndemonstrate the scalability of the proposed approach, the effectiveness of the\nmodel order selection procedure, and the performance of the proposed algorithm\nin terms of minimizing the normalized cut metric.\n"
      },
      {
        "title": "The SeqBin Constraint Revisited",
        "abstract": "  We revisit the SeqBin constraint. This meta-constraint subsumes a number of\nimportant global constraints like Change, Smooth and IncreasingNValue. We show\nthat the previously proposed filtering algorithm for SeqBin has two drawbacks\neven under strong restrictions: it does not detect bounds disentailment and it\nis not idempotent. We identify the cause for these problems, and propose a new\npropagator that overcomes both issues. Our algorithm is based on a connection\nto the problem of finding a path of a given cost in a restricted $n$-partite\ngraph. Our propagator enforces domain consistency in O(nd^2) and, for special\ncases of SeqBin that include Change, Smooth and IncreasingNValue, in O(nd)\ntime.\n"
      },
      {
        "title": "Flow-Based Propagators for the SEQUENCE and Related Global Constraints",
        "abstract": "  We propose new filtering algorithms for the SEQUENCE constraint and some\nextensions of the SEQUENCE constraint based on network flows. We enforce domain\nconsistency on the SEQUENCE constraint in $O(n^2)$ time down a branch of the\nsearch tree. This improves upon the best existing domain consistency algorithm\nby a factor of $O(\\log n)$. The flows used in these algorithms are derived from\na linear program. Some of them differ from the flows used to propagate global\nconstraints like GCC since the domains of the variables are encoded as costs on\nthe edges rather than capacities. Such flows are efficient for maintaining\nbounds consistency over large domains and may be useful for other global\nconstraints.\n"
      },
      {
        "title": "The Weighted CFG Constraint",
        "abstract": "  We introduce the weighted CFG constraint and propose a propagation algorithm\nthat enforces domain consistency in $O(n^3|G|)$ time. We show that this\nalgorithm can be decomposed into a set of primitive arithmetic constraints\nwithout hindering propagation.\n"
      },
      {
        "title": "Dynamic Network Updating Techniques For Diagnostic Reasoning",
        "abstract": "  A new probabilistic network construction system, DYNASTY, is proposed for\ndiagnostic reasoning given variables whose probabilities change over time.\nDiagnostic reasoning is formulated as a sequential stochastic process, and is\nmodeled using influence diagrams. Given a set O of observations, DYNASTY\ncreates an influence diagram in order to devise the best action given O.\nSensitivity analyses are conducted to determine if the best network has been\ncreated, given the uncertainty in network parameters and topology. DYNASTY uses\nan equivalence class approach to provide decision thresholds for the\nsensitivity analysis. This equivalence-class approach to diagnostic reasoning\ndifferentiates diagnoses only if the required actions are different. A set of\nnetwork-topology updating algorithms are proposed for dynamically updating the\nnetwork when necessary.\n"
      },
      {
        "title": "A Polynomial Time Algorithm for Finding Bayesian Probabilities from\n  Marginal Constraints",
        "abstract": "  A method of calculating probability values from a system of marginal\nconstraints is presented. Previous systems for finding the probability of a\nsingle attribute have either made an independence assumption concerning the\nevidence or have required, in the worst case, time exponential in the number of\nattributes of the system. In this paper a closed form solution to the\nprobability of an attribute given the evidence is found. The closed form\nsolution, however does not enforce the (non-linear) constraint that all terms\nin the underlying distribution be positive. The equation requires O(r^3) steps\nto evaluate, where r is the number of independent marginal constraints\ndescribing the system at the time of evaluation. Furthermore, a marginal\nconstraint may be exchanged with a new constraint, and a new solution\ncalculated in O(r^2) steps. This method is appropriate for calculating\nprobabilities in a real time expert system\n"
      },
      {
        "title": "Logarithmic Time Parallel Bayesian Inference",
        "abstract": "  I present a parallel algorithm for exact probabilistic inference in Bayesian\nnetworks. For polytree networks with n variables, the worst-case time\ncomplexity is O(log n) on a CREW PRAM (concurrent-read, exclusive-write\nparallel random-access machine) with n processors, for any constant number of\nevidence variables. For arbitrary networks, the time complexity is O(r^{3w}*log\nn) for n processors, or O(w*log n) for r^{3w}*n processors, where r is the\nmaximum range of any variable, and w is the induced width (the maximum clique\nsize), after moralizing and triangulating the network.\n"
      },
      {
        "title": "GANC: Greedy Agglomerative Normalized Cut",
        "abstract": "  This paper describes a graph clustering algorithm that aims to minimize the\nnormalized cut criterion and has a model order selection procedure. The\nperformance of the proposed algorithm is comparable to spectral approaches in\nterms of minimizing normalized cut. However, unlike spectral approaches, the\nproposed algorithm scales to graphs with millions of nodes and edges. The\nalgorithm consists of three components that are processed sequentially: a\ngreedy agglomerative hierarchical clustering procedure, model order selection,\nand a local refinement.\n  For a graph of n nodes and O(n) edges, the computational complexity of the\nalgorithm is O(n log^2 n), a major improvement over the O(n^3) complexity of\nspectral methods. Experiments are performed on real and synthetic networks to\ndemonstrate the scalability of the proposed approach, the effectiveness of the\nmodel order selection procedure, and the performance of the proposed algorithm\nin terms of minimizing the normalized cut metric.\n"
      },
      {
        "title": "On characterizing Inclusion of Bayesian Networks",
        "abstract": "  Every directed acyclic graph (DAG) over a finite non-empty set of variables\n(= nodes) N induces an independence model over N, which is a list of\nconditional independence statements over N.The inclusion problem is how to\ncharacterize (in graphical terms) whether all independence statements in the\nmodel induced by a DAG K are in the model induced by a second DAG L. Meek\n(1997) conjectured that this inclusion holds iff there exists a sequence of\nDAGs from L to K such that only certain 'legal' arrow reversal and 'legal'\narrow adding operations are performed to get the next DAG in the sequence.In\nthis paper we give several characterizations of inclusion of DAG models and\nverify Meek's conjecture in the case that the DAGs K and L differ in at most\none adjacency. As a warming up a rigorous proof of well-known graphical\ncharacterizations of equivalence of DAGs, which is a highly related problem, is\ngiven.\n"
      },
      {
        "title": "Reasoning With Qualitative Probabilities Can Be Tractable",
        "abstract": "  We recently described a formalism for reasoning with if-then rules that re\nexpressed with different levels of firmness [18]. The formalism interprets\nthese rules as extreme conditional probability statements, specifying orders of\nmagnitude of disbelief, which impose constraints over possible rankings of\nworlds. It was shown that, once we compute a priority function Z+ on the rules,\nthe degree to which a given query is confirmed or denied can be computed in\nO(log n`) propositional satisfiability tests, where n is the number of rules in\nthe knowledge base. In this paper, we show that computing Z+ requires O(n2 X\nlog n) satisfiability tests, not an exponential number as was conjectured in\n[18], which reduces to polynomial complexity in the case of Horn expressions.\nWe also show how reasoning with imprecise observations can be incorporated in\nour formalism and how the popular notions of belief revision and epistemic\nentrenchment are embodied naturally and tractably.\n"
      },
      {
        "title": "Bayesian Error-Bars for Belief Net Inference",
        "abstract": "  A Bayesian Belief Network (BN) is a model of a joint distribution over a\nsetof n variables, with a DAG structure to represent the immediate\ndependenciesbetween the variables, and a set of parameters (aka CPTables) to\nrepresent thelocal conditional probabilities of a node, given each assignment\nto itsparents. In many situations, these parameters are themselves random\nvariables - this may reflect the uncertainty of the domain expert, or may come\nfrom atraining sample used to estimate the parameter values. The distribution\noverthese \"CPtable variables\" induces a distribution over the response the\nBNwill return to any \"What is Pr(H | E)?\" query. This paper investigates\nthevariance of this response, showing first that it is asymptotically\nnormal,then providing its mean and asymptotical variance. We then present\naneffective general algorithm for computing this variance, which has the\nsamecomplexity as simply computing the (mean value of) the response itself -\nie,O(n 2^w), where n is the number of variables and w is the effective\ntreewidth. Finally, we provide empirical evidence that this algorithm,\nwhichincorporates assumptions and approximations, works effectively in\npractice,given only small samples.\n"
      },
      {
        "title": "On the Robustness of Most Probable Explanations",
        "abstract": "  In Bayesian networks, a Most Probable Explanation (MPE) is a complete\nvariable instantiation with a highest probability given the current evidence.\nIn this paper, we discuss the problem of finding robustness conditions of the\nMPE under single parameter changes. Specifically, we ask the question: How much\nchange in a single network parameter can we afford to apply while keeping the\nMPE unchanged? We will describe a procedure, which is the first of its kind,\nthat computes this answer for each parameter in the Bayesian network variable\nin time O(n exp(w)), where n is the number of network variables and w is its\ntreewidth.\n"
      },
      {
        "title": "Noisy Search with Comparative Feedback",
        "abstract": "  We present theoretical results in terms of lower and upper bounds on the\nquery complexity of noisy search with comparative feedback. In this search\nmodel, the noise in the feedback depends on the distance between query points\nand the search target. Consequently, the error probability in the feedback is\nnot fixed but varies for the queries posed by the search algorithm. Our results\nshow that a target out of n items can be found in O(log n) queries. We also\nshow the surprising result that for k possible answers per query, the speedup\nis not log k (as for k-ary search) but only log log k in some cases.\n"
      },
      {
        "title": "Lex-Partitioning: A New Option for BDD Search",
        "abstract": "  For the exploration of large state spaces, symbolic search using binary\ndecision diagrams (BDDs) can save huge amounts of memory and computation time.\nState sets are represented and modified by accessing and manipulating their\ncharacteristic functions. BDD partitioning is used to compute the image as the\ndisjunction of smaller subimages.\n  In this paper, we propose a novel BDD partitioning option. The partitioning\nis lexicographical in the binary representation of the states contained in the\nset that is represented by a BDD and uniform with respect to the number of\nstates represented. The motivation of controlling the state set sizes in the\npartitioning is to eventually bridge the gap between explicit and symbolic\nsearch.\n  Let n be the size of the binary state vector. We propose an O(n) ranking and\nunranking scheme that supports negated edges and operates on top of precomputed\nsatcount values. For the uniform split of a BDD, we then use unranking to\nprovide paths along which we partition the BDDs. In a shared BDD representation\nthe efforts are O(n). The algorithms are fully integrated in the CUDD library\nand evaluated in strongly solving general game playing benchmarks.\n"
      },
      {
        "title": "Local search for stable marriage problems",
        "abstract": "  The stable marriage (SM) problem has a wide variety of practical\napplications, ranging from matching resident doctors to hospitals, to matching\nstudents to schools, or more generally to any two-sided market. In the\nclassical formulation, n men and n women express their preferences (via a\nstrict total order) over the members of the other sex. Solving a SM problem\nmeans finding a stable marriage where stability is an envy-free notion: no man\nand woman who are not married to each other would both prefer each other to\ntheir partners or to being single. We consider both the classical stable\nmarriage problem and one of its useful variations (denoted SMTI) where the men\nand women express their preferences in the form of an incomplete preference\nlist with ties over a subset of the members of the other sex. Matchings are\npermitted only with people who appear in these lists, an we try to find a\nstable matching that marries as many people as possible. Whilst the SM problem\nis polynomial to solve, the SMTI problem is NP-hard. We propose to tackle both\nproblems via a local search approach, which exploits properties of the problems\nto reduce the size of the neighborhood and to make local moves efficiently. We\nevaluate empirically our algorithm for SM problems by measuring its runtime\nbehaviour and its ability to sample the lattice of all possible stable\nmarriages. We evaluate our algorithm for SMTI problems in terms of both its\nruntime behaviour and its ability to find a maximum cardinality stable\nmarriage.For SM problems, the number of steps of our algorithm grows only as\nO(nlog(n)), and that it samples very well the set of all stable marriages. It\nis thus a fair and efficient approach to generate stable marriages.Furthermore,\nour approach for SMTI problems is able to solve large problems, quickly\nreturning stable matchings of large and often optimal size despite the\nNP-hardness of this problem.\n"
      },
      {
        "title": "Steepest Ascent Hill Climbing For A Mathematical Problem",
        "abstract": "  The paper proposes artificial intelligence technique called hill climbing to\nfind numerical solutions of Diophantine Equations. Such equations are important\nas they have many applications in fields like public key cryptography, integer\nfactorization, algebraic curves, projective curves and data dependency in super\ncomputers. Importantly, it has been proved that there is no general method to\nfind solutions of such equations. This paper is an attempt to find numerical\nsolutions of Diophantine equations using steepest ascent version of Hill\nClimbing. The method, which uses tree representation to depict possible\nsolutions of Diophantine equations, adopts a novel methodology to generate\nsuccessors. The heuristic function used help to make the process of finding\nsolution as a minimization process. The work illustrates the effectiveness of\nthe proposed methodology using a class of Diophantine equations given by a1. x1\np1 + a2. x2 p2 + ...... + an . xn pn = N where ai and N are integers. The\nexperimental results validate that the procedure proposed is successful in\nfinding solutions of Diophantine Equations with sufficiently large powers and\nlarge number of variables.\n"
      }
    ]
  }
]